[{"categories":null,"content":" 1 概要 promqlinterというlinterを作った 構文検査を行ってくれる デフォルトで組み込むべきLintルールを募集しています GitHub Actionsで使えることを想定しています PromQL Linter Frameworkを作った 追加で、ユーザ独自にLintルールをつなぎ込めるようになっています ","date":"2022-12-18","objectID":"/ja/diaries/promqlinter/:1:0","series":null,"tags":["prometheus"],"title":"PromQL Linterをつくった","uri":"/ja/diaries/promqlinter/#概要"},{"categories":null,"content":" 2 背景最近、業務でPrometheusを触っているのですが、 PromQLに慣れていないことから、組み込み関数に関する型検査でエラーを出してしまったり、 そもそもどんな演算が利用できるのかわかっていなかったりします。 私はなにか新しいプログラミング言語を勉強しようと思ったとき、 それについての公式ドキュメントや構文仕様を読んで勉強することが多いのですが、 PromQLも例にもれず、なんとなくパーサを書いてみようかな?と思っていました。 また、業務ではPrometheus Operatorを利用した宣言的なアラート管理を行ったりしています。 その関係で、Kubernetesマニフェスト内のPromQLをうまく検査できるツールを作ることにしました。 業務については、先日発表したこちらのスライドが参考になります。 https://speakerdeck.com/drumato/activities-about-kubernetes-operation-improvements-as-an-sre ","date":"2022-12-18","objectID":"/ja/diaries/promqlinter/:2:0","series":null,"tags":["prometheus"],"title":"PromQL Linterをつくった","uri":"/ja/diaries/promqlinter/#背景"},{"categories":null,"content":" 3 実装結局パーサは自作ではなく、Prometheusコミュニティのものを流用することにしたのですが、 このパッケージにより、構文エラーだけではなく、組み込み関数の型検査等も行ってくれることがわかりました。 これをうまく利用できるツールとして、 promqlinter を開発しました。 コンセプトが思い浮かんでから2日で実装したので、まだまだ機能は少ないですが。 https://github.com/Drumato/promqlinter 現在、主な機能として以下を備えています。 標準入力から、パイプでつなげて使えるようにしている PrometheusOperatorの PrometheusRule リソースにも対応している CRDのexprフィールドを探索して、このLinterを起動 エラーの箇所は、色付きで、わかりやすいフォーマット付きで表示 ","date":"2022-12-18","objectID":"/ja/diaries/promqlinter/:3:0","series":null,"tags":["prometheus"],"title":"PromQL Linterをつくった","uri":"/ja/diaries/promqlinter/#実装"},{"categories":null,"content":" 4 PromQL Linter Frameworkについてこのプロジェクトで利用している仕組みとして、 ユーザが独自にLintルールを追加できる、 PromQL Linter Framework というものを用意しています。 といっても、単にGo interfaceで好きにLintルールが挟み込めるようになっているというだけの、シンプルなものです。 // pkg/linter/plugin.go package linter import \"github.com/prometheus/prometheus/promql/parser\" // PromQLinterPlugin is an interface that all linter plugin must implement. type PromQLinterPlugin interface { // Name represents the name of the plugin. // the name is used in the reporting message from the linter. Name() string // Execute lints the PromQL expression. Execute(expr parser.Expr) (Diagnostics, error) } すべてのLintルールは、上記インタフェースを実装するようにするだけで、このFrameworkに挟み込むことができます。 例えば、 examples/dummy には、独自のルールを用意するサンプルが置いてあります。 このdummy pluginも、構文検査の恩恵が受けられる点と、 pkg/linter.Diagnostic の利用により、フォーマットされたエラーメッセージを利用できる点が便利です。 // examples/dummy/main.go package main import ( \"fmt\" \"os\" \"github.com/Drumato/promqlinter/pkg/linter\" \"github.com/prometheus/prometheus/promql/parser\" ) type samplePlugin struct{} // Execute implements linter.PromQLinterPlugin func (*samplePlugin) Execute(expr parser.Expr) (linter.Diagnostics, error) { ds := linter.NewDiagnostics() ds.Add(linter.ColoredInfoDiagnostic( parser.PositionRange{}, \"foo\", )) ds.Add(linter.ColoredInfoDiagnostic( parser.PositionRange{}, \"bar\", )) ds.Add(linter.ColoredInfoDiagnostic( parser.PositionRange{}, \"baz\", )) return ds, nil } // Name implements linter.PromQLinterPlugin func (*samplePlugin) Name() string { return \"sample-plugin\" } var _ linter.PromQLinterPlugin = \u0026samplePlugin{} func main() { l := linter.New( linter.WithPlugin(\u0026samplePlugin{}), linter.WithOutStream(os.Stdout), ) result, err := l.Execute(\"http_requests_total\", linter.DiagnosticLevelWarning) if err != nil { fmt.Fprintf(os.Stderr, \"%+v\\n\", err) os.Exit(1) } if result.Failed() { os.Exit(1) } } ","date":"2022-12-18","objectID":"/ja/diaries/promqlinter/:4:0","series":null,"tags":["prometheus"],"title":"PromQL Linterをつくった","uri":"/ja/diaries/promqlinter/#promql-linter-frameworkについて"},{"categories":null,"content":" 5 GitHub Actions業務では、このpromqlinterをCIに組み込んで、 Kubernetesマニフェスト内のPromQL Expressionが構文エラーを起こしていないかチェックさせようとしています。 そのために、本プロジェクトには action.yml を配置しています。 ","date":"2022-12-18","objectID":"/ja/diaries/promqlinter/:5:0","series":null,"tags":["prometheus"],"title":"PromQL Linterをつくった","uri":"/ja/diaries/promqlinter/#github-actions"},{"categories":null,"content":" 6 おわりに今回は、promqlinterというツールと、それが提供するフレームワークについてご紹介しました。 GitHub Actions及びツールはOSSとして公開しているので、 ぜひフィードバック頂けますと幸いです。 特に、promqlinterにデフォルトで導入してほしいLintルール等の要望があれば、 是非Issueでご共有ください。 ","date":"2022-12-18","objectID":"/ja/diaries/promqlinter/:6:0","series":null,"tags":["prometheus"],"title":"PromQL Linterをつくった","uri":"/ja/diaries/promqlinter/#おわりに"},{"categories":null,"content":"久しぶりに更新。 最近も変わらず、Kubernetesのことばかりやっている。 特にCluster APIの検証とか、それに関わるエコシステムの実装とかを重点的にやっている。 4月に新卒入社し、7月に配属されて以来、ずっと業務に関することや、 それに繋がりそうな技術ばかり調べていたので、 久しぶりに趣味の勉強したいなと思い、いくつか技術書を買った。 そして、沖縄に向かう飛行機でまとまった読書時間を確保できそうだったので、 ｢ネットワーク自動化とプログラマビリティ｣ をざっと流し読みした。 https://www.oreilly.co.jp/books/9784873119816/ ここでは短く、雑に感想をまとめておこうと思う。 ","date":"2022-10-11","objectID":"/ja/diaries/network-automation-and-programability/:0:0","series":null,"tags":["network"],"title":"｢ネットワーク自動化とプログラマビリティ｣を読んで","uri":"/ja/diaries/network-automation-and-programability/#"},{"categories":null,"content":" 1 概要本書の内容を個人的にまとめるなら、以下の成分をちょうど均等にまとめたもののように感じた。 “ネットワーク自動化とはなにか、どのように実践するか” を説明できるようにするための情報 ネットワーク自動化を実現する技術の背景 注目されることとなった歴史的理由 ネットワーク自動化を実現することの重要性 どういった目的で導入するべきなのかの紹介 上記を実践したい “ネットワークエンジニア” に向けた、具体的な技術知識 Linuxのファイルシステムやコマンド、sysctlパラメータを制御したネットワーク機能の紹介 Linuxと合わせてよく利用されるPythonの簡単な紹介 Jinjaなどのテンプレート言語実用例 AnsibleやSaltなどの自動化ツール それらをGitで管理して、CIに組み込むまでの流れ解説 本書を読む前提として、私はSDNやネットワーク自動化、関連したReliability Engineeringまでの考え方や実例を学ばさせていただいたことから、これらの概念を読み進めるのに苦労しなかった。 仮想ルータクラスタを自動でローリングアップデートする仕組みの検討と実装 BGP Graceful Restartに関わる各OSSルーティングプラットフォームの動向調査 一方で本書を読み、改めてOpenFlow周りの歴史的経緯や、今日のネットワーキングに与える影響度みたいな部分を勉強した。 これらの内容はは、読み物的な意味でとても貴重だった。 こういった内容をきちんと日本語で言語化されている本が出版されていることは非常に大きいと思う。 監訳者の土屋さんには、YouTubeでshow intを通じた活動も相まって、本当に頭が上がらない。 ","date":"2022-10-11","objectID":"/ja/diaries/network-automation-and-programability/:1:0","series":null,"tags":["network"],"title":"｢ネットワーク自動化とプログラマビリティ｣を読んで","uri":"/ja/diaries/network-automation-and-programability/#概要"},{"categories":null,"content":" 2 特にしたい話","date":"2022-10-11","objectID":"/ja/diaries/network-automation-and-programability/:2:0","series":null,"tags":["network"],"title":"｢ネットワーク自動化とプログラマビリティ｣を読んで","uri":"/ja/diaries/network-automation-and-programability/#特にしたい話"},{"categories":null,"content":" 3 ソフトウェアエンジニアがネットワークに取り組むということ私はソフトウェア開発のレイヤからネットワーク、 特にSDNコントローラの開発やアーキテクチャに興味を持ち取り組んできた経緯がある。 したがって、3章から6章、そして8/9章は既知で、慣れ親しんだ内容が多かった。 しかし、XML-RPCやNETCONFコンフィグレーション/オペレーションの実例はあまり詳しくなかったので、勉強になった。 全体で600ページ弱の分厚い本だが、3章以降の各章はある程度独立しているので、 目次で内容を俯瞰してつまみ食いすれば、そこまで読むハードルは高くないと思う。 必ずしもそうとはいえないが、 ソフトウェア開発について取り組んできた人間がネットワークアーキテクチャや運用等に関わるパターンには、 ｢ソフトウェアによる運用の効率化、あるいはプログラマビリティの実現を “ネイティブに” 考え始められる｣ という利点があるように感じた。 上述した記事のように、私はKubernetesを利用したSDNプラットフォームの開発に参加させていただいた背景もあり、 Webアプリケーションやその他ソフトウェアシステムの運用と、ネットワーク運用に対するアプローチの違いをそこまで大きく認識していない。 もちろんネットワークレイヤでは、より物理的な障害特性等を考慮したりなど、考えるべきポイントが異なるのは言うまでもないが、 “ソフトウェアないし何らかの自動化ソリューションを適用して改善する” ことに対する抵抗が全くない。 これは、GoogleがSREの採用方針として “ピュアなソフトウェアエンジニアスキルを持っていること” を掲げているのに近いものだと感じている。 ","date":"2022-10-11","objectID":"/ja/diaries/network-automation-and-programability/:3:0","series":null,"tags":["network"],"title":"｢ネットワーク自動化とプログラマビリティ｣を読んで","uri":"/ja/diaries/network-automation-and-programability/#ソフトウェアエンジニアがネットワークに取り組むということ"},{"categories":null,"content":" 4 組織の中でどうネットワークインテリジェンスを高めるかSRE界隈でよく議論されるものと近い話題として、 こうした自動化やプログラマビリティの導入等、運用方針の大規模な刷新にはビジネスレイヤや組織への考慮が欠かせない、というものがある。 そのような点もきれいに書かれていて好印象だった。 ","date":"2022-10-11","objectID":"/ja/diaries/network-automation-and-programability/:4:0","series":null,"tags":["network"],"title":"｢ネットワーク自動化とプログラマビリティ｣を読んで","uri":"/ja/diaries/network-automation-and-programability/#組織の中でどうネットワークインテリジェンスを高めるか"},{"categories":null,"content":" 5 総評私はこの本を、以下のような人に薦めたいと感じた。 ネットワークに関する知識はあるが、既存ソリューションや独自システムを構築して自動化に手を出してみたいネットワークエンジニアの方 今日のネットワーキングがどのようなことに感心を持っているかという一例を勉強したいソフトウェアエンジニアの方 今日においては、単にSDNなどを利用して自動化するだけでなく、DCN用に新たな輻輳制御アルゴリズムが考案されたりと様々ある その中でも、ネットワーク自動化については長い間取り組まれてきた、大きな分野であり、勉強の価値はある ","date":"2022-10-11","objectID":"/ja/diaries/network-automation-and-programability/:5:0","series":null,"tags":["network"],"title":"｢ネットワーク自動化とプログラマビリティ｣を読んで","uri":"/ja/diaries/network-automation-and-programability/#総評"},{"categories":null,"content":" 6 余談そろそろ社内で作っていたり動かしている諸々を外部成果に切り出せそうな気がしている。 これができれば、自分のKubernetesエンジニア/SREとしてのキャリアになりそうな気がするので、努力あるのみ。 それとは別に、こういった趣味の勉強は欠かさず行って、エンジニアとしての価値を高め続けたいと感じている。 ","date":"2022-10-11","objectID":"/ja/diaries/network-automation-and-programability/:6:0","series":null,"tags":["network"],"title":"｢ネットワーク自動化とプログラマビリティ｣を読んで","uri":"/ja/diaries/network-automation-and-programability/#余談"},{"categories":null,"content":"Blogを移行したのをきっかけに, ときどき日記みたいなのを投稿していくことにした. 今日はその, Blogを移行した話をつらつらと. repositoryはこちら. https://github.com/Drumato/drumato.comv2 ","date":"2022-06-07","objectID":"/ja/diaries/myblog/:0:0","series":null,"tags":["typescript"],"title":"The 1st TypeScript output","uri":"/ja/diaries/myblog/#"},{"categories":null,"content":" 1 経緯まず, そもそもなんでBlogを移行しようと思っていたのか, みたいな話だけど, 理由は特にない. ある日新卒同期に, ｢鈴木僚太さんが執筆されているブルーベリー本 と呼ばれる本が欲しいんだよね｣って話をしてたら, なんと恵んでくれることになった. それを機にTypeScriptに入門したので, せっかくならなにか作らないと と思って手を出したのが自作Blogだった. ちなみにTypeScriptのcodeを初めて書いた日から1週間ぐらいしか経ってないので, 今考えると普通に無茶だと思う. 以前運用していたBlogは Hugo と呼ばれるGo製のSSG frameworkで, すごいお手軽にtemplateを利用するだけでBlogを作り上げられる優れもの. 当時私が書いたのはblog templateに設定を注ぎ込むtomlと, ちょっと好みでstyleを書き換えたぐらい. それでとてもきれいなBlogが動いていたので, UI/Frontend を書いたことがない私にはうってつけだった. しかし今, 新卒engineerとして色々な分野の知識をcatch upしていると, Web Programmingに対する興味というか, ｢これはinfra engineerとして活躍していきたいとしても, 身につけておかなければいけない\"引き出し\"だな｣ と感じる機会が増えてきた. なので自作Blogを作ろうと思ったと, そういう感じ. ","date":"2022-06-07","objectID":"/ja/diaries/myblog/:1:0","series":null,"tags":["typescript"],"title":"The 1st TypeScript output","uri":"/ja/diaries/myblog/#経緯"},{"categories":null,"content":" 2 対応している機能 あるdirectoryにmarkdownを置いておくと, build時に頑張ってcontentsを作ってくれる 記事/日記一覧機能が一応ある(paginationはまだない) tagで検索することもできる code snippetはsyntax highlightが効くようになっている ToCだけvscodeの拡張機能で作れば, heading idは勝手に作ってくれる 一応頑張ってPC/mobileで最低限見れるものを作る 英語と日本語に対応している こうしてみると, Frontendのcodingを一切したことがない私にしては頑張ったんじゃないかと思えてくる. yarn create next-app は実行したけど, それ以外は基本的に全部自分で書いたし. 実際の記事画面を下に. fontはKlee Oneというもので統一してる. 可愛い. Chromeの検証機能を使ってmobile画面を想定してみた. こちらもそれなりに見やすいんじゃないかなーって思っている. ずっとheaderが崩れる問題に直面してたんだけど, hamburger menuにすることで解決した. ","date":"2022-06-07","objectID":"/ja/diaries/myblog/:2:0","series":null,"tags":["typescript"],"title":"The 1st TypeScript output","uri":"/ja/diaries/myblog/#対応している機能"},{"categories":null,"content":" 3 使った技術さて, どのように作ろうかと思っていたのだけれど, 特段comment機能みたいなものも求めていなかったし, これ結局SSGで完結しそうだなー, みたいな感覚があった. なので結局は以下のような構成に. Material UIを採用した理由は特になくて, 名前を知っていたから. language: TypeScript framework: Next.js(only SSG mode) UI library: MUI(Material UI) markdown rendering: react-markdown ","date":"2022-06-07","objectID":"/ja/diaries/myblog/:3:0","series":null,"tags":["typescript"],"title":"The 1st TypeScript output","uri":"/ja/diaries/myblog/#使った技術"},{"categories":null,"content":" 4 まだ作り込みたい部分 dark mode 月別で記事を表示する機能 SNS share ","date":"2022-06-07","objectID":"/ja/diaries/myblog/:4:0","series":null,"tags":["typescript"],"title":"The 1st TypeScript output","uri":"/ja/diaries/myblog/#まだ作り込みたい部分"},{"categories":null,"content":" 5 感想TypeScript自体はかなり好きな言語だと思う. 私はtype systemに乗っかってcodingするのが大好きだし, script languageならではの軽快さもある. GoやRustばかり書いているしそれらも大好きだけれど, TypeScriptはそれに続くぐらい好きな言語になるかもしれない. Next.js自体はかなり入門しやすいんじゃないかなって思った. 少なくとも今回実装する上で必要になった機能については特に苦労しなかった. もちろんperformanceの観点で最適化するべき部分はたくさんあるのかもしれないし, SSG onlyだったから簡単だったんだよって話もあるかもしれないけど. 次はISRを触ってみたい. 強いて苦労した点をあげるなら, library云々というよりもstyling全般だと思う. 特にstyleが崩れる問題とかを解決するのはそれなりに時間がかかった. あとはreact-markdownの仕様とか使い方かな. ","date":"2022-06-07","objectID":"/ja/diaries/myblog/:5:0","series":null,"tags":["typescript"],"title":"The 1st TypeScript output","uri":"/ja/diaries/myblog/#感想"},{"categories":null,"content":" 6 これから個人的な趣味で2022年4月からbackend開発みたいな分野にはちょくちょく手を出していたんだけど, 今回ついにTypeScriptでなにかものを作ってみて, Web Programmingに真剣に挑戦してみる動機が十分にあると再確認できた. 私はinfra software engineeringの分野で活躍していきたいと思っているし, それは変わらないのだけれど, そもそもCloud Technologyをどんどん使っていかないと持つことのできない視点があると思うし, いずれuser friendlyなものを作る必要があると思う. そのためにWeb UIを作る経験が役立つと思う. ということで, 次はもう少し仕様が大きく, 動的に色々扱うWeb Applicationを作って見ようと思う. serviceとして公開するかはわからないけど, 必ずoutputはする予定. ","date":"2022-06-07","objectID":"/ja/diaries/myblog/:6:0","series":null,"tags":["typescript"],"title":"The 1st TypeScript output","uri":"/ja/diaries/myblog/#これから"},{"categories":null,"content":"現在も活発に開発しているため，仕様が大きく変更される場合があります． お久しぶりです． 最近は新卒社員として研修を頑張ったり，趣味時間には自分に足りていない技術のcatch upを行っているDrumatoです． Go 1.18からGenericsが導入 されましたね，皆さん使ってますか? 私は最近になってようやく仕様を勉強し始めたのですが， そもそも楽しかったGo Programmingが更に楽しくなって，私はとても嬉しいです． 私のGo Generics DebutはSingly-Linked-Listの実装でした．凄い簡略化したものですが． より高度に扱いたくなってきた私にとって，Parser Combinatorは非常に良い練習の題材だったのですが， 作っていくうちに，練習に限らず普通に便利なlibraryができているんじゃないか，とうぬぼれ始めています． そこで今回は，私が作って遊んでいるParser Combinatorの紹介をしたいと思います．RepositoryとDocumentはこちら． https://github.com/Drumato/peachcomb https://pkg.go.dev/github.com/Drumato/peachcomb idea自体はGo Genericsのtutorialを見た段階である程度思いついていて，あとはそれをある日の朝適当に実装してみた，というのが始まりです． ちなみにGo Genericsの説明は既に多くのGopherさん達が書いてくださっているので，そちらを参照されるのがよいと思います． 私はofficial tutorial以外の資料を読まずに作れたので，この記事を読む上でそれら資料の内容を完全に理解している必要はないかもしれません． https://go.dev/doc/tutorial/generics ","date":"2022-04-10","objectID":"/ja/posts/parsecomb/:0:0","series":null,"tags":["go","parser"],"title":"Go GenericsでつくるParser Combinator","uri":"/ja/posts/parsecomb/#"},{"categories":null,"content":" 1 概要peachcombは， parserを初期化 して 呼び出すだけで使える，というような原始的な機能を提供します． ということで，早速使用例をお見せしたいと思います． 次のsampleは， | 記号で区切られた数列をparseするものです． https://go.dev/play/p/qIbzx_IWxbr package main import ( \"github.com/Drumato/peachcomb/pkg/strparse\" \"github.com/Drumato/peachcomb/pkg/combinator\" ) func main() { element := strparse.Digit1() separator := strparse.Rune('|') p := combinator.Separated1(element, separator) i, o, err := p([]rune(\"123|456|789Drumato\")) fmt.Println(i) fmt.Printf(\"%s\\n\", o) fmt.Println(err) } $ go run main.go Drumato [123 456 789] \u003cnil\u003e i は入力全体から，parser p によって消費された文字列を除いた残りの入力が格納されています． o はparser p の成果物が含まれます．ここでは []string{\"123\", \"456\", \"789\"} のようになっています． 使い方は以上です．特に難しい点はないと思います． ","date":"2022-04-10","objectID":"/ja/posts/parsecomb/:1:0","series":null,"tags":["go","parser"],"title":"Go GenericsでつくるParser Combinator","uri":"/ja/posts/parsecomb/#概要"},{"categories":null,"content":" 2 仕組み設計は基本的にRust製のParser Libraryである Geal/nom をかなり参考にしています． こちらの記事で解説していますが，dynamic dispatchをほぼ使わずにgeneralなparserを構成できる点が魅力で， 私がRust applicationでなにかparserを書くとき，基本的にnomを使っています． nomの仕組みを簡単に説明すると，“trait boundsでゴリゴリに強制することで高速かつ汎用的にparserを組み合わせられる\"ように頑張っているlibraryです． parser inputに要求するtrait を細かく分けることで，基本的にほぼすべてのparserがtype-parametricに扱えるようになっています． 一方peachcombは，nomよりだいぶlooseではあるものの，近いものを実現しようと頑張っています． ここではその仕組みを簡潔に説明します．code base自体はそこまで大きくないので， 私の説明が分かりづらかったらcodeを読んでいただければと思います． ","date":"2022-04-10","objectID":"/ja/posts/parsecomb/:2:0","series":null,"tags":["go","parser"],"title":"Go GenericsでつくるParser Combinator","uri":"/ja/posts/parsecomb/#仕組み"},{"categories":null,"content":" 2.1 Parser Signatureまずはじめに，peachcomb内のすべてのparserは type Parser[E comparable, O parser.ParseOutput] を満たすようになっています． つまり，このsignatureを理解できればほぼ勝ったも同然です． 具体的には，以下のように定義されています． type Parser[E comparable, O ParseOutput] func(input ParseInput[E]) (ParseInput[E], O, ParseError) type ParseInput[E comparable] []E type ParseOutput interface{} type ParseError interface { error } 特に難しくないですね．現状parsecomb parsersは []E というsliceを受け取るに過ぎません． これは実装の妥協点の一つです．詳しくは後述します． このようにparser typeを定義する利点は数多く存在しますが， 特に利益を実感してもらいやすい点を説明します． まずはじめに，peachcombを利用するある時点において， もしpeachcombに機能が足りなくても， 自分で Parser[E, O] を満たすparserを作れば，それを Map() などの汎用関数に引き渡すことができます． ある良いparserを作ったら，それをpeachcombにcontributionしつつ，mergeされるまでは自分のprojectで定義したものを代用してもうまく動作します． 次に，ほとんどのparserをgenericに作ることができます．ただのhigher order functionですね． 先述したように， package combinator にあるparser関数はすべて Parser[E, O]を満たすすべてのparserを受け取る ことができます． これによって文字列やbyte列など，入力の種類ごとに Map() などを実装する必要がなくなります． package strparse や package byteparse にあるのは，それぞれの入力でのみ必要な特殊関数のみ置くようになっています． 最後に，userはparserの使い方を一貫して理解することができるようになっています．ただの関数なので． ","date":"2022-04-10","objectID":"/ja/posts/parsecomb/:2:1","series":null,"tags":["go","parser"],"title":"Go GenericsでつくるParser Combinator","uri":"/ja/posts/parsecomb/#parser-signature"},{"categories":null,"content":" 2.2 Type Resolving実際に型が解決される様子を見てみましょう． ここで，次のようなparserを構築します． わかりやすさのために型を明示していますが，明示しなくてももちろん使えます． ちなみにGo Playgroundを用意したので，そちらで実行することもできます． https://go.dev/play/p/oiZCn732MOh package main import ( \"fmt\" \"github.com/Drumato/peachcomb/pkg/combinator\" \"github.com/Drumato/peachcomb/pkg/parser\" \"github.com/Drumato/peachcomb/pkg/strparse\" ) func main() { var element parser.Parser[rune, string] = strparse.Digit1() var separator parser.Parser[rune, rune] = strparse.Rune('|') var p parser.Parser[rune, []string] = combinator.Separated1(element, separator) var i []rune var o []string var err error i, o, err = p([]rune(\"123|456|789Drumato\")) fmt.Println(string(i)) fmt.Printf(\"%d\\n\", len(o)) fmt.Printf(\"%s %s %s\\n\", o[0], o[1], o[2]) fmt.Println(err) } それぞれ， element … []rune を受け取り string を成果物とするparser separator … []rune を受け取り rune を成果物とするparser p … []rune を受け取り []string を成果物とするparser という感じです． ではこれに対し，genericなfunction signatureを照らし合わせて考えてみましょう． func Digit1() parser.Parser[rune, string] func Rune(expected rune) parser.Parser[rune, rune] func Separated1[ E comparable, EO parser.ParseOutput, SO parser.ParseOutput, ]( element parser.Parser[E, EO], separator parser.Parser[E, SO]) parser.Parser[E, []EO] Digit1() によって element は []rune を入力し string を返すものとわかり， Rune() によって separator は []rune を入力し rune を返すものだとわかります． これによって Separated1() のtype parameterは次のようになります． E … rune EO … string SO … rune Separated1() は SO 型の成果物をparser内部で捨てて， []EO を構築します． ということで， p が無事 parser.Parser[rune, []string] を満たすことがcompile時にわかります． 次に，compileが失敗する例を紹介します． 実用的には型を明示しないで使いたいので，次は推論させつつ使ってみます． 実行時ではなくcompile timeで警告されるのがGenericsを使用している最大の利点 なので， ここは丁寧に説明したいと思います． https://go.dev/play/p/J0pRsPk4_Pf package main import ( \"fmt\" \"github.com/Drumato/peachcomb/pkg/byteparse\" \"github.com/Drumato/peachcomb/pkg/combinator\" ) func main() { sub := byteparse.UInt8() p := combinator.Many1(sub) i, o, err := p([]rune(\"aaaabaa\")) fmt.Println(string(i)) fmt.Println(string(o)) fmt.Println(err) } pkg devを見ていただければわかりますが，それぞれのparserは次のように解決されます． sub … parser.Parser[byte, uint8] p … parser.Parser[byte, []uint8] Go Playgroundで実行してみるとわかりますが，これはcompileが失敗します． error messageを以下に示します． 内容を要約すると， p.Parse() は []byte を要求するが， []rune を受け取っている，というものです． ./prog.go:13:23: cannot use []rune(\"aaaabaa\") (value of type []rune) as type parser.ParseInput[byte] in argument to p 今回は入力が間違っている例を紹介しましたが，parser間に不整合がある場合も同様にcompile errorが発生します． https://go.dev/play/p/Zssq5aSvheM package main import ( \"fmt\" \"github.com/Drumato/peachcomb/pkg/combinator\" \"github.com/Drumato/peachcomb/pkg/strparse\" ) func main() { sub := strparse.Digit1() p := combinator.Map(sub, func(v byte) (bool, error) { return v == 0, nil }) i, o, err := p([]byte(\"11112222abc\")) fmt.Println(string(i)) fmt.Println(o) fmt.Println(err) } 実行しようとすると，以下のようなcompile errorが起こります． ./prog.go:12:27: type func(v byte) (bool, error) of func(v byte) (bool, error) {…} does not match inferred type func(string) (O, error) for func(SO) (O, error) めちゃくちゃ長くて読みづらいですが，これも簡単にまとめると， combinator.Map() が推論した型と func(v byte) (bool, error) の内容が一致しません，的なあれです． まず，例によってparser関数は以下のように解決されます． sub … parser.Parser[rune, string] p … parser.Parser[rune, bool] Map() は簡略化すると Map[E, SO, O](sub Parser[E, SO], fn SO -\u003e (O, error)) です ここで Map() は E: rune, SO: string から， func (v string) -\u003e (O, error) なfn literalを求めますが， 実際には func(v byte) -\u003e (bool, error) が来たので怒った．という感じです． ","date":"2022-04-10","objectID":"/ja/posts/parsecomb/:2:2","series":null,"tags":["go","parser"],"title":"Go GenericsでつくるParser Combinator","uri":"/ja/posts/parsecomb/#type-resolving"},{"categories":null,"content":" 2.3 Custom Input Typesuser interfaceに注力した結果， []rune や []byte などの想定された入力にかぎらず，ある制約を満たしたいかなる型も受け入れられるようになりました． これによって， []myToken のような独自の型を入力できるようになります． 実際にcustom input typeを扱うexampleを用意しているので，よろしければそちらをご覧ください． https://github.com/Drumato/peachcomb/blob/v0.2.0/examples/custominput/main.go compiler開発などではよくあるのですが，tokenizerが各tokenにsource code上の位置情報を付与して， errorを詳細にする，ということがあります． この場合，普通に []rune を使ったり []byte の入力を受け取るとそれらを扱うことができません． このようなneedsを満たすためにも，interfaceを公開するmotivationが存在します． ちなみに, peachcombでは []rune や []byte を受け取るようにしていても， positionを保持する構造体のmethod内でそれぞれpeachcombのparserを呼ぶようにすれば，custom input typeを使用しなくても位置情報を取り回すことはできます． ","date":"2022-04-10","objectID":"/ja/posts/parsecomb/:2:3","series":null,"tags":["go","parser"],"title":"Go GenericsでつくるParser Combinator","uri":"/ja/posts/parsecomb/#custom-input-types"},{"categories":null,"content":" 3 実装で妥協した点ここでは今後やりたいことや，現在の実装で妥協している点をお話します． ","date":"2022-04-10","objectID":"/ja/posts/parsecomb/:3:0","series":null,"tags":["go","parser"],"title":"Go GenericsでつくるParser Combinator","uri":"/ja/posts/parsecomb/#実装で妥協した点"},{"categories":null,"content":" 3.1 string を受け取れないGoでは，strings などによって string に対する便利関数がたくさん提供されています． また，userからも直感的に扱えるので，こちらとしてはぜひ string を入力できるようにしたいところです． 実際，実装当初はそれを想定して開発していました． super genericに実装しようとしたところ，string と rune の親子関係( string は rune の集合と考えることができる)を表現する必要が出てきます． これは， Satisfy() などが入力の先頭を切り出す操作を実行するときに切り出されたものの型を把握する必要があるためです． しかし，Goは string と rune に親子関係を見出すようにはなっていません． よって parser.Parser にその対応関係を外部から渡すようにしたいのですが，こうなると煩雑になるだけでなく，以下のような問題が起こります． Goでは string に対するindex accessを行うと byte が切り出されます． つまり明示的に []E() のようなcastを行って []rune として切り出すなどの仕組みが必要です． これは私があと少しGo Genericsに詳しくなったり，頭を捻れば解決できそうな問題なので，とりあえず []rune を受け取ることで妥協しました． いずれ対応したいと思っているので，以下のようなissueを立てています． https://github.com/Drumato/peachcomb/issues/7 ","date":"2022-04-10","objectID":"/ja/posts/parsecomb/:3:1","series":null,"tags":["go","parser"],"title":"Go GenericsでつくるParser Combinator","uri":"/ja/posts/parsecomb/#string-を受け取れない"},{"categories":null,"content":" 4 終わりに今回はGo Genericsを使って遊んでいたら意外と使えそうなものができているかも，みたいな話をしました． 新しい言語機能に触ることで知識のupdateが発生し，最新の技術にもついていけるようになるので，積極的に取り組んでいきたいところです． 一方，言語自体も色々触ることで見えてくるものがあるので，RustとGoだけしか書かない私ですが，最近はNimを触っています． ここらへんの話題は先日投稿したpodcastでも触れているので，よかったらお聞きください． https://youtu.be/mwW8i0pHaAU ","date":"2022-04-10","objectID":"/ja/posts/parsecomb/:4:0","series":null,"tags":["go","parser"],"title":"Go GenericsでつくるParser Combinator","uri":"/ja/posts/parsecomb/#終わりに"},{"categories":null,"content":"私は現在趣味で FRRouting の開発に手を出しており， ちょこちょこ遊んだり，contribution chanceを狙っているのですが(mainstream mergeは2回のみ経験)， その中でも現在取り組んでいるSRv6 ManagerのYANG backend対応です． これはNorthbound APIという興味深い仕組みに関わるものであり，とてもやりごたえがあるtaskです． 日々ウンウン唸りながら頑張ってcodingしているので，その内容をまとめておこうと思います． 本実装の内容はまだ不完全であり，実装方針のmemoみたいな側面が大きいですが， ともかく何かしら知見になるかもしれないので． 本記事が対象とするPoCは こちら に． ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:0:0","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#"},{"categories":null,"content":" 1 前提知識ここではごく簡単に事前知識を共有します． ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:1:0","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#前提知識"},{"categories":null,"content":" 1.1 FRR Zebra SRv6 Managerまず，FRRではZebra daemonがSRv6 locator等の資源を管理しており， bgpdやisisdなどのrouting daemonはzebraに問い合わせることでそれら資源の使用権を受け取り， それらを広報などに使用する，というような “server - client” model を採用しています． このうち前者，Zebraに存在するSRv6資源の管理者を SRv6 Manager と呼びます． FRR repoには BGP SRv6でVPNv6を構築するtopotest が置いてあるので， その内容を読んでみると仕組みがわかりやすいかなと思います． ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:1:1","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#frr-zebra-srv6-manager"},{"categories":null,"content":" 1.2 Northbound gRPCFRRoutingで最もpopularかつ手軽に扱えるdynamic configuration interfaceにvtyshがあります． これはCisco routerなどで使用できるInteractive CLIとほぼ同等の機能を提供するものです． 一方，FRRは Northbound gRPC と呼ばれる， gRPC communicationによるNorthbound APIの提供を実装しています． userはgRPC clientを実装して使用することで，それぞれのrouting daemonに対するRPCを発行でき， programmableにnetwork configurationを行える，というものです． 詳細は こちらのdocument や こちら を御覧ください． ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:1:2","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#northbound-grpc"},{"categories":null,"content":" 1.3 FRR YANG backend先述したNorthbound gRPCですが，FRRではこれを以下のようにして実現しています． YANG でFRRが扱う資源をmodeling 上記Data Modelに Northbound API Callbacks を紐付ける このように，YANG data storeを中心とした実装にする利点はいくつか存在します． まず1つ目に，operatorはYANG fileを参照するだけで “何がreadonlyで，何がoperationalで” というのを確認できます． YANGは特定のprogramming language, 及び実装に依存しないので，network engineerなら誰でも理解することができます． 2つ目に，Ciscoなどのnetwork vendorはそれぞれの製品が使用する YANG modelを公開 しています． 実装の中心にYANGを置き，それをnetwork vendorとcompatibilityがあるように整備すれば， できるだけcisco routerなどと同じように扱うことができます． ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:1:3","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#frr-yang-backend"},{"categories":null,"content":" 2 本題","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:2:0","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#本題"},{"categories":null,"content":" 2.1 現状先述したようにFRRのdaemonをYANG backendに置き換えることには大きなmotivationがあり， FRR communityでも2019年頃から率先して置き換えよう という動きがありました． 本記事の内容からは離れますが， mgmtdという新たなplatformを導入しようという動き もあります． そこで私は，このYANG backendの実装を通じてFRRについて詳しくなろうと思いました． 先述したZebra SRv6 Managerは現在YANG backendに非対応であり，従来のprimitiveな方法で実装されていますが， SRv6 Managerはその責任と仕事に反して小さな規模で実装されており，また読みやすく注意されています． 私はSRv6 Manager全体を大まかに読んだ経験もあるため，“土地勘\"もあるし，書き換えるイメージもなんとなくできていました． ということで，現在私はSRv6 ManagerをYANG backendに置き換えようと取り組んでいます． 最終的に，mainstreamへのmergeを行うかはとりあえずおいておいて， 私がFRRを勉強したり，それについての知見を共有できるところを目指します． ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:2:1","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#現状"},{"categories":null,"content":" 2.2 目標ここまでの状況をもとに，本taskの目標を整理してみます． それぞれのcomponentをpracticalに置き換えること daemon side SRv6 Manager vtysh command config management もちろんSRv6 Managerとしての機能は損なわないこと topotestsで使われているSRv6 Managerの機能は動作すること ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:2:2","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#目標"},{"categories":null,"content":" 3 現段階での実装ここからは実際に，どのようにしてこれを実現しているかを解説していきます． 本taskのPoCは，以下3つのsub-taskに分けて考える事ができます． YANG fileの定義 YANG data nodeに対応するcallbackの定義，実装 vtyshの実装変更 現在は，srv6_locatorというSRv6 Managerの機能に関するtopotestが存在し， それがうまく動くまで実装できています． ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:3:0","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#現段階での実装"},{"categories":null,"content":" 3.1 YANG fileの定義まずはじめに，data modelとなるYANG fileの定義を行います． このfileはdaemon/vtyshの実装の基点となるため，慎重に設計する必要があり，これは簡単ではありません． そこで現在は，IOS-XR 7.5.1のYANGを参照して設計しています． CiscoやJuniperなどのnetwork vendorは使用しているyangを公開しており， それを参考にすることでwell-definedなYANGを設計することができます． 例えば，CiscoのYANGは こちら にあります． SRv6に関するyangを探してみると，大別して以下の種類が存在することがわかります． このうち，まずは cfg と datatypes に絞って考えることにします． Cisco-IOS-XR-segment-routing-srv6-oper.yang Cisco-IOS-XR-segment-routing-srv6-datatypes.yang Cisco-IOS-XR-segment-routing-srv6-cfg.yang さて，このyangを読んでみると， segment-routing-srv6-cfg:srv6 というcontainer blockは segment-routing-ms-cfg:sr をaugmentする形で作られています． これはSegment Routing自体のdata modelであり，そのsub treeとしてsrv6 blockが付属するようになっているのです． これを踏襲して，本PoCでは以下のようなyangを用意します． 現状，FRRのZebraにSegment Routing自体の統一的な基盤はありませんが(後述)，それはひとまず考えないことにします． frr-srv6.yang … segment-routing-srv6-datatypes 相当のyang frr-zebra-sr.yang … segment-routing-ms-cfg 相当のyang SRv6 Managerとはあまり関係ないが，今後必要になるかもしれない frr-zebra-srv6.yang … segment-routing-srv6-cfg 相当のyang frr-zebra-sr をaugmentする yangの内容をすべて取り上げるのは大変なので，PoCをご覧いただければと思います． 後々，vtyshの実装を紹介する際にxpathの内容を見るので，そこでなんとなく感覚がつかめると思います． ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:3:1","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#yang-fileの定義"},{"categories":null,"content":" 3.2 Northbound API CallbacksYANGでSRv6に関するdata modelを記述したあとは，それぞれのdata nodeに対してNorthbound API callbacksを実装する必要があります． 例えばisisdでは， router isis AREA_TAG として新しいisis instanceを作成しますが， これに対応してYANG list-nodeのentryが新しく作成され，data storeに保存されるようになっています． これはFRR内で NB_OP_CREATE と呼ばれるAPI callによって行われ， また， create と呼ばれるcallbackが紐付いて呼ばれるようになっています． ここでは簡単にNorthbound APIで使用されるCallback(の一部)について解説しておきます． Callback Type Description create Configuration list-node entry/type empty/leaf-list entryの作成時に呼ばれる modify Configuration leaf-node valueが変更される際に呼ばれる destroy Configuration あるlist-node entry/leaf-list entry/etcが削除される際に呼ばれる get_elem Operation あるleaf/leaf-list entry/etcを取得するcallbackで，operational-dataを取得する際に呼ばれる lookup_entry Operation あるlist-nodeについて，あるkeyを持つentryを探索する cli_show Operation あるnodeに対応するCLI commandを出力する cli_show_end Operation container/listnodeなどはCLI command blockを生成するため，そのterminationを出力する これらcallbackを，先述したYANGに対して定義する，というのが次のtaskです． まずは struct frr_yang_module_info という構造体を定義して， daemonのinstantiation( FRR_DAEMON_INFO というmacroで行われます )時に引き渡すという実装を行います． https://github.com/Drumato/frr/blob/a41251800b09b9b93726a18fb891127a3e10340b/zebra/zebra_srv6_nb.c#L31 /* stripped */ const struct frr_yang_module_info frr_zebra_srv6_info = { .name = \"frr-zebra-srv6\", .nodes = { { .xpath = \"/frr-zebra-sr:sr/frr-zebra-srv6:srv6\", .cbs = { .cli_show = cli_show_segment_routing_srv6, .cli_show_end = cli_show_segment_routing_srv6_end, }, .priority = NB_DFLT_PRIORITY - 1, }, { .xpath = \"/frr-zebra-sr:sr/frr-zebra-srv6:srv6/locators\", .cbs = { .cli_show = cli_show_srv6_locators, .cli_show_end = cli_show_srv6_locators_end, }, }, { .xpath = \"/frr-zebra-sr:sr/frr-zebra-srv6:srv6/locators/locators\", .cbs = { .cli_show = cli_show_srv6_locators_locators, .cli_show_end = cli_show_srv6_locators_locators_end, }, }, { .xpath = \"/frr-zebra-sr:sr/frr-zebra-srv6:srv6/locators/locators/locator\", .cbs = { .cli_show = cli_show_srv6_locator, .cli_show_end = cli_show_srv6_locator_end, .create = nb_lib_srv6_locator_create, .destroy = nb_lib_srv6_locator_destroy, }, }, /* stripped */ } }; 同じようにして struct frr_yang_module_info frr_zebra_sr_info も定義します． あとは，ここで指定したcallbacksを地道に頑張って実装していくだけです． まずは， nb_lib_srv6_locator_create をご紹介します． これは先述した create callbackであり， 新しいSRv6 locatorが作成された際に呼び出されます． 以下に示す，callbackの中身について解説します． まず，FRRではいくつかの理由から， YANG data treeとそれに対応する状態を管理するC data の2つを管理しています． YANGに対するlist-node appendは自動的に行われますが， ここでは zebra_srv6_locator_add() を呼ぶことで，SRv6 Managerが管理するmaster変数を更新しています． 次に，このcallbackが呼び出された時点で args には対応するdnodeが格納されています． また，API clientからnameが渡されているので(後述)， それをもとにSRv6 Manager側の関数を呼び出してあげて初期化します． また， nb_running_set_entry() という関数を呼び出します． これによって，以後 list locator のchild nodeに対する modify callback等では， lookup等を呼び出さずに 親nodeに対応する struct srv6_locator を引っ張ってこれます． https://github.com/Drumato/frr/blob/a41251800b09b9b93726a18fb891127a3e10340b/zebra/zebra_srv6_nb_config.c#L29 // stripped /* * XPath: /frr-zebra-sr:sr/frr-zebra-srv6:srv6/locators/locators/locator */ int nb_lib_srv6_locator_create(struct nb_cb_create_args *args) { struct srv6_locator *loc; struct srv6_locator_chunk *chunk; const char *loc_name; if (args-\u003eevent != NB_EV_APPLY) return NB_OK; loc_name = yang_dnode_get_string(args-\u003ednode, \"./name\"); loc = zebra_srv6_locator_lookup(loc_name); if (!loc) { /* SRv6 manager pre-allocates one chunk for zclients */ loc = srv6_locator_alloc(loc_name); chunk = srv6_locator_chunk_alloc(); chunk-\u003eproto = NO_PROTO; listnode_add(loc-\u003echunks, chunk); } zebra_srv6_locator_add(loc); nb_running_set_entry(args-\u003ednode, loc); return NB_OK; } // stripped 続いてlocator prefixを変更する nb_lib_srv6_locator_prefix_modify をご紹介します． これは list locator が管理する leaf prefix (厳密には container prefix を経由しています)を書き換える際に呼ばれる modify callbackです． nb_running_get_entry() で対応する struct srv6_locator を引っ張ってきて， prefix を書き換えます． https://github.com/Drumato/frr/blob/a41251800b09b9b93726a18fb891127a3e10340b/zebra/zebra_srv6_nb_config.c#L124 // stripped /* * XPath: /frr-zebra-sr:sr/frr-zebra-srv6:srv6/locators/locators/locator/prefix/prefix */ int nb","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:3:2","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#northbound-api-callbacks"},{"categories":null,"content":" 3.3 SRv6 Manager CLIここまででdaemon側のNorthbound API対応はできているのですが， vtysh側の実装がdaemon側の関数を直接叩くように実装されているままなので，これを変更します． 具体的には，vtysh command側の実装を単なるNorthbound API callで実装することができます． まずは， srv6_locator_cmd という， locator config modeに遷移するcommandの実装です． まずxpathを構築しますが，このときに name を渡し，locatorの初期化時に使えるようにします． あとは nb_cli_enqueue_change() でAPI callをenqueueして， apply_changes() でこれを適用します． この際に， NB_OP_CREATE を指定するのがpointです． https://github.com/Drumato/frr/blob/a41251800b09b9b93726a18fb891127a3e10340b/zebra/zebra_srv6_vty.c#L246 // stripped DEFPY_YANG_NOSH(srv6_locator, srv6_locator_cmd, \"locator LOC_NAME$name\", SRV6_LOCATOR_CMD_STR) { char xpath[XPATH_MAXLEN]; int rv; snprintf(xpath, sizeof(xpath), \"/frr-zebra-sr:sr\" \"/frr-zebra-srv6:srv6\" \"/locators/locators/locator[name='%s']\", name); nb_cli_enqueue_change(vty, xpath, NB_OP_CREATE, NULL); rv = nb_cli_apply_changes(vty, xpath); if (rv == CMD_SUCCESS) VTY_PUSH_XPATH(SRV6_LOC_NODE, xpath); return rv; } // stripped 続いて， cli_show callbackの実装をご紹介します． struct lyd_node *node (FRRが使用するlibyang側の構造体) には対応するdata nodeが含まれています． https://github.com/Drumato/frr/blob/a41251800b09b9b93726a18fb891127a3e10340b/zebra/zebra_srv6_vty.c#L405 // stripped void cli_show_srv6_locator(struct vty *vty, const struct lyd_node *dnode, bool show_defaults) { const char *loc_name = NULL; loc_name = yang_dnode_get_string(dnode, \"./name\"); vty_out(vty, \" locator %s\\n\", loc_name); } // stripped このようにcli_showを地道に実装していくと，Zebra SRv6に関するconfigがcli_showですべて置換できるようになります． 詳細は省略しますが，vtyshで show running-config を実行したときに zebra_sr_config() という関数が呼ばれます． isisdと同じように，この関数もすべて cli_show で置換します． FRRではすでに nb_cli_show_dnode_cmds() という，再帰的にdnodeのcli_showを呼び出してくれる便利な関数があります． これを使用して，簡潔に記述することができます． https://github.com/Drumato/frr/blob/a41251800b09b9b93726a18fb891127a3e10340b/zebra/zebra_srv6_vty.c#L453 // stripped static int zebra_sr_config(struct vty *vty) { int write_count = 0; struct lyd_node *dnode; if (zebra_srv6_is_enable()) { dnode = yang_dnode_get(running_config-\u003ednode, \"/frr-zebra-sr:sr\" \"/frr-zebra-srv6:srv6\"); if (dnode) { nb_cli_show_dnode_cmds(vty, dnode, false); write_count++; } } return write_count; } // stripped ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:3:3","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#srv6-manager-cli"},{"categories":null,"content":" 4 今後取り組むべきことここまででsrv6_locator topotestが動作するようになったのですが， 実際にNorthbound API backendとしてほしい機能はまだ存在します． よって，ここからはそれについて解説します． また，発展的話題として，先述したmgmtdについても少しだけ触れることにします． ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:4:0","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#今後取り組むべきこと"},{"categories":null,"content":" 4.1 segment-routing blockの扱い現状FRRでSRv6 locatorを定義する際には，以下のように指定します(ref: https://github.com/FRRouting/frr/blob/master/tests/topotests/srv6_locator/r1/zebra.conf#L11 )． segment-routing srv6 locators locator loc1 prefix 2001:db8:1:1::/64 exit exit exit exit 一方，Cisco CLIは先述したものとは異なります(ref: https://www.cisco.com/c/en/us/td/docs/routers/asr9000/software/asr9k-r7-3/segment-routing/configuration/guide/b-segment-routing-cg-asr9000-73x/m-configure-srv6-usid.html?referring_site=RE\u0026pos=1\u0026page=https://www.cisco.com/c/en/us/td/docs/routers/asr9000/software/asr9k-r6-6/segment-routing/configuration/guide/b-segment-routing-cg-asr9000-66x/b-segment-routing-cg-asr9000-66x_chapter_011.html#Cisco_Concept.dita_9cdec09b-6edf-4bb8-8137-6d546bfe0093 )． segment-routing srv6 locators locator loc1 prefix 2001:db8:1:1::/64 exit exit exit この非一貫性を解消する，というtaskがあります． FRRではpathdとZebra SRv6 Managerの双方で SEGMENT_ROUITNG_NODE ( segment-routing block) を定義しており， またconfigure時に --enable-pathd を入力しないとZebra SRv6 vtysh commandsが動作しないという問題があります． これを防ぐためには segment-routing srv6 をうまく定義する必要がありますが， 私の調査ではこの実装にはいくつかの落とし穴があり，簡単ではありません． しかしCisco CLIと同様の運用体験を実現するためには必要なpatchだと考えています． ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:4:1","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#segment-routing-blockの扱い"},{"categories":null,"content":" 4.2 show yang operational-dataここまでの実装はNorthbound API callbacksにおける configuration callbackに限定されており， operational callbackは考慮されていません． これによって， show yang operational-data などのcommandでSRv6資源の情報を取得したりすることはできません． これは，PythonやGoなどでgrpc clientを実装してFRRにinteractした場合も同様です． これを実現するためには get_keys/get_next/get_elem/lookup_entry 等のcallbackに対応する必要があります． ここまでもそうだったのですが，依然として網羅的なdocumentは存在せず，FRRの実装を深く読み込んで理解しなければ動作させることはできません． ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:4:2","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#show-yang-operational-data"},{"categories":null,"content":" 4.3 mgmtdについての調査と検証先述したように，YANG basedにFRR managementを行うことに責任を持つmgmtdというdaemonが提案されています． このdaemonは以下の機能を持ち，FRRの開発/運用体験を高めるという意味で非常に期待しています． API client等に対するfrontend interfaceの提供 running/candidate/startup datastoreが明確になり，わかりやすいAPIが定義される すべてのFRR daemonに対するbackend interfaceの提供 それぞれはmgmtdに対する問い合わせによってconfig/dataを操作する これによりdaemonごとに異なる実装が減る Candidate Config CommitのRollback/History機能 mgmtdはFRR全体を巻き込む大きな変更を伴いますが(ただし段階的にmigrationはできそう)， これによって現在私が抱えている多くの実装上の問題が解消されるかもしれません． なのでmgmtdの実現が待ち遠しいですし，積極的にcontributionしたいとも思っています． 初のmgmtd backend clientとしてstaticdが選ばれていますが， これをbgpd含む他のrouting daemonに対応する，という仕事にはすごい魅力を感じます． ぜひやってみたいですね． ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:4:3","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#mgmtdについての調査と検証"},{"categories":null,"content":" 5 まとめ今回は，FRRoutingを改造して遊んでいる様子をご紹介しました． このように巨大で実績のあるOSSのcodingは魅力がありますし， 迅速な理解と環境構築，coding力など様々なskillを要求されるのはとても楽しいです． しかし，FRRoutingへのcontributionはまだまだ足りないので，もっと頑張りたい． できれば次のcontributionは，ちゃんとしたNetworking featureの実装をやりたいですね． ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:5:0","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#まとめ"},{"categories":null,"content":" 6 参考資料 http://docs.frrouting.org/en/latest/grpc.html http://docs.frrouting.org/projects/dev-guide/en/latest/grpc.html 【インターンレポート】FRRouting IS-IS SRv6 Extension 設計と実装に関して zebra: srv6 manager ","date":"2022-01-04","objectID":"/ja/posts/srv6-manager/:6:0","series":null,"tags":["frr","srv6"],"title":"Replace FRR Zebra SRv6 Manager with YANG Backend","uri":"/ja/posts/srv6-manager/#参考資料"},{"categories":null,"content":"この記事は IPFactory Advent Calendar 2021 の11日目です． 私がIPFactoryとして活動するのは今年度が最後なので，何かしら技術的知見が残せればと思って執筆しています． ご存知の通り，Kubernetesはたくさんの拡張性をuserに提供しています． これは 公式document でも紹介されています． Extensibility Description Custom Controller 独自にresource reconcilerを記述できる仕組み CRD OpenAPI Schemaをもとに，新たなresourceを定義できるような機能で，CRD自体が組み込みresource Admission Webhook API request時にValidation/Mutationを行えるようなWebhook Serverを建てられる仕組み Kubernetes Scheduler Plugin NodeのScoring/Filtering algorithmに影響を与え，Pod Schedulingの挙動を変更する機能 CNI Plugin flannelやCalicoに代表される，Container Networkingを実現するためのPluggable機構 これと同じように，Kubernetes運用者のほとんどが使用する kubectl でも拡張性が提供されています． それを kubectl plugin といい，それを開発/利用することで運用を効率化できます． 本記事ではこのkubectl pluginについて紹介しつつ， kubectl plugin開発に関連する話題を取り上げて， 最終的に私が開発しているcode generatorを解説します． ","date":"2021-12-06","objectID":"/ja/posts/kubectl-plugin-builder/:0:0","series":null,"tags":["kubernetes","go","kubectl"],"title":"Kubectl Plugin Builder","uri":"/ja/posts/kubectl-plugin-builder/#"},{"categories":null,"content":" 1 Background","date":"2021-12-06","objectID":"/ja/posts/kubectl-plugin-builder/:1:0","series":null,"tags":["kubernetes","go","kubectl"],"title":"Kubectl Plugin Builder","uri":"/ja/posts/kubectl-plugin-builder/#background"},{"categories":null,"content":" 1.1 kubectl pluginとはここではkubectl pluginについて復習します． kubectl pluginとはその実ただの実行形式です． kubectl本体が認識できるpathに置かれ， kubectl-* という命名がされていればkubectl pluginとして扱われます． 公式documentではShell Scriptで実装する例が紹介されています． kubectl pluginの利点はいくつかありますが，Kubernetes運用者にとって，kubectl本体のcommandと自作のoperation toolを統一的に扱えるのは非常に便利です． kubectl plugin list でどのようなpluginがinstallされているか確認することもできます． 著名なkubectl pluginの一つに，postfinance/kubectl-ns があります． kubernetes/sample-cli-pluginの題材でもありますし， awesome-kubectl-pluginsでも紹介されています． kubeconfigにはcontextを埋め込めるfieldが存在しますが， そのうちnamespaceの情報を簡単に扱うためのpluginです． kubectl-nsは多くのことを成し遂げないtoolに見えますが， 個人的には， 小さな仕事を実現するpluginを組み合わせる という作り方がとても良いと思っています． この理由は後述します． ","date":"2021-12-06","objectID":"/ja/posts/kubectl-plugin-builder/:1:1","series":null,"tags":["kubernetes","go","kubectl"],"title":"Kubectl Plugin Builder","uri":"/ja/posts/kubectl-plugin-builder/#kubectl-pluginとは"},{"categories":null,"content":" 1.2 kubectl pluginの作り方先程述べたように，kubectl plugin自体はただのexecutableであるため， shell scriptやPythonにGoなど，特定の言語に限らず実装することができます． よってここでは，私が考える kubectl pluginをうまく実装する方法 にfocusしたいと思います． 私はGoで，かつ spf13/cobra などのCLI application builderを使用して開発するのを強くおすすめします． 第一に，Kubernetesの運用者にとっての最も大きな関心は Kubernetesの運用を簡単に便利にする というものであり， それをどのように構築するかについてはあんまりcostを割きたくないからです．これは 本当に小さなpluginはshell script等でサクッと作るべき という主張にも見えますが，どちらかというと 小さくても，scaleしても管理しやすい言語でやったほうが良い ということを意味しています． この発想から， 小さなpluginを組み合わせる 方法の利点も見えてくると思います． 第二に，GoはKubernetes Ecosystemのほとんどすべてが採用している言語であり， Kubernetes Engineerにとって親しい言語だと言えるからです． kube-apiserverやkube-scheduler, kubectl本体などのcore componentなどもGoで書かれています． operation toolであると考えたとき，新しくteamに入ってきたmemberがすぐに使えるほうが便利です． これは，その分野でmainstreamとなっている言語で開発する利点を活かした形です． 最後に，kubectl pluginのほとんどが実際にGoで開発されており， 更にそれら殆どがcobraを使用している，という点です． kubectl pluginは case by caseで必要なものが異なる という点から実例を起点とした文献がほとんどですが， 多くの実装は公開されているため，それらを参照して書くということがやりやすくなります． ","date":"2021-12-06","objectID":"/ja/posts/kubectl-plugin-builder/:1:2","series":null,"tags":["kubernetes","go","kubectl"],"title":"Kubectl Plugin Builder","uri":"/ja/posts/kubectl-plugin-builder/#kubectl-pluginの作り方"},{"categories":null,"content":" 1.3 Goでkubectl pluginを作ることで見えてくる問題いざGoでkubectl pluginを書こうとしたとき，いくつかのboilerplateが必要であることがわかります． client-goの初期化処理 cli-runtimeの初期化処理 -n/--namespace などに代表される汎用的なcli flagの利用 Complete/Validate/Run という，kubectl plugin implsで頻出するpractice これらは一度書くだけなら特に難しくないですが， やはり何度も書くと退屈な部分になってきますし， この書き方が微妙に異なることで素早く理解/改修できないと困ります． また，kubectl pluginも一般的にAPI clientを初期化して使用しますが， maintainabilityの高いpluginを開発するためにはいい感じにinterfaceを整備して， testableに開発する，みたいなことが必要になってきます． しかし，これをきれいに設計して，というのも一種のcostとして考えられます． ","date":"2021-12-06","objectID":"/ja/posts/kubectl-plugin-builder/:1:3","series":null,"tags":["kubernetes","go","kubectl"],"title":"Kubectl Plugin Builder","uri":"/ja/posts/kubectl-plugin-builder/#goでkubectl-pluginを作ることで見えてくる問題"},{"categories":null,"content":" 2 kubectl-plugin-builderそこで，私はkubebuilder(本記事では解説しません)の思想や実績を参考にして， kubectl pluginの開発をサクッと始められるものを作り始めました． kubebuilderほどKubernetes communityで認められるものにできるかはわかりませんが， 少なくともidea自体はだいぶ便利な自負があるので，これからも開発は継続していきます． 実装は GitHub においてあります． また，かんたんな使い方についてはDocumentを書いています． 主な機能は次のとおりです． project初期化機能 cli application architectureをyamlから宣言的に生成する機能 flag command alias yamlに新しいcommand definitionを追加する機能 pluginの出力formatを制御する機構 高々数k行の実装なのですぐ理解できると思いますし， 実装を読まなくても適当にcommand打って生成されたfile眺めてたらわかります． ","date":"2021-12-06","objectID":"/ja/posts/kubectl-plugin-builder/:2:0","series":null,"tags":["kubernetes","go","kubectl"],"title":"Kubectl Plugin Builder","uri":"/ja/posts/kubectl-plugin-builder/#kubectl-plugin-builder"},{"categories":null,"content":" 2.1 簡単な使い方まずは適当なdirectoryでprojectを初期化します． $ mkdir kubectl-demo \u0026\u0026 cd kubectl-demo $ kubectl-plugin-builder new github.com/Drumato/kubectl-demo Initialization Complete! Run `go mod tidy` to install third-party modules. するといくつかのfileが生成されます． kubectl-plugin-builder new 実行直後のprojectは以下のような構成になっています． $ tree\r.\r├── cli.yaml\r├── cmd\r│ └── kubectl-demo\r│ └── main.go\r├── go.mod\r├── internal\r│ └── cmd\r│ ├── demo\r│ │ ├── command.go\r│ │ └── handler.go\r│ └── node.go\r├── LICENSE\r└── Makefile\r5 directories, 8 file cli.yaml … pluginのCLI app architectureを定義するspec make generate(kubectl-plugin-builder generate) で使用される LICENSE … 現在はMITのみ対応している Makefile … 開発に便利なtaskを持つtask runner format … すべてのGo packageのformat test … すべてのGo packageのtest build … plugin build generate … 宣言的にGo filesを生成する install … pluginを INSTALL_DIR にinstallする(defaultだと /usr/bin) internal/cmd/node.go … CLINodeOptions interfaceを定義するfile plugin内のすべてのcommandがこのinterfaceを実装していることを仮定する internal/cmd/demo … root commandの定義 cmd/kubectl-demo/main.go … the plugin’s entrypoint もちろんこの段階でbuildすることができます． $ go mod tidy $ make \u003e /dev/null $ ./kubectl-demo -h Usage: demo [flags] Flags: -h, --help help for demo -o, --output string the command's output mode (default \"normal\") さて，それぞれのfileについて紹介します． まず cmd/kubectl-demo/main.go からです． // Code generated by kubectl-plugin-builder. package main import ( \"fmt\" \"github.com/Drumato/kubectl-demo/internal/cmd/demo\" \"os\" \"k8s.io/cli-runtime/pkg/genericclioptions\" ) func main() { streams := genericclioptions.IOStreams{ In: os.Stdin, Out: os.Stdout, ErrOut: os.Stderr, } if err := demo.NewCommand(\u0026streams).Execute(); err != nil { fmt.Fprintf(os.Stderr, \"ERROR: %+v\\n\", err) os.Exit(1) } } ここで genericclioptions.IOStreams のinstanceを渡します． これは各commandがtestを書く場合を想定して，I/O Captureのために渡している感じです． testの際には IOSTreams.Out に bytes.Buffer などを渡せば，出力結果をtestすることができます． 次に internal/cmd/node.go です． // Code generated by kubectl-plugin-builder. /* MIT License * * Copyright (c) 2021 you * * Permission is hereby granted, free of charge, to any person obtaining a copy * of this software and associated documentation files (the \"Software\"), to deal * in the Software without restriction, including without limitation the rights * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell * copies of the Software, and to permit persons to whom the Software is * furnished to do so, subject to the following conditions: * * The above copyright notice and this permission notice shall be included in all * copies or substantial portions of the Software. * * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE * SOFTWARE. */ package cmd import ( \"github.com/spf13/cobra\" ) type CLINodeOptions interface { Complete(cmd *cobra.Command, args []string) error Validate() error Run() error } type OutputMode = string const ( OutputModeNormal OutputMode = \"normal\" // OutputModeJSON // OutputModeYAML ) ここでは CLINodeOptions interfaceの定義と， OutputMode と呼ばれる，各commandの出力結果を制御するための型が出力されます． すべてのcommandがこのinterfaceを実装するようになっているので， 自動的に Complete/Validate/Run modelを踏襲することができる，というわけです． Code generated by kubectl-plugin-builder. と // Code generated by kubectl-plugin-builder; DO NOT EDIT. の区別があり， 前者の場合はuserによる更新を許容していて，後者は宣言的にreplaceされ続けます． 実際のcommand定義である internal/cmd/demo/command.go を見てみましょう． $ cat internal/cmd/demo/command.go // Code generated by kubectl-plugin-builder; DO NOT EDIT. /* MIT License * * Copyright (c) 2021 you * * Permission is hereby granted, free of charge, to any person obtaining a copy * of this software and associated documentation files (the \"Software\"), to deal * in the Software without res","date":"2021-12-06","objectID":"/ja/posts/kubectl-plugin-builder/:2:1","series":null,"tags":["kubernetes","go","kubectl"],"title":"Kubectl Plugin Builder","uri":"/ja/posts/kubectl-plugin-builder/#簡単な使い方"},{"categories":null,"content":" 2.2 今後の展望ここまでで基盤となるbuilder部分は作れたと思うので， あとはcmd argを自動でparseしてくれるようにしたり，client-go/pkg/clientset の初期化をしてくれたりという， 開発する上で便利な細々としたcode生成， そして tests/spec.yaml に書いた期待出力からそれをtestする internal/cmd/\u003cCMD_NAME\u003e/handler_test.go を自動生成するといった機能を作ろうと思っています． ","date":"2021-12-06","objectID":"/ja/posts/kubectl-plugin-builder/:2:2","series":null,"tags":["kubernetes","go","kubectl"],"title":"Kubectl Plugin Builder","uri":"/ja/posts/kubectl-plugin-builder/#今後の展望"},{"categories":null,"content":" 3 Conclusion本日はkubectl pluginについての関心事から紹介しつつ， 私が開発しているkubectl-plugin-builderを紹介しました． よろしかったらこれを使って遊んでみてください! ","date":"2021-12-06","objectID":"/ja/posts/kubectl-plugin-builder/:3:0","series":null,"tags":["kubernetes","go","kubectl"],"title":"Kubectl Plugin Builder","uri":"/ja/posts/kubectl-plugin-builder/#conclusion"},{"categories":null,"content":" 4 References Extend kubectl with plugins ","date":"2021-12-06","objectID":"/ja/posts/kubectl-plugin-builder/:4:0","series":null,"tags":["kubernetes","go","kubectl"],"title":"Kubectl Plugin Builder","uri":"/ja/posts/kubectl-plugin-builder/#references"},{"categories":null,"content":"BGPでは複数のneighborから同じprefixに対する経路を受け取ることがあります． それに対して bestpath selection というalgorithmを適用し， 実際にFIBにinstallする経路を決定します． ここではFRRを用いてそれを検証し， BGPのお気持ちを頑張って理解してみます． FRRのalgorithm はこちらに記載されています． 今回はこのうち3番，Local Route Checkまでを検証してみます． 恐らくですが全部のruleを検証するように，後々書きかえると思います． もっといいtopoに変更したり，ちゃんとpacapして出すと思います． それまでの暫定的な記事だと思っていただければ． 今回用いるtinet configは， こちら に置いてあります． 実行環境も載せておきます． ","date":"2021-10-04","objectID":"/ja/posts/bgpd-bestpath/:0:0","series":null,"tags":["bgp","frr"],"title":"FRR BGPdのbestpath selectionの3番までを動かして検証する","uri":"/ja/posts/bgpd-bestpath/#"},{"categories":null,"content":" 1 Weight checkまずは経路につけるweightを制御することでbestpathにどう影響が出るのか見ていきます． まずはequal weightで経路広報してみて， その後weightをつけるとどうなるか，というようにして見てみます． 次のようなnetworkを考えてみます． ここで，R2, R3からは redistribute connected を設定しています． configを見るとわかりますがprefix-listでroute filteringを行っているので， R2, R3は 10.0.0.0/16 , 10.0.1.0/24 のみ広報します． upperからdefault routeを広報し neighbor PEER default-originate を設定するというのもやってみたかったですが． 対応するtinet configはこちらに． +-----------------------------------+ | | | | | R1 | | AS65001 | | | | | | .1 .2 | +----------+-------------------+----+ | 10.0.0.0/16 | | | +---------------------+---+ +----------+---------------+ | .251 | | .252 | | R2 | | R3 | | AS65002 | | AS65003 | | | | | | | | | | .1 | | .2 | +-------------+-----------+ +---------+----------------+ | 10.0.1.0/24 | +--+--------------------------+------+ | .251 .252 | | | | C1 | | AS65004 | | | | | | | +------------------------------------+ 一例として，R1 configを見てみましょう． $ docker exec R1 vtysh -c 'sh run' Building configuration... Current configuration: ! frr version 8.0 frr defaults traditional hostname R1 log syslog informational no ipv6 forwarding service integrated-vtysh-config ! interface net1 ip address 10.0.0.1/16 ! router bgp 65001 bgp router-id 1.1.1.1 bgp bestpath as-path multipath-relax neighbor 10.0.0.251 remote-as 65002 neighbor 10.0.0.252 remote-as 65003 ! address-family ipv4 unicast neighbor 10.0.0.251 route-map RMAP_LOWER in neighbor 10.0.0.252 route-map RMAP_LOWER in exit-address-family ! ip prefix-list PLIST_LOWER seq 5 permit 10.0.1.0/24 ! route-map RMAP_LOWER permit 10 match ip address prefix-list PLIST_LOWER ! line vty ! end RMAP_LOWER は C1が接続するnetwork prefixだけをpermitし， R2, R3からはその経路しかもらわないようにfilteringします． R2, R3は実際には 10.0.0.0/16 にも接続していますが， 同じくR1も接続していてその経路はいらないので弾くイメージです． 今度はC1でrib/fibを見てみましょう． $ docker exec C1 vtysh -c 'sh bgp ipv4 unicast' BGP table version is 2, local router ID is 4.4.4.4, vrf id 0 Default local pref 100, local AS 65004 Status codes: s suppressed, d damped, h history, * valid, \u003e best, = multipath, i internal, r RIB-failure, S Stale, R Removed Nexthop codes: @NNN nexthop's vrf id, \u003c announce-nh-self Origin codes: i - IGP, e - EGP, ? - incomplete RPKI validation codes: V valid, I invalid, N Not found Network Next Hop Metric LocPrf Weight Path *= 10.0.0.0/16 10.0.1.2 0 0 65003 ? *\u003e 10.0.1.1 0 0 65002 ? Displayed 1 routes and 2 total paths $ docker exec C1 vtysh -c 'sh ip route' Codes: K - kernel route, C - connected, S - static, R - RIP, O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP, T - Table, v - VNC, V - VNC-Direct, A - Babel, F - PBR, f - OpenFabric, \u003e - selected route, * - FIB route, q - queued, r - rejected, b - backup t - trapped, o - offload failure B\u003e* 10.0.0.0/16 [20/0] via 10.0.1.1, net2, weight 1, 00:00:14 * via 10.0.1.2, net2, weight 1, 00:00:14 C\u003e* 10.0.1.0/24 is directly connected, net2, 00:00:16 いい感じにmultipathが広報されていますね． それでは本題です． C1のroute-mapで set weight をかけてみましょう． $ docker exec C1 vtysh -c 'sh run' Building configuration... Current configuration: ! frr version 8.0 frr defaults traditional hostname C1 log syslog informational no ipv6 forwarding service integrated-vtysh-config ! interface net2 ip address 10.0.1.254/24 ! router bgp 65004 bgp router-id 4.4.4.4 bgp bestpath as-path multipath-relax neighbor 10.0.1.1 remote-as 65002 neighbor 10.0.1.2 remote-as 65003 ! address-family ipv4 unicast neighbor 10.0.1.1 route-map RMAP_UPPER1 in neighbor 10.0.1.2 route-map RMAP_UPPER2 in exit-address-family ! ip prefix-list PLIST_UPPER seq 5 permit 10.0.0.0/16 ! route-map RMAP_UPPER1 permit 10 match ip address prefix-list PLIST_UPPER set weight 10 ! route-map RMAP_UPPER2 permit 10 match ip address prefix-list PLIST_UPPER set weight 20 ! line vty ! end この状態でC1のrib/fibを見ると変化を確認できます． $ docker exec C1 vtysh -c 'sh bgp ipv4 unicast' BGP table version is 2, local router ID is 4.4.4.4, vrf id 0 Default local pref 100, local AS 65004 Status codes: s suppressed, d damped, h history, * valid, \u003e best, = multipath, i internal, r RIB-failure, S Stale, R Removed Nexthop codes: ","date":"2021-10-04","objectID":"/ja/posts/bgpd-bestpath/:1:0","series":null,"tags":["bgp","frr"],"title":"FRR BGPdのbestpath selectionの3番までを動かして検証する","uri":"/ja/posts/bgpd-bestpath/#weight-check"},{"categories":null,"content":" 2 Local Preference Checkつづいてlocal preferenceです． これはiBGP内で用いられるpath attributeの LOCAL_PREF に関わってきます． ここで対象とするものは，先程のnetworkからほとんど変わっていません． loでpeeringする点くらい? loのaddressはIGPとかを使わずにstatic routeで横着しています． 対応するtinet configはこちらに． 記事では解説しませんが，repositoryにはequal-localpref.yaml も置いてあるので参考にしてください． 先ほどと同じくmultipathがselectされるだけですが． さて，ここではR2, R3のconfigから，C1に適用するroute-mapを見てみます． LOCAL_PREF path attributeはAS内のすべてのiBGP peerを流れるので， R2で設定するとそれがC1に届くまで維持されます． まず，R2では set local-preference 200 を設定しています． $ docker exec R2 vtysh -c 'sh route-map RMAP_UPPER' ZEBRA: route-map: RMAP_UPPER Invoked: 0 Optimization: enabled Processed Change: false permit, sequence 10 Invoked 0 Match clauses: ip address prefix-list PLIST_UPPER Set clauses: Call clause: Action: Exit routemap BGP: route-map: RMAP_UPPER Invoked: 12 Optimization: enabled Processed Change: false permit, sequence 10 Invoked 6 Match clauses: ip address prefix-list PLIST_UPPER Set clauses: local-preference 200 Call clause: Action: Exit routemap 次に，R3では set local-preference 400 を設定しています． $ docker exec R3 vtysh -c 'sh route-map RMAP_UPPER' ZEBRA: route-map: RMAP_UPPER Invoked: 0 Optimization: enabled Processed Change: false permit, sequence 10 Invoked 0 Match clauses: ip address prefix-list PLIST_UPPER Set clauses: Call clause: Action: Exit routemap BGP: route-map: RMAP_UPPER Invoked: 13 Optimization: enabled Processed Change: false permit, sequence 10 Invoked 7 Match clauses: ip address prefix-list PLIST_UPPER Set clauses: local-preference 400 Call clause: Action: Exit routemap この状態で 10.0.0.0/16 の経路が広報されるとき， UPDATE messageのLOCAL_PREF attributeが期待する値に書き換わり， C1でのbestpath selectionに影響を与えるはずです． $ docker exec C1 vtysh -c 'sh bgp ipv4 unicast' BGP table version is 1, local router ID is 4.4.4.4, vrf id 0 Default local pref 100, local AS 65002 Status codes: s suppressed, d damped, h history, * valid, \u003e best, = multipath, i internal, r RIB-failure, S Stale, R Removed Nexthop codes: @NNN nexthop's vrf id, \u003c announce-nh-self Origin codes: i - IGP, e - EGP, ? - incomplete RPKI validation codes: V valid, I invalid, N Not found Network Next Hop Metric LocPrf Weight Path *\u003ei10.0.0.0/16 10.0.255.3 0 400 0 ? * i 10.0.255.2 0 200 0 ? Displayed 1 routes and 2 total paths $ docker exec C1 vtysh -c 'sh ip route' Codes: K - kernel route, C - connected, S - static, R - RIP, O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP, T - Table, v - VNC, V - VNC-Direct, A - Babel, F - PBR, f - OpenFabric, \u003e - selected route, * - FIB route, q - queued, r - rejected, b - backup t - trapped, o - offload failure B\u003e 10.0.0.0/16 [200/0] via 10.0.255.3 (recursive), weight 1, 00:00:19 * via 10.0.1.2, net2, weight 1, 00:00:19 C\u003e* 10.0.1.0/24 is directly connected, net2, 00:00:21 S\u003e* 10.0.255.2/32 [1/0] via 10.0.1.1, net2, weight 1, 00:00:21 S\u003e* 10.0.255.3/32 [1/0] via 10.0.1.2, net2, weight 1, 00:00:21 C\u003e* 10.0.255.4/32 is directly connected, lo, 00:00:21 想定どおりの挙動ですね． LOCAL_PREF attributeに格納された値が大きい方をbestpathとして優先します． ところで NEXT_HOP attributeのrecursive lookupに全然詳しくないことがわかったので， これはいずれ勉強して記事にしなければ． ","date":"2021-10-04","objectID":"/ja/posts/bgpd-bestpath/:2:0","series":null,"tags":["bgp","frr"],"title":"FRR BGPdのbestpath selectionの3番までを動かして検証する","uri":"/ja/posts/bgpd-bestpath/#local-preference-check"},{"categories":null,"content":" 3 Local Route Check最後にLocal Route Checkですが， これはsimpleにstatic/aggregates/redistributed routeを優先する，というものです． 実はこのruleはすでに確認しています． Local Preference Check のR2でribを見てみましょう． $ docker exec R2 vtysh -c 'sh bgp ipv4 unicast' BGP table version is 3, local router ID is 2.2.2.2, vrf id 0 Default local pref 100, local AS 65002 Status codes: s suppressed, d damped, h history, * valid, \u003e best, = multipath, i internal, r RIB-failure, S Stale, R Removed Nexthop codes: @NNN nexthop's vrf id, \u003c announce-nh-self Origin codes: i - IGP, e - EGP, ? - incomplete RPKI validation codes: V valid, I invalid, N Not found Network Next Hop Metric LocPrf Weight Path * i10.0.0.0/16 10.0.255.3 0 400 0 ? *\u003e 0.0.0.0 0 32768 ? *\u003e 10.0.1.0/24 0.0.0.0 0 32768 ? *\u003e 10.0.255.2/32 0.0.0.0 0 32768 ? Displayed 3 routes and 4 total paths ここで，R2は 10.0.0.0/16 に関する2つの経路を持っていることがわかります． R3とiBGP peerを形成して，R3からもらった経路 そもそものconnected route bestpathに選ばれている経路を見ると， 上記でいう2つ目，connected routeが対応していそうですね． というのがLocal Route Checkの挙動です．simple. 逆にいうと， static routeよりも高いweight( FRRなら 32768 より上 ) でneighborを設定すれば， static routeよりもらってきた経路を優先することができるわけです． ","date":"2021-10-04","objectID":"/ja/posts/bgpd-bestpath/:3:0","series":null,"tags":["bgp","frr"],"title":"FRR BGPdのbestpath selectionの3番までを動かして検証する","uri":"/ja/posts/bgpd-bestpath/#local-route-check"},{"categories":null,"content":" 4 おわりにここでは簡単にFRRでBGPd bestpath selectionを感じながら色々動かしてみました． 他にも， AS_PATH Length Check Route Origin Check MED Check External Check IGP Cost Check を含む多くの項目が存在しますが， multipathよりも優先される項目は全部検証しておいても良いかな?と思っています． というか，BGP何もわかっていない… ","date":"2021-10-04","objectID":"/ja/posts/bgpd-bestpath/:4:0","series":null,"tags":["bgp","frr"],"title":"FRR BGPdのbestpath selectionの3番までを動かして検証する","uri":"/ja/posts/bgpd-bestpath/#おわりに"},{"categories":null,"content":"こんにちは． 2020年の4月から1年間，the 10th of Cybozu Labs Youthとして活動しました． その活動についてまとめたいと思います． 本日行われた成果発表会での発表資料はこちらに． labs youth活動で行ったことについては上記資料で説明しているので， ここではlabs youth活動の感想をつらつらと述べていきます． ","date":"2021-03-30","objectID":"/ja/posts/cybozu-labs-youth-10th/:0:0","series":null,"tags":["cybozu-labs-youth","rust"],"title":"Cybozu Labs Youth 10thとして一年間活動しました","uri":"/ja/posts/cybozu-labs-youth-10th/#"},{"categories":null,"content":" 1 Cybozu Labs Youthとは公式ページの文言をそのまま引用します． サイボウズ・ラボユースは、 世界に通用する日本の若手エンジニアの発掘と育成を目指すことを目的とし、 学生の若手クリエイターに研究開発の機会を提供する場として、 2011年3月31日に設立されました。 引用: https://labs.cybozu.co.jp/youth.html これについては多くの卒業生(先輩)方が既にブログを上げていらっしゃいますので， そちらをご覧いただければと思います． ","date":"2021-03-30","objectID":"/ja/posts/cybozu-labs-youth-10th/:1:0","series":null,"tags":["cybozu-labs-youth","rust"],"title":"Cybozu Labs Youth 10thとして一年間活動しました","uri":"/ja/posts/cybozu-labs-youth-10th/#cybozu-labs-youthとは"},{"categories":null,"content":" 2 採択当初の目標まず背景として， 私は専門学校2年生の時(ちょうど2年前)， SecHack365という人材育成プロジェクトに参加していました． それについては，SecHack365自体のレポと， 作った実装を解説する記事を上げています． 上記記事からわかるように， 初めてコンパイラ/アセンブラ/リンカを作った私ですが， 一年間の活動で感じたことは\"もっとちゃんと作りたい\"というものでした． というのも， コンパイラ(自作言語)は\"整数演算ができる手続き型言語\"をサポートするに過ぎなかった 言語機能はほぼなかった x86_64についての勉強しかできなかった おかげでx86_64 instructionについてはわずかに詳しくなった しかし，他アーキテクチャの勉強もしたくなった そもそもコンパイラの学術書には frontend/backendを分離する\"きれいなコンパイラ\" の設計法が書いてある 全部が一つのバイナリだった コンパイラ/アセンブラ/リンカ/ローダ/readelf/checksecが全て一つのバイナリで動く 正直中身はぐちゃぐちゃで，二度と触りたくないものになってしまった という問題点があるからです． そこで，labs youthの活動では次の目標を掲げました． 自作言語の機能をもっと充実させよう コンパイラは複数アーキテクチャに対応しよう アセンブラやリンカはそれぞれ別プロジェクトとしよう ELFに関するライブラリはそれまた分離しよう ここまでが，採択当初に掲げていた目標になります． ","date":"2021-03-30","objectID":"/ja/posts/cybozu-labs-youth-10th/:2:0","series":null,"tags":["cybozu-labs-youth","rust"],"title":"Cybozu Labs Youth 10thとして一年間活動しました","uri":"/ja/posts/cybozu-labs-youth-10th/#採択当初の目標"},{"categories":null,"content":" 3 活動を通じての感想","date":"2021-03-30","objectID":"/ja/posts/cybozu-labs-youth-10th/:3:0","series":null,"tags":["cybozu-labs-youth","rust"],"title":"Cybozu Labs Youth 10thとして一年間活動しました","uri":"/ja/posts/cybozu-labs-youth-10th/#活動を通じての感想"},{"categories":null,"content":" 3.1 ソフトウェアの設計は難しいラボユース活動で一番勉強になったポイントです． 今回作ったものを\"設計\"という観点から再分類してみます． コンパイラ … 先述するfrontend/backendの分離が単純に課題 elf-utilities, py-linkage … Rust/Pythonライブラリ 何を公開して，何を公開しないのか どうすれば\"汎用的\"になるのか 自分のプロジェクトから使いたい =\u003e 注意しないとユースケースに依存する実装になってしまう アセンブラ/リンカ/elfpeach … そのライブラリを使う側 どこまでの機能をライブラリに閉じ込めるのか，どこからはユーザ実装にするのか というように， ざっと並べても上記問題が見えてきます． 私にとって “汎用的なライブラリ” を作る経験は初めてだったので， “使いやすく，コード拡張がしやすい設計\"について考え続け，悩み続ける一年でした． そして，その答えが出ることはありませんでした． SecHack365参加当時よりもレベルの高いことに手を出している自覚はあったので， 強いやりがいを感じて作業していたものの， **“ドメイン知識の前にソフトウェアエンジニアリングの力が不足している”**感覚がありました． “設計が悪いかも\"と不安になってプロジェクト全体を削除し， また1k~2k行書いたぐらいで\"あれ，もしかして賢くないやり方してるかも\"と思って削除， また作り直す，みたいなことを繰り返していました． 結局，発表資料に書いてあるように， 本来やりたかった実装のいくつかを保留する結果となってしまったわけです． そして， “これSecHack365でも同じ過ちをしていたな\"と気づきました(遅い)． ","date":"2021-03-30","objectID":"/ja/posts/cybozu-labs-youth-10th/:3:1","series":null,"tags":["cybozu-labs-youth","rust"],"title":"Cybozu Labs Youth 10thとして一年間活動しました","uri":"/ja/posts/cybozu-labs-youth-10th/#ソフトウェアの設計は難しい"},{"categories":null,"content":" 3.2 やっぱりツールチェーン開発は楽しい発表資料に書いてあるとおり，私はコンパイラについての専門知識を十分に持っているわけではありません． コンパイラの最新論文を読んでいるわけでも， 一般的な最適化手法を論理的に説明でき，実装しているわけでもありません． アセンブラについても同様で， x64のすべての命令に対応しているわけではありません． リンカも全くLTOしないですし， 複数ファイルにも対応していません． でも，やっぱり楽しいです． 実行ファイルに関する全てを制御している感じがして， 考えることが多いですし，“実装が終わらない\"というところに楽しさがあると思っています． おわりに でも話しますが， 私は現在network分野についてspecialityを持つ為の勉強をやり始めています． 最近GoBGPの記事とか上げてるのでご存知かもしれませんが． もしかしたらこの成果物にコードを書き加えることがないかもしれないけど， また何かのタイミングでツールチェーンに関するプログラミングやりたいな，と思いました． ","date":"2021-03-30","objectID":"/ja/posts/cybozu-labs-youth-10th/:3:2","series":null,"tags":["cybozu-labs-youth","rust"],"title":"Cybozu Labs Youth 10thとして一年間活動しました","uri":"/ja/posts/cybozu-labs-youth-10th/#やっぱりツールチェーン開発は楽しい"},{"categories":null,"content":" 4 おわりに上記感想にある通り， “ソフトウェアエンジニアリング\"について深く考える一年になりました． 改めて，cybozu labsの皆さんにはお礼を述べておきます． 現在はnetwork developmentやprivate cloud designに興味を持っていて， また知識0の状態からコツコツ勉強する形になりますが， ほそぼそと頑張っています． ソフトウェアエンジニアリングについてはこの領域にも役立つと思っているので， 今回の経験を活かして勉強するつもりです． 言語処理系やツールチェーンについての活動は趣味レベルで，少し優先度は低くなってしまいますが， labs youthの経験は必ずどこかで活きると思っています． ","date":"2021-03-30","objectID":"/ja/posts/cybozu-labs-youth-10th/:4:0","series":null,"tags":["cybozu-labs-youth","rust"],"title":"Cybozu Labs Youth 10thとして一年間活動しました","uri":"/ja/posts/cybozu-labs-youth-10th/#おわりに"},{"categories":null,"content":"お久しぶりです． 最近は自作コンパイラや， YouTubeでのアウトプット活動 をやってたりしました． 現在インフラ部門での就職活動に取り組んでおり， 将来その分野で専門的に精進したいという思いから， Linuxに関する知識をもう一度整理しようと考えました． 今回はLinuxの機能でも特に中核を担う\"ファイル\"と， ファイルを扱う上でまず必要になる open(2) システムコールについてまとめます． 基本的にはユーザ視点のドキュメントになりますが， Linuxカーネルにあるシステムコールの実装をちょっと覗き見するまでの記事です． 詳解UNIXプログラミング等，書籍の内容も含みます． ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:0:0","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#"},{"categories":null,"content":" 1 ユーザから見るopen(2)","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:1:0","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#ユーザから見るopen2"},{"categories":null,"content":" 1.1 基本まずはmanの内容を引っ張り出してきます． #include \u003csys/types.h\u003e #include \u003csys/stat.h\u003e #include \u003cfcntl.h\u003e int open(const char *pathname, int flags); int open(const char *pathname, int flags, mode_t mode); int creat(const char *pathname, mode_t mode); int openat(int dirfd, const char *pathname, int flags); int openat(int dirfd, const char *pathname, int flags, mode_t mode); 引数にはファイルパスである pathname と， open(2) 時の挙動を制御する flags が存在します． また，mode は，open(2) によって新しくファイルが作成される場合，そのファイルに設定するパーミッションを設定します． 返り値の int 型はファイルディスクリプタを表しますが， 何らかの原因で失敗した場合は-1を返します． ファイルディスクリプタは非負の整数であるため unsigned int 型を返したくなりますが， C言語でエラーを表現する場合，負の数をエラーとする事は非常に多いのでしょうがないですね． int open(const char *pathname, int flags, unsigned int *fd); として， fd に書き込むという方式もよく取られます． こうすればエラーとファイルディスクリプタをうまく分けられるので，私がC言語でエラーを返す関数を定義する時はこのようにしがち． 実際にopen(2)に渡されるフラグを見てみましょう． すべてのフラグについて解説するわけではなく， 私の方で特筆すべきと判断した内容にのみ触れます． 1.1.1 O_APPENDファイルを追加モードでオープンします． イメージしづらいと思うので実際に使ってみます． 以下のようなファイルを用意します． Drumato 123 次のようなCプログラムをコンパイル\u0026リンクして実行します． #include \u003cfcntl.h\u003e #include \u003cstdio.h\u003e #include \u003cstring.h\u003e #include \u003csys/stat.h\u003e #include \u003csys/types.h\u003e #include \u003cunistd.h\u003e int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY | O_APPEND); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } 実行結果は以下のようになります． $ gcc c.c $ ./a.out $ cat sample.txt Drumato 123 New Drumato 上記Cプログラムは以下のCプログラムと同じように振る舞います． #include \u003cfcntl.h\u003e #include \u003cstdio.h\u003e #include \u003cstring.h\u003e #include \u003csys/stat.h\u003e #include \u003csys/types.h\u003e #include \u003cunistd.h\u003e int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } if (lseek(fd, 0, SEEK_END) == -1) { fprintf(stderr, \"lseek(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } つまり，O_APPENDによって開かれたfdは， ファイルのオフセットをファイル末尾に設定した状態でユーザに渡されます． 1.1.2 O_CLOEXECプロセスの親子関係において， 親プロセスが開いているファイルディスクリプタは，子プロセスにもそのまま引き継がれます． この挙動を許したくない場合，つまり子プロセスにファイルディスクリプタ群をコピーしたくない場合， open(2) して開いたファイルディスクリプタに対して fcntl(2) を呼び出し， FD_CLOEXEC フラグを設定するというプログラムを書く必要があります． これを close-on-exec フラグの設定 といいます． しかし， open(2) の呼び出し終了から fcntl(2) の呼び出しが行われるまでに， 子プロセス等からfdを触られてしまうかもしれません． これを回避するためにLinux 2.6.23 以降から， O_CLOEXEC フラグを設定できるようになりました． fcntl(2) を明示的に呼び出さなくても，fdに対してclose-on-exec フラグを設定してくれます． 実際に試してみましょう． まずは子プロセスを生成する親プロセスのプログラムを作ります． #include \u003cdirent.h\u003e #include \u003cfcntl.h\u003e #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #include \u003cstring.h\u003e #include \u003csys/stat.h\u003e #include \u003csys/types.h\u003e #include \u003csys/wait.h\u003e #include \u003cunistd.h\u003e static void print_file_descriptors_by_current(); void open_sample_txt_many_times() { for (int i = 0; i \u003c 20; i++) { int fd; if ((fd = open(\"sample.txt\", O_RDONLY)) == -1) { fprintf(stderr, \"open(2) failed\\n\"); exit(1); } } } int main(void) { open_sample_txt_many_times(); print_file_descriptors_by_current(); pid_t pid; if ((pid = fork()) == -1) { fprintf(stderr, \"fork(2) failed\\n\"); return 1; } else if (pid == 0) { fprintf(stderr, \"\\nchild process start\\n\"); char *child_argv[] = {\"./child\", NULL}; execve(\"./child\", child_argv, NULL); return 0; } else { int status; if (waitpid(pid, \u0026status, 0) == -1) { fprintf(stderr, \"waitpid(2) failed\\n\"); return 1; } fprintf(stderr, \"child process end\\n\"); return 0; } } static void print_file_descriptors_by_current() { pid_t pid = getpid(); DIR *dir; struct dirent *dp; if ((dir = opendir(\"/proc/self/fd\")) == NULL) { fprintf(stderr, \"opendir(3) failed\"); exit(1); } int i = 0; for (dp = readdir(dir); dp != NULL; dp = readdir(dir)) { printf(\"files[%d] =\u003e %s (in pid=%d)\\n\", i, dp-\u003ed_name, pid); i++; } } procfsには /proc/[pid]/fd というディレクトリが存在し， pid に対応するプロセスが開いているファイルディスクリプタのエントリが存在します． 親プロセスではたくさんのファイルを開いておきましょう． 次に，子プロセスのプログラムを作ります． このプログラムでも print_file_descriptors_by_current() を呼び出すことで， 子プロセスに親プロセスのファイルディスクリプタが引き継がれているかどうか確認します． #include \u003cdi","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:1:1","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#基本"},{"categories":null,"content":" 1.1 基本まずはmanの内容を引っ張り出してきます． #include #include #include int open(const char *pathname, int flags); int open(const char *pathname, int flags, mode_t mode); int creat(const char *pathname, mode_t mode); int openat(int dirfd, const char *pathname, int flags); int openat(int dirfd, const char *pathname, int flags, mode_t mode); 引数にはファイルパスである pathname と， open(2) 時の挙動を制御する flags が存在します． また，mode は，open(2) によって新しくファイルが作成される場合，そのファイルに設定するパーミッションを設定します． 返り値の int 型はファイルディスクリプタを表しますが， 何らかの原因で失敗した場合は-1を返します． ファイルディスクリプタは非負の整数であるため unsigned int 型を返したくなりますが， C言語でエラーを表現する場合，負の数をエラーとする事は非常に多いのでしょうがないですね． int open(const char *pathname, int flags, unsigned int *fd); として， fd に書き込むという方式もよく取られます． こうすればエラーとファイルディスクリプタをうまく分けられるので，私がC言語でエラーを返す関数を定義する時はこのようにしがち． 実際にopen(2)に渡されるフラグを見てみましょう． すべてのフラグについて解説するわけではなく， 私の方で特筆すべきと判断した内容にのみ触れます． 1.1.1 O_APPENDファイルを追加モードでオープンします． イメージしづらいと思うので実際に使ってみます． 以下のようなファイルを用意します． Drumato 123 次のようなCプログラムをコンパイル\u0026リンクして実行します． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY | O_APPEND); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } 実行結果は以下のようになります． $ gcc c.c $ ./a.out $ cat sample.txt Drumato 123 New Drumato 上記Cプログラムは以下のCプログラムと同じように振る舞います． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } if (lseek(fd, 0, SEEK_END) == -1) { fprintf(stderr, \"lseek(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } つまり，O_APPENDによって開かれたfdは， ファイルのオフセットをファイル末尾に設定した状態でユーザに渡されます． 1.1.2 O_CLOEXECプロセスの親子関係において， 親プロセスが開いているファイルディスクリプタは，子プロセスにもそのまま引き継がれます． この挙動を許したくない場合，つまり子プロセスにファイルディスクリプタ群をコピーしたくない場合， open(2) して開いたファイルディスクリプタに対して fcntl(2) を呼び出し， FD_CLOEXEC フラグを設定するというプログラムを書く必要があります． これを close-on-exec フラグの設定 といいます． しかし， open(2) の呼び出し終了から fcntl(2) の呼び出しが行われるまでに， 子プロセス等からfdを触られてしまうかもしれません． これを回避するためにLinux 2.6.23 以降から， O_CLOEXEC フラグを設定できるようになりました． fcntl(2) を明示的に呼び出さなくても，fdに対してclose-on-exec フラグを設定してくれます． 実際に試してみましょう． まずは子プロセスを生成する親プロセスのプログラムを作ります． #include #include #include #include #include #include #include #include #include static void print_file_descriptors_by_current(); void open_sample_txt_many_times() { for (int i = 0; i \u003c 20; i++) { int fd; if ((fd = open(\"sample.txt\", O_RDONLY)) == -1) { fprintf(stderr, \"open(2) failed\\n\"); exit(1); } } } int main(void) { open_sample_txt_many_times(); print_file_descriptors_by_current(); pid_t pid; if ((pid = fork()) == -1) { fprintf(stderr, \"fork(2) failed\\n\"); return 1; } else if (pid == 0) { fprintf(stderr, \"\\nchild process start\\n\"); char *child_argv[] = {\"./child\", NULL}; execve(\"./child\", child_argv, NULL); return 0; } else { int status; if (waitpid(pid, \u0026status, 0) == -1) { fprintf(stderr, \"waitpid(2) failed\\n\"); return 1; } fprintf(stderr, \"child process end\\n\"); return 0; } } static void print_file_descriptors_by_current() { pid_t pid = getpid(); DIR *dir; struct dirent *dp; if ((dir = opendir(\"/proc/self/fd\")) == NULL) { fprintf(stderr, \"opendir(3) failed\"); exit(1); } int i = 0; for (dp = readdir(dir); dp != NULL; dp = readdir(dir)) { printf(\"files[%d] =\u003e %s (in pid=%d)\\n\", i, dp-\u003ed_name, pid); i++; } } procfsには /proc/[pid]/fd というディレクトリが存在し， pid に対応するプロセスが開いているファイルディスクリプタのエントリが存在します． 親プロセスではたくさんのファイルを開いておきましょう． 次に，子プロセスのプログラムを作ります． このプログラムでも print_file_descriptors_by_current() を呼び出すことで， 子プロセスに親プロセスのファイルディスクリプタが引き継がれているかどうか確認します． #include ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:1:1","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#o_append"},{"categories":null,"content":" 1.1 基本まずはmanの内容を引っ張り出してきます． #include #include #include int open(const char *pathname, int flags); int open(const char *pathname, int flags, mode_t mode); int creat(const char *pathname, mode_t mode); int openat(int dirfd, const char *pathname, int flags); int openat(int dirfd, const char *pathname, int flags, mode_t mode); 引数にはファイルパスである pathname と， open(2) 時の挙動を制御する flags が存在します． また，mode は，open(2) によって新しくファイルが作成される場合，そのファイルに設定するパーミッションを設定します． 返り値の int 型はファイルディスクリプタを表しますが， 何らかの原因で失敗した場合は-1を返します． ファイルディスクリプタは非負の整数であるため unsigned int 型を返したくなりますが， C言語でエラーを表現する場合，負の数をエラーとする事は非常に多いのでしょうがないですね． int open(const char *pathname, int flags, unsigned int *fd); として， fd に書き込むという方式もよく取られます． こうすればエラーとファイルディスクリプタをうまく分けられるので，私がC言語でエラーを返す関数を定義する時はこのようにしがち． 実際にopen(2)に渡されるフラグを見てみましょう． すべてのフラグについて解説するわけではなく， 私の方で特筆すべきと判断した内容にのみ触れます． 1.1.1 O_APPENDファイルを追加モードでオープンします． イメージしづらいと思うので実際に使ってみます． 以下のようなファイルを用意します． Drumato 123 次のようなCプログラムをコンパイル\u0026リンクして実行します． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY | O_APPEND); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } 実行結果は以下のようになります． $ gcc c.c $ ./a.out $ cat sample.txt Drumato 123 New Drumato 上記Cプログラムは以下のCプログラムと同じように振る舞います． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } if (lseek(fd, 0, SEEK_END) == -1) { fprintf(stderr, \"lseek(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } つまり，O_APPENDによって開かれたfdは， ファイルのオフセットをファイル末尾に設定した状態でユーザに渡されます． 1.1.2 O_CLOEXECプロセスの親子関係において， 親プロセスが開いているファイルディスクリプタは，子プロセスにもそのまま引き継がれます． この挙動を許したくない場合，つまり子プロセスにファイルディスクリプタ群をコピーしたくない場合， open(2) して開いたファイルディスクリプタに対して fcntl(2) を呼び出し， FD_CLOEXEC フラグを設定するというプログラムを書く必要があります． これを close-on-exec フラグの設定 といいます． しかし， open(2) の呼び出し終了から fcntl(2) の呼び出しが行われるまでに， 子プロセス等からfdを触られてしまうかもしれません． これを回避するためにLinux 2.6.23 以降から， O_CLOEXEC フラグを設定できるようになりました． fcntl(2) を明示的に呼び出さなくても，fdに対してclose-on-exec フラグを設定してくれます． 実際に試してみましょう． まずは子プロセスを生成する親プロセスのプログラムを作ります． #include #include #include #include #include #include #include #include #include static void print_file_descriptors_by_current(); void open_sample_txt_many_times() { for (int i = 0; i \u003c 20; i++) { int fd; if ((fd = open(\"sample.txt\", O_RDONLY)) == -1) { fprintf(stderr, \"open(2) failed\\n\"); exit(1); } } } int main(void) { open_sample_txt_many_times(); print_file_descriptors_by_current(); pid_t pid; if ((pid = fork()) == -1) { fprintf(stderr, \"fork(2) failed\\n\"); return 1; } else if (pid == 0) { fprintf(stderr, \"\\nchild process start\\n\"); char *child_argv[] = {\"./child\", NULL}; execve(\"./child\", child_argv, NULL); return 0; } else { int status; if (waitpid(pid, \u0026status, 0) == -1) { fprintf(stderr, \"waitpid(2) failed\\n\"); return 1; } fprintf(stderr, \"child process end\\n\"); return 0; } } static void print_file_descriptors_by_current() { pid_t pid = getpid(); DIR *dir; struct dirent *dp; if ((dir = opendir(\"/proc/self/fd\")) == NULL) { fprintf(stderr, \"opendir(3) failed\"); exit(1); } int i = 0; for (dp = readdir(dir); dp != NULL; dp = readdir(dir)) { printf(\"files[%d] =\u003e %s (in pid=%d)\\n\", i, dp-\u003ed_name, pid); i++; } } procfsには /proc/[pid]/fd というディレクトリが存在し， pid に対応するプロセスが開いているファイルディスクリプタのエントリが存在します． 親プロセスではたくさんのファイルを開いておきましょう． 次に，子プロセスのプログラムを作ります． このプログラムでも print_file_descriptors_by_current() を呼び出すことで， 子プロセスに親プロセスのファイルディスクリプタが引き継がれているかどうか確認します． #include ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:1:1","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#o_cloexec"},{"categories":null,"content":" 1.1 基本まずはmanの内容を引っ張り出してきます． #include #include #include int open(const char *pathname, int flags); int open(const char *pathname, int flags, mode_t mode); int creat(const char *pathname, mode_t mode); int openat(int dirfd, const char *pathname, int flags); int openat(int dirfd, const char *pathname, int flags, mode_t mode); 引数にはファイルパスである pathname と， open(2) 時の挙動を制御する flags が存在します． また，mode は，open(2) によって新しくファイルが作成される場合，そのファイルに設定するパーミッションを設定します． 返り値の int 型はファイルディスクリプタを表しますが， 何らかの原因で失敗した場合は-1を返します． ファイルディスクリプタは非負の整数であるため unsigned int 型を返したくなりますが， C言語でエラーを表現する場合，負の数をエラーとする事は非常に多いのでしょうがないですね． int open(const char *pathname, int flags, unsigned int *fd); として， fd に書き込むという方式もよく取られます． こうすればエラーとファイルディスクリプタをうまく分けられるので，私がC言語でエラーを返す関数を定義する時はこのようにしがち． 実際にopen(2)に渡されるフラグを見てみましょう． すべてのフラグについて解説するわけではなく， 私の方で特筆すべきと判断した内容にのみ触れます． 1.1.1 O_APPENDファイルを追加モードでオープンします． イメージしづらいと思うので実際に使ってみます． 以下のようなファイルを用意します． Drumato 123 次のようなCプログラムをコンパイル\u0026リンクして実行します． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY | O_APPEND); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } 実行結果は以下のようになります． $ gcc c.c $ ./a.out $ cat sample.txt Drumato 123 New Drumato 上記Cプログラムは以下のCプログラムと同じように振る舞います． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } if (lseek(fd, 0, SEEK_END) == -1) { fprintf(stderr, \"lseek(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } つまり，O_APPENDによって開かれたfdは， ファイルのオフセットをファイル末尾に設定した状態でユーザに渡されます． 1.1.2 O_CLOEXECプロセスの親子関係において， 親プロセスが開いているファイルディスクリプタは，子プロセスにもそのまま引き継がれます． この挙動を許したくない場合，つまり子プロセスにファイルディスクリプタ群をコピーしたくない場合， open(2) して開いたファイルディスクリプタに対して fcntl(2) を呼び出し， FD_CLOEXEC フラグを設定するというプログラムを書く必要があります． これを close-on-exec フラグの設定 といいます． しかし， open(2) の呼び出し終了から fcntl(2) の呼び出しが行われるまでに， 子プロセス等からfdを触られてしまうかもしれません． これを回避するためにLinux 2.6.23 以降から， O_CLOEXEC フラグを設定できるようになりました． fcntl(2) を明示的に呼び出さなくても，fdに対してclose-on-exec フラグを設定してくれます． 実際に試してみましょう． まずは子プロセスを生成する親プロセスのプログラムを作ります． #include #include #include #include #include #include #include #include #include static void print_file_descriptors_by_current(); void open_sample_txt_many_times() { for (int i = 0; i \u003c 20; i++) { int fd; if ((fd = open(\"sample.txt\", O_RDONLY)) == -1) { fprintf(stderr, \"open(2) failed\\n\"); exit(1); } } } int main(void) { open_sample_txt_many_times(); print_file_descriptors_by_current(); pid_t pid; if ((pid = fork()) == -1) { fprintf(stderr, \"fork(2) failed\\n\"); return 1; } else if (pid == 0) { fprintf(stderr, \"\\nchild process start\\n\"); char *child_argv[] = {\"./child\", NULL}; execve(\"./child\", child_argv, NULL); return 0; } else { int status; if (waitpid(pid, \u0026status, 0) == -1) { fprintf(stderr, \"waitpid(2) failed\\n\"); return 1; } fprintf(stderr, \"child process end\\n\"); return 0; } } static void print_file_descriptors_by_current() { pid_t pid = getpid(); DIR *dir; struct dirent *dp; if ((dir = opendir(\"/proc/self/fd\")) == NULL) { fprintf(stderr, \"opendir(3) failed\"); exit(1); } int i = 0; for (dp = readdir(dir); dp != NULL; dp = readdir(dir)) { printf(\"files[%d] =\u003e %s (in pid=%d)\\n\", i, dp-\u003ed_name, pid); i++; } } procfsには /proc/[pid]/fd というディレクトリが存在し， pid に対応するプロセスが開いているファイルディスクリプタのエントリが存在します． 親プロセスではたくさんのファイルを開いておきましょう． 次に，子プロセスのプログラムを作ります． このプログラムでも print_file_descriptors_by_current() を呼び出すことで， 子プロセスに親プロセスのファイルディスクリプタが引き継がれているかどうか確認します． #include ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:1:1","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#o_creat"},{"categories":null,"content":" 1.1 基本まずはmanの内容を引っ張り出してきます． #include #include #include int open(const char *pathname, int flags); int open(const char *pathname, int flags, mode_t mode); int creat(const char *pathname, mode_t mode); int openat(int dirfd, const char *pathname, int flags); int openat(int dirfd, const char *pathname, int flags, mode_t mode); 引数にはファイルパスである pathname と， open(2) 時の挙動を制御する flags が存在します． また，mode は，open(2) によって新しくファイルが作成される場合，そのファイルに設定するパーミッションを設定します． 返り値の int 型はファイルディスクリプタを表しますが， 何らかの原因で失敗した場合は-1を返します． ファイルディスクリプタは非負の整数であるため unsigned int 型を返したくなりますが， C言語でエラーを表現する場合，負の数をエラーとする事は非常に多いのでしょうがないですね． int open(const char *pathname, int flags, unsigned int *fd); として， fd に書き込むという方式もよく取られます． こうすればエラーとファイルディスクリプタをうまく分けられるので，私がC言語でエラーを返す関数を定義する時はこのようにしがち． 実際にopen(2)に渡されるフラグを見てみましょう． すべてのフラグについて解説するわけではなく， 私の方で特筆すべきと判断した内容にのみ触れます． 1.1.1 O_APPENDファイルを追加モードでオープンします． イメージしづらいと思うので実際に使ってみます． 以下のようなファイルを用意します． Drumato 123 次のようなCプログラムをコンパイル\u0026リンクして実行します． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY | O_APPEND); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } 実行結果は以下のようになります． $ gcc c.c $ ./a.out $ cat sample.txt Drumato 123 New Drumato 上記Cプログラムは以下のCプログラムと同じように振る舞います． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } if (lseek(fd, 0, SEEK_END) == -1) { fprintf(stderr, \"lseek(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } つまり，O_APPENDによって開かれたfdは， ファイルのオフセットをファイル末尾に設定した状態でユーザに渡されます． 1.1.2 O_CLOEXECプロセスの親子関係において， 親プロセスが開いているファイルディスクリプタは，子プロセスにもそのまま引き継がれます． この挙動を許したくない場合，つまり子プロセスにファイルディスクリプタ群をコピーしたくない場合， open(2) して開いたファイルディスクリプタに対して fcntl(2) を呼び出し， FD_CLOEXEC フラグを設定するというプログラムを書く必要があります． これを close-on-exec フラグの設定 といいます． しかし， open(2) の呼び出し終了から fcntl(2) の呼び出しが行われるまでに， 子プロセス等からfdを触られてしまうかもしれません． これを回避するためにLinux 2.6.23 以降から， O_CLOEXEC フラグを設定できるようになりました． fcntl(2) を明示的に呼び出さなくても，fdに対してclose-on-exec フラグを設定してくれます． 実際に試してみましょう． まずは子プロセスを生成する親プロセスのプログラムを作ります． #include #include #include #include #include #include #include #include #include static void print_file_descriptors_by_current(); void open_sample_txt_many_times() { for (int i = 0; i \u003c 20; i++) { int fd; if ((fd = open(\"sample.txt\", O_RDONLY)) == -1) { fprintf(stderr, \"open(2) failed\\n\"); exit(1); } } } int main(void) { open_sample_txt_many_times(); print_file_descriptors_by_current(); pid_t pid; if ((pid = fork()) == -1) { fprintf(stderr, \"fork(2) failed\\n\"); return 1; } else if (pid == 0) { fprintf(stderr, \"\\nchild process start\\n\"); char *child_argv[] = {\"./child\", NULL}; execve(\"./child\", child_argv, NULL); return 0; } else { int status; if (waitpid(pid, \u0026status, 0) == -1) { fprintf(stderr, \"waitpid(2) failed\\n\"); return 1; } fprintf(stderr, \"child process end\\n\"); return 0; } } static void print_file_descriptors_by_current() { pid_t pid = getpid(); DIR *dir; struct dirent *dp; if ((dir = opendir(\"/proc/self/fd\")) == NULL) { fprintf(stderr, \"opendir(3) failed\"); exit(1); } int i = 0; for (dp = readdir(dir); dp != NULL; dp = readdir(dir)) { printf(\"files[%d] =\u003e %s (in pid=%d)\\n\", i, dp-\u003ed_name, pid); i++; } } procfsには /proc/[pid]/fd というディレクトリが存在し， pid に対応するプロセスが開いているファイルディスクリプタのエントリが存在します． 親プロセスではたくさんのファイルを開いておきましょう． 次に，子プロセスのプログラムを作ります． このプログラムでも print_file_descriptors_by_current() を呼び出すことで， 子プロセスに親プロセスのファイルディスクリプタが引き継がれているかどうか確認します． #include ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:1:1","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#o_directory"},{"categories":null,"content":" 1.1 基本まずはmanの内容を引っ張り出してきます． #include #include #include int open(const char *pathname, int flags); int open(const char *pathname, int flags, mode_t mode); int creat(const char *pathname, mode_t mode); int openat(int dirfd, const char *pathname, int flags); int openat(int dirfd, const char *pathname, int flags, mode_t mode); 引数にはファイルパスである pathname と， open(2) 時の挙動を制御する flags が存在します． また，mode は，open(2) によって新しくファイルが作成される場合，そのファイルに設定するパーミッションを設定します． 返り値の int 型はファイルディスクリプタを表しますが， 何らかの原因で失敗した場合は-1を返します． ファイルディスクリプタは非負の整数であるため unsigned int 型を返したくなりますが， C言語でエラーを表現する場合，負の数をエラーとする事は非常に多いのでしょうがないですね． int open(const char *pathname, int flags, unsigned int *fd); として， fd に書き込むという方式もよく取られます． こうすればエラーとファイルディスクリプタをうまく分けられるので，私がC言語でエラーを返す関数を定義する時はこのようにしがち． 実際にopen(2)に渡されるフラグを見てみましょう． すべてのフラグについて解説するわけではなく， 私の方で特筆すべきと判断した内容にのみ触れます． 1.1.1 O_APPENDファイルを追加モードでオープンします． イメージしづらいと思うので実際に使ってみます． 以下のようなファイルを用意します． Drumato 123 次のようなCプログラムをコンパイル\u0026リンクして実行します． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY | O_APPEND); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } 実行結果は以下のようになります． $ gcc c.c $ ./a.out $ cat sample.txt Drumato 123 New Drumato 上記Cプログラムは以下のCプログラムと同じように振る舞います． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } if (lseek(fd, 0, SEEK_END) == -1) { fprintf(stderr, \"lseek(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } つまり，O_APPENDによって開かれたfdは， ファイルのオフセットをファイル末尾に設定した状態でユーザに渡されます． 1.1.2 O_CLOEXECプロセスの親子関係において， 親プロセスが開いているファイルディスクリプタは，子プロセスにもそのまま引き継がれます． この挙動を許したくない場合，つまり子プロセスにファイルディスクリプタ群をコピーしたくない場合， open(2) して開いたファイルディスクリプタに対して fcntl(2) を呼び出し， FD_CLOEXEC フラグを設定するというプログラムを書く必要があります． これを close-on-exec フラグの設定 といいます． しかし， open(2) の呼び出し終了から fcntl(2) の呼び出しが行われるまでに， 子プロセス等からfdを触られてしまうかもしれません． これを回避するためにLinux 2.6.23 以降から， O_CLOEXEC フラグを設定できるようになりました． fcntl(2) を明示的に呼び出さなくても，fdに対してclose-on-exec フラグを設定してくれます． 実際に試してみましょう． まずは子プロセスを生成する親プロセスのプログラムを作ります． #include #include #include #include #include #include #include #include #include static void print_file_descriptors_by_current(); void open_sample_txt_many_times() { for (int i = 0; i \u003c 20; i++) { int fd; if ((fd = open(\"sample.txt\", O_RDONLY)) == -1) { fprintf(stderr, \"open(2) failed\\n\"); exit(1); } } } int main(void) { open_sample_txt_many_times(); print_file_descriptors_by_current(); pid_t pid; if ((pid = fork()) == -1) { fprintf(stderr, \"fork(2) failed\\n\"); return 1; } else if (pid == 0) { fprintf(stderr, \"\\nchild process start\\n\"); char *child_argv[] = {\"./child\", NULL}; execve(\"./child\", child_argv, NULL); return 0; } else { int status; if (waitpid(pid, \u0026status, 0) == -1) { fprintf(stderr, \"waitpid(2) failed\\n\"); return 1; } fprintf(stderr, \"child process end\\n\"); return 0; } } static void print_file_descriptors_by_current() { pid_t pid = getpid(); DIR *dir; struct dirent *dp; if ((dir = opendir(\"/proc/self/fd\")) == NULL) { fprintf(stderr, \"opendir(3) failed\"); exit(1); } int i = 0; for (dp = readdir(dir); dp != NULL; dp = readdir(dir)) { printf(\"files[%d] =\u003e %s (in pid=%d)\\n\", i, dp-\u003ed_name, pid); i++; } } procfsには /proc/[pid]/fd というディレクトリが存在し， pid に対応するプロセスが開いているファイルディスクリプタのエントリが存在します． 親プロセスではたくさんのファイルを開いておきましょう． 次に，子プロセスのプログラムを作ります． このプログラムでも print_file_descriptors_by_current() を呼び出すことで， 子プロセスに親プロセスのファイルディスクリプタが引き継がれているかどうか確認します． #include ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:1:1","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#o_excl"},{"categories":null,"content":" 1.1 基本まずはmanの内容を引っ張り出してきます． #include #include #include int open(const char *pathname, int flags); int open(const char *pathname, int flags, mode_t mode); int creat(const char *pathname, mode_t mode); int openat(int dirfd, const char *pathname, int flags); int openat(int dirfd, const char *pathname, int flags, mode_t mode); 引数にはファイルパスである pathname と， open(2) 時の挙動を制御する flags が存在します． また，mode は，open(2) によって新しくファイルが作成される場合，そのファイルに設定するパーミッションを設定します． 返り値の int 型はファイルディスクリプタを表しますが， 何らかの原因で失敗した場合は-1を返します． ファイルディスクリプタは非負の整数であるため unsigned int 型を返したくなりますが， C言語でエラーを表現する場合，負の数をエラーとする事は非常に多いのでしょうがないですね． int open(const char *pathname, int flags, unsigned int *fd); として， fd に書き込むという方式もよく取られます． こうすればエラーとファイルディスクリプタをうまく分けられるので，私がC言語でエラーを返す関数を定義する時はこのようにしがち． 実際にopen(2)に渡されるフラグを見てみましょう． すべてのフラグについて解説するわけではなく， 私の方で特筆すべきと判断した内容にのみ触れます． 1.1.1 O_APPENDファイルを追加モードでオープンします． イメージしづらいと思うので実際に使ってみます． 以下のようなファイルを用意します． Drumato 123 次のようなCプログラムをコンパイル\u0026リンクして実行します． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY | O_APPEND); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } 実行結果は以下のようになります． $ gcc c.c $ ./a.out $ cat sample.txt Drumato 123 New Drumato 上記Cプログラムは以下のCプログラムと同じように振る舞います． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } if (lseek(fd, 0, SEEK_END) == -1) { fprintf(stderr, \"lseek(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } つまり，O_APPENDによって開かれたfdは， ファイルのオフセットをファイル末尾に設定した状態でユーザに渡されます． 1.1.2 O_CLOEXECプロセスの親子関係において， 親プロセスが開いているファイルディスクリプタは，子プロセスにもそのまま引き継がれます． この挙動を許したくない場合，つまり子プロセスにファイルディスクリプタ群をコピーしたくない場合， open(2) して開いたファイルディスクリプタに対して fcntl(2) を呼び出し， FD_CLOEXEC フラグを設定するというプログラムを書く必要があります． これを close-on-exec フラグの設定 といいます． しかし， open(2) の呼び出し終了から fcntl(2) の呼び出しが行われるまでに， 子プロセス等からfdを触られてしまうかもしれません． これを回避するためにLinux 2.6.23 以降から， O_CLOEXEC フラグを設定できるようになりました． fcntl(2) を明示的に呼び出さなくても，fdに対してclose-on-exec フラグを設定してくれます． 実際に試してみましょう． まずは子プロセスを生成する親プロセスのプログラムを作ります． #include #include #include #include #include #include #include #include #include static void print_file_descriptors_by_current(); void open_sample_txt_many_times() { for (int i = 0; i \u003c 20; i++) { int fd; if ((fd = open(\"sample.txt\", O_RDONLY)) == -1) { fprintf(stderr, \"open(2) failed\\n\"); exit(1); } } } int main(void) { open_sample_txt_many_times(); print_file_descriptors_by_current(); pid_t pid; if ((pid = fork()) == -1) { fprintf(stderr, \"fork(2) failed\\n\"); return 1; } else if (pid == 0) { fprintf(stderr, \"\\nchild process start\\n\"); char *child_argv[] = {\"./child\", NULL}; execve(\"./child\", child_argv, NULL); return 0; } else { int status; if (waitpid(pid, \u0026status, 0) == -1) { fprintf(stderr, \"waitpid(2) failed\\n\"); return 1; } fprintf(stderr, \"child process end\\n\"); return 0; } } static void print_file_descriptors_by_current() { pid_t pid = getpid(); DIR *dir; struct dirent *dp; if ((dir = opendir(\"/proc/self/fd\")) == NULL) { fprintf(stderr, \"opendir(3) failed\"); exit(1); } int i = 0; for (dp = readdir(dir); dp != NULL; dp = readdir(dir)) { printf(\"files[%d] =\u003e %s (in pid=%d)\\n\", i, dp-\u003ed_name, pid); i++; } } procfsには /proc/[pid]/fd というディレクトリが存在し， pid に対応するプロセスが開いているファイルディスクリプタのエントリが存在します． 親プロセスではたくさんのファイルを開いておきましょう． 次に，子プロセスのプログラムを作ります． このプログラムでも print_file_descriptors_by_current() を呼び出すことで， 子プロセスに親プロセスのファイルディスクリプタが引き継がれているかどうか確認します． #include ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:1:1","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#o_path"},{"categories":null,"content":" 1.1 基本まずはmanの内容を引っ張り出してきます． #include #include #include int open(const char *pathname, int flags); int open(const char *pathname, int flags, mode_t mode); int creat(const char *pathname, mode_t mode); int openat(int dirfd, const char *pathname, int flags); int openat(int dirfd, const char *pathname, int flags, mode_t mode); 引数にはファイルパスである pathname と， open(2) 時の挙動を制御する flags が存在します． また，mode は，open(2) によって新しくファイルが作成される場合，そのファイルに設定するパーミッションを設定します． 返り値の int 型はファイルディスクリプタを表しますが， 何らかの原因で失敗した場合は-1を返します． ファイルディスクリプタは非負の整数であるため unsigned int 型を返したくなりますが， C言語でエラーを表現する場合，負の数をエラーとする事は非常に多いのでしょうがないですね． int open(const char *pathname, int flags, unsigned int *fd); として， fd に書き込むという方式もよく取られます． こうすればエラーとファイルディスクリプタをうまく分けられるので，私がC言語でエラーを返す関数を定義する時はこのようにしがち． 実際にopen(2)に渡されるフラグを見てみましょう． すべてのフラグについて解説するわけではなく， 私の方で特筆すべきと判断した内容にのみ触れます． 1.1.1 O_APPENDファイルを追加モードでオープンします． イメージしづらいと思うので実際に使ってみます． 以下のようなファイルを用意します． Drumato 123 次のようなCプログラムをコンパイル\u0026リンクして実行します． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY | O_APPEND); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } 実行結果は以下のようになります． $ gcc c.c $ ./a.out $ cat sample.txt Drumato 123 New Drumato 上記Cプログラムは以下のCプログラムと同じように振る舞います． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } if (lseek(fd, 0, SEEK_END) == -1) { fprintf(stderr, \"lseek(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } つまり，O_APPENDによって開かれたfdは， ファイルのオフセットをファイル末尾に設定した状態でユーザに渡されます． 1.1.2 O_CLOEXECプロセスの親子関係において， 親プロセスが開いているファイルディスクリプタは，子プロセスにもそのまま引き継がれます． この挙動を許したくない場合，つまり子プロセスにファイルディスクリプタ群をコピーしたくない場合， open(2) して開いたファイルディスクリプタに対して fcntl(2) を呼び出し， FD_CLOEXEC フラグを設定するというプログラムを書く必要があります． これを close-on-exec フラグの設定 といいます． しかし， open(2) の呼び出し終了から fcntl(2) の呼び出しが行われるまでに， 子プロセス等からfdを触られてしまうかもしれません． これを回避するためにLinux 2.6.23 以降から， O_CLOEXEC フラグを設定できるようになりました． fcntl(2) を明示的に呼び出さなくても，fdに対してclose-on-exec フラグを設定してくれます． 実際に試してみましょう． まずは子プロセスを生成する親プロセスのプログラムを作ります． #include #include #include #include #include #include #include #include #include static void print_file_descriptors_by_current(); void open_sample_txt_many_times() { for (int i = 0; i \u003c 20; i++) { int fd; if ((fd = open(\"sample.txt\", O_RDONLY)) == -1) { fprintf(stderr, \"open(2) failed\\n\"); exit(1); } } } int main(void) { open_sample_txt_many_times(); print_file_descriptors_by_current(); pid_t pid; if ((pid = fork()) == -1) { fprintf(stderr, \"fork(2) failed\\n\"); return 1; } else if (pid == 0) { fprintf(stderr, \"\\nchild process start\\n\"); char *child_argv[] = {\"./child\", NULL}; execve(\"./child\", child_argv, NULL); return 0; } else { int status; if (waitpid(pid, \u0026status, 0) == -1) { fprintf(stderr, \"waitpid(2) failed\\n\"); return 1; } fprintf(stderr, \"child process end\\n\"); return 0; } } static void print_file_descriptors_by_current() { pid_t pid = getpid(); DIR *dir; struct dirent *dp; if ((dir = opendir(\"/proc/self/fd\")) == NULL) { fprintf(stderr, \"opendir(3) failed\"); exit(1); } int i = 0; for (dp = readdir(dir); dp != NULL; dp = readdir(dir)) { printf(\"files[%d] =\u003e %s (in pid=%d)\\n\", i, dp-\u003ed_name, pid); i++; } } procfsには /proc/[pid]/fd というディレクトリが存在し， pid に対応するプロセスが開いているファイルディスクリプタのエントリが存在します． 親プロセスではたくさんのファイルを開いておきましょう． 次に，子プロセスのプログラムを作ります． このプログラムでも print_file_descriptors_by_current() を呼び出すことで， 子プロセスに親プロセスのファイルディスクリプタが引き継がれているかどうか確認します． #include ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:1:1","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#o_tmpfile"},{"categories":null,"content":" 1.1 基本まずはmanの内容を引っ張り出してきます． #include #include #include int open(const char *pathname, int flags); int open(const char *pathname, int flags, mode_t mode); int creat(const char *pathname, mode_t mode); int openat(int dirfd, const char *pathname, int flags); int openat(int dirfd, const char *pathname, int flags, mode_t mode); 引数にはファイルパスである pathname と， open(2) 時の挙動を制御する flags が存在します． また，mode は，open(2) によって新しくファイルが作成される場合，そのファイルに設定するパーミッションを設定します． 返り値の int 型はファイルディスクリプタを表しますが， 何らかの原因で失敗した場合は-1を返します． ファイルディスクリプタは非負の整数であるため unsigned int 型を返したくなりますが， C言語でエラーを表現する場合，負の数をエラーとする事は非常に多いのでしょうがないですね． int open(const char *pathname, int flags, unsigned int *fd); として， fd に書き込むという方式もよく取られます． こうすればエラーとファイルディスクリプタをうまく分けられるので，私がC言語でエラーを返す関数を定義する時はこのようにしがち． 実際にopen(2)に渡されるフラグを見てみましょう． すべてのフラグについて解説するわけではなく， 私の方で特筆すべきと判断した内容にのみ触れます． 1.1.1 O_APPENDファイルを追加モードでオープンします． イメージしづらいと思うので実際に使ってみます． 以下のようなファイルを用意します． Drumato 123 次のようなCプログラムをコンパイル\u0026リンクして実行します． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY | O_APPEND); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } 実行結果は以下のようになります． $ gcc c.c $ ./a.out $ cat sample.txt Drumato 123 New Drumato 上記Cプログラムは以下のCプログラムと同じように振る舞います． #include #include #include #include #include #include int main() { const char *msg = \"New Drumato\\n\"; int fd = open(\"sample.txt\", O_WRONLY); if (fd == -1) { fprintf(stderr, \"open(2) failed\\n\"); return 1; } if (lseek(fd, 0, SEEK_END) == -1) { fprintf(stderr, \"lseek(2) failed\\n\"); return 1; } ssize_t nbytes = write(fd, msg, strlen(msg)); if (nbytes == -1) { fprintf(stderr, \"write(2) failed\\n\"); return 1; } return 0; } つまり，O_APPENDによって開かれたfdは， ファイルのオフセットをファイル末尾に設定した状態でユーザに渡されます． 1.1.2 O_CLOEXECプロセスの親子関係において， 親プロセスが開いているファイルディスクリプタは，子プロセスにもそのまま引き継がれます． この挙動を許したくない場合，つまり子プロセスにファイルディスクリプタ群をコピーしたくない場合， open(2) して開いたファイルディスクリプタに対して fcntl(2) を呼び出し， FD_CLOEXEC フラグを設定するというプログラムを書く必要があります． これを close-on-exec フラグの設定 といいます． しかし， open(2) の呼び出し終了から fcntl(2) の呼び出しが行われるまでに， 子プロセス等からfdを触られてしまうかもしれません． これを回避するためにLinux 2.6.23 以降から， O_CLOEXEC フラグを設定できるようになりました． fcntl(2) を明示的に呼び出さなくても，fdに対してclose-on-exec フラグを設定してくれます． 実際に試してみましょう． まずは子プロセスを生成する親プロセスのプログラムを作ります． #include #include #include #include #include #include #include #include #include static void print_file_descriptors_by_current(); void open_sample_txt_many_times() { for (int i = 0; i \u003c 20; i++) { int fd; if ((fd = open(\"sample.txt\", O_RDONLY)) == -1) { fprintf(stderr, \"open(2) failed\\n\"); exit(1); } } } int main(void) { open_sample_txt_many_times(); print_file_descriptors_by_current(); pid_t pid; if ((pid = fork()) == -1) { fprintf(stderr, \"fork(2) failed\\n\"); return 1; } else if (pid == 0) { fprintf(stderr, \"\\nchild process start\\n\"); char *child_argv[] = {\"./child\", NULL}; execve(\"./child\", child_argv, NULL); return 0; } else { int status; if (waitpid(pid, \u0026status, 0) == -1) { fprintf(stderr, \"waitpid(2) failed\\n\"); return 1; } fprintf(stderr, \"child process end\\n\"); return 0; } } static void print_file_descriptors_by_current() { pid_t pid = getpid(); DIR *dir; struct dirent *dp; if ((dir = opendir(\"/proc/self/fd\")) == NULL) { fprintf(stderr, \"opendir(3) failed\"); exit(1); } int i = 0; for (dp = readdir(dir); dp != NULL; dp = readdir(dir)) { printf(\"files[%d] =\u003e %s (in pid=%d)\\n\", i, dp-\u003ed_name, pid); i++; } } procfsには /proc/[pid]/fd というディレクトリが存在し， pid に対応するプロセスが開いているファイルディスクリプタのエントリが存在します． 親プロセスではたくさんのファイルを開いておきましょう． 次に，子プロセスのプログラムを作ります． このプログラムでも print_file_descriptors_by_current() を呼び出すことで， 子プロセスに親プロセスのファイルディスクリプタが引き継がれているかどうか確認します． #include ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:1:1","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#o_trunc"},{"categories":null,"content":" 2 カーネルから見るopen(2)ここからはLinuxカーネルの中身に入っていって， open(2) によってOSのどんな機能が動いているのかを理解していきましょう． 対象となるLinuxカーネルは v5.10.9 です． ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:2:0","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#カーネルから見るopen2"},{"categories":null,"content":" 2.1 前提知識Linuxにおいて各ユーザプロセスはファイルディスクリプタを使ってメモリ上のマッピングにアクセスします． プロセスディスクリプタを表す task_struct 構造体には struct files_struct *files; というメンバがあり， これは オープンファイルオブジェクト を管理する構造体です． オープンファイルオブジェクトはオープンファイルディスクリプタとも， ファイルハンドルとも呼ばれます． オープンファイルディスクリプタという呼称はPOSIXで用いられるようです． dup(2) 等によってファイルディスクリプタが複製されることがありますが， この場合2つのファイルディスクリプタが同じオープンファイルオブジェクトを指すことになります． /* * Open file table structure */ struct files_struct { /* * read mostly part */ atomic_t count; bool resize_in_progress; wait_queue_head_t resize_wait; struct fdtable __rcu *fdt; struct fdtable fdtab; /* * written part on a separate cache line in SMP */ spinlock_t file_lock ____cacheline_aligned_in_smp; unsigned int next_fd; unsigned long close_on_exec_init[1]; unsigned long open_fds_init[1]; unsigned long full_fds_bits_init[1]; struct file __rcu * fd_array[NR_OPEN_DEFAULT]; }; このうち,struct fdtable *fdt を用いてオープンファイルオブジェクトにアクセスします． int fd は単にこのテーブルのインデックスでしかありません． 例えばread(2) では struct fd f = fdget_pos(fd); のようにして int を struct fd に変換していますが， 最終的に rcu_dereference_raw(fdt-\u003efd[fd]); という関数呼び出しの中で struct fdtable.fd にアクセスしています． struct fdtable { unsigned int max_fds; struct file __rcu **fd; /* current fd array */ unsigned long *close_on_exec; unsigned long *open_fds; unsigned long *full_fds_bits; struct rcu_head rcu; }; struct file __rcu **fd; がオープンファイルオブジェクトのテーブルです． つまり struct file がオープンファイルオブジェクトの実体となります． 大きな構造体なので，ここでは定義の紹介はしません． 先程のサンプルで sample.txt を20回openしたように， 同じファイルを複数回オープンする事はあり得るので， 一つのファイル(inode)に対して複数のオープンファイルオブジェクトが存在する可能性があります． ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:2:1","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#前提知識"},{"categories":null,"content":" 2.2 “大まか\"なコードリーディングそれでは実際に open(2) の中身に入っていきます． 全ての関数を深堀りするわけではなく，あくまで大まかな理解にとどめます． 2.2.1 簡単なコールツリー簡単にコールツリーを書き起こしておきました． 私が後々本格的にコードリーディングする場合に使用するつもりで作りましたが， よかったら参考にしてください． - `sys_open` ... `open(2)` のカーネル側エントリポイント - `force_o_largefile` ... プロセスの実行ドメインを検証し，必要に応じて `O_LARGEFILE` を設定 - `do_sys_open` ... `struct open_how` の構築 - `do_sys_openat2` ... `open(2)` の実体 - `build_open_flags` ... より詳細なフラグの設定，検証 - `getname` ... ユーザプロセス空間の`filename` を取得する - `get_unused_fd_flags` ... カレントプロセスが使用していないfdを探索して返す - `do_filp_open` ... `struct file` に必要な情報を突っ込んで返す - `path_openat` ... `do_filp_open` の本筋 - `alloc_empty_file` ... `kmem_cache_zalloc()` で `struct file` を初期化 - `do_open` ... ここまでの情報をもとにファイルを開く - `vfs_open` ... 仮想ファイルシステムに問い合わせ，実際にファイルを開く - `fd_install` ... 新しい `struct file` をfdtableに登録する - `putname` ... `kmem_cache_free` を呼んで `struct filename` を開放 2.2.2 SYSCALL_DEFINE3システムコールの実装を追う場合， まずは SYSCALL_DEFINEN マクロ関数の呼び出しを見ると良いです． 例えばopen(2) であれば，/fs/open.c#1192 にあります． SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode) { if (force_o_largefile()) flags |= O_LARGEFILE; return do_sys_open(AT_FDCWD, filename, flags, mode); } force_o_largefile() マクロは，プロセスのパーソナリティを調べます． PER_LINUX32 フラグが立っていなければtrueが返るという処理で， 恐らく32bit Linuxかどうかのチェックだと思います． O_LARGEFILE フラグについては説明していませんでしたが， ファイルサイズが32bit幅で表せず，64bit幅を必要とする場合に設定します． プロセスのパーソナリティによっては，このフラグが自動で設定されるということですね． 2.2.3 do_sys_open()long do_sys_open(int dfd, const char __user *filename, int flags, umode_t mode) { struct open_how how = build_open_how(flags, mode); return do_sys_openat2(dfd, filename, \u0026how); } build_open_how() 関数はその名の通り， open(2) の挙動を制御する構造体 struct open_how を組み立てる関数です． 組み立てたあとは open(2) の実体である do_sys_openat2() に渡して呼び出します． dfd には AT_FDCWD が渡されています． これは， open(2) は dfd に AT_FDCWD を指定した openat(2) と実質的に同じ動作をするからです． 2.2.4 build_open_how()inline struct open_how build_open_how(int flags, umode_t mode) { struct open_how how = { .flags = flags \u0026 VALID_OPEN_FLAGS, .mode = mode \u0026 S_IALLUGO, }; /* O_PATH beats everything else. */ if (how.flags \u0026 O_PATH) how.flags \u0026= O_PATH_FLAGS; /* Modes should only be set for create-like flags. */ if (!WILL_CREATE(how.flags)) how.mode = 0; return how; } WILL_CREATE() マクロによってファイルが作成されるかどうかのチェックが行われます． そうでない場合 struct open_how.mode は使用されません． O_PATH_FLAGS マクロは (O_DIRECTORY | O_NOFOLLOW | O_PATH | O_CLOEXEC) に展開されます． つまり， O_PATH フラグを渡した時点で，上記以外のフラグは全て無視されます． 2.2.5 do_sys_openat2()static long do_sys_openat2(int dfd, const char __user *filename, struct open_how *how) { struct open_flags op; int fd = build_open_flags(how, \u0026op); struct filename *tmp; if (fd) return fd; tmp = getname(filename); if (IS_ERR(tmp)) return PTR_ERR(tmp); fd = get_unused_fd_flags(how-\u003eflags); if (fd \u003e= 0) { struct file *f = do_filp_open(dfd, tmp, \u0026op); if (IS_ERR(f)) { put_unused_fd(fd); fd = PTR_ERR(f); } else { fsnotify_open(f); fd_install(fd, f); } } putname(tmp); return fd; } build_open_flags によって，詳細なフラグの検証と設定が行われます． その後 get_unused_fd_flags で未使用のファイルディスクリプタを探索，取得します． 実際にファイルを開くのは do_filp_open という手続きです． ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:2:2","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#大まかなコードリーディング"},{"categories":null,"content":" 2.2 “大まか\"なコードリーディングそれでは実際に open(2) の中身に入っていきます． 全ての関数を深堀りするわけではなく，あくまで大まかな理解にとどめます． 2.2.1 簡単なコールツリー簡単にコールツリーを書き起こしておきました． 私が後々本格的にコードリーディングする場合に使用するつもりで作りましたが， よかったら参考にしてください． - `sys_open` ... `open(2)` のカーネル側エントリポイント - `force_o_largefile` ... プロセスの実行ドメインを検証し，必要に応じて `O_LARGEFILE` を設定 - `do_sys_open` ... `struct open_how` の構築 - `do_sys_openat2` ... `open(2)` の実体 - `build_open_flags` ... より詳細なフラグの設定，検証 - `getname` ... ユーザプロセス空間の`filename` を取得する - `get_unused_fd_flags` ... カレントプロセスが使用していないfdを探索して返す - `do_filp_open` ... `struct file` に必要な情報を突っ込んで返す - `path_openat` ... `do_filp_open` の本筋 - `alloc_empty_file` ... `kmem_cache_zalloc()` で `struct file` を初期化 - `do_open` ... ここまでの情報をもとにファイルを開く - `vfs_open` ... 仮想ファイルシステムに問い合わせ，実際にファイルを開く - `fd_install` ... 新しい `struct file` をfdtableに登録する - `putname` ... `kmem_cache_free` を呼んで `struct filename` を開放 2.2.2 SYSCALL_DEFINE3システムコールの実装を追う場合， まずは SYSCALL_DEFINEN マクロ関数の呼び出しを見ると良いです． 例えばopen(2) であれば，/fs/open.c#1192 にあります． SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode) { if (force_o_largefile()) flags |= O_LARGEFILE; return do_sys_open(AT_FDCWD, filename, flags, mode); } force_o_largefile() マクロは，プロセスのパーソナリティを調べます． PER_LINUX32 フラグが立っていなければtrueが返るという処理で， 恐らく32bit Linuxかどうかのチェックだと思います． O_LARGEFILE フラグについては説明していませんでしたが， ファイルサイズが32bit幅で表せず，64bit幅を必要とする場合に設定します． プロセスのパーソナリティによっては，このフラグが自動で設定されるということですね． 2.2.3 do_sys_open()long do_sys_open(int dfd, const char __user *filename, int flags, umode_t mode) { struct open_how how = build_open_how(flags, mode); return do_sys_openat2(dfd, filename, \u0026how); } build_open_how() 関数はその名の通り， open(2) の挙動を制御する構造体 struct open_how を組み立てる関数です． 組み立てたあとは open(2) の実体である do_sys_openat2() に渡して呼び出します． dfd には AT_FDCWD が渡されています． これは， open(2) は dfd に AT_FDCWD を指定した openat(2) と実質的に同じ動作をするからです． 2.2.4 build_open_how()inline struct open_how build_open_how(int flags, umode_t mode) { struct open_how how = { .flags = flags \u0026 VALID_OPEN_FLAGS, .mode = mode \u0026 S_IALLUGO, }; /* O_PATH beats everything else. */ if (how.flags \u0026 O_PATH) how.flags \u0026= O_PATH_FLAGS; /* Modes should only be set for create-like flags. */ if (!WILL_CREATE(how.flags)) how.mode = 0; return how; } WILL_CREATE() マクロによってファイルが作成されるかどうかのチェックが行われます． そうでない場合 struct open_how.mode は使用されません． O_PATH_FLAGS マクロは (O_DIRECTORY | O_NOFOLLOW | O_PATH | O_CLOEXEC) に展開されます． つまり， O_PATH フラグを渡した時点で，上記以外のフラグは全て無視されます． 2.2.5 do_sys_openat2()static long do_sys_openat2(int dfd, const char __user *filename, struct open_how *how) { struct open_flags op; int fd = build_open_flags(how, \u0026op); struct filename *tmp; if (fd) return fd; tmp = getname(filename); if (IS_ERR(tmp)) return PTR_ERR(tmp); fd = get_unused_fd_flags(how-\u003eflags); if (fd \u003e= 0) { struct file *f = do_filp_open(dfd, tmp, \u0026op); if (IS_ERR(f)) { put_unused_fd(fd); fd = PTR_ERR(f); } else { fsnotify_open(f); fd_install(fd, f); } } putname(tmp); return fd; } build_open_flags によって，詳細なフラグの検証と設定が行われます． その後 get_unused_fd_flags で未使用のファイルディスクリプタを探索，取得します． 実際にファイルを開くのは do_filp_open という手続きです． ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:2:2","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#簡単なコールツリー"},{"categories":null,"content":" 2.2 “大まか\"なコードリーディングそれでは実際に open(2) の中身に入っていきます． 全ての関数を深堀りするわけではなく，あくまで大まかな理解にとどめます． 2.2.1 簡単なコールツリー簡単にコールツリーを書き起こしておきました． 私が後々本格的にコードリーディングする場合に使用するつもりで作りましたが， よかったら参考にしてください． - `sys_open` ... `open(2)` のカーネル側エントリポイント - `force_o_largefile` ... プロセスの実行ドメインを検証し，必要に応じて `O_LARGEFILE` を設定 - `do_sys_open` ... `struct open_how` の構築 - `do_sys_openat2` ... `open(2)` の実体 - `build_open_flags` ... より詳細なフラグの設定，検証 - `getname` ... ユーザプロセス空間の`filename` を取得する - `get_unused_fd_flags` ... カレントプロセスが使用していないfdを探索して返す - `do_filp_open` ... `struct file` に必要な情報を突っ込んで返す - `path_openat` ... `do_filp_open` の本筋 - `alloc_empty_file` ... `kmem_cache_zalloc()` で `struct file` を初期化 - `do_open` ... ここまでの情報をもとにファイルを開く - `vfs_open` ... 仮想ファイルシステムに問い合わせ，実際にファイルを開く - `fd_install` ... 新しい `struct file` をfdtableに登録する - `putname` ... `kmem_cache_free` を呼んで `struct filename` を開放 2.2.2 SYSCALL_DEFINE3システムコールの実装を追う場合， まずは SYSCALL_DEFINEN マクロ関数の呼び出しを見ると良いです． 例えばopen(2) であれば，/fs/open.c#1192 にあります． SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode) { if (force_o_largefile()) flags |= O_LARGEFILE; return do_sys_open(AT_FDCWD, filename, flags, mode); } force_o_largefile() マクロは，プロセスのパーソナリティを調べます． PER_LINUX32 フラグが立っていなければtrueが返るという処理で， 恐らく32bit Linuxかどうかのチェックだと思います． O_LARGEFILE フラグについては説明していませんでしたが， ファイルサイズが32bit幅で表せず，64bit幅を必要とする場合に設定します． プロセスのパーソナリティによっては，このフラグが自動で設定されるということですね． 2.2.3 do_sys_open()long do_sys_open(int dfd, const char __user *filename, int flags, umode_t mode) { struct open_how how = build_open_how(flags, mode); return do_sys_openat2(dfd, filename, \u0026how); } build_open_how() 関数はその名の通り， open(2) の挙動を制御する構造体 struct open_how を組み立てる関数です． 組み立てたあとは open(2) の実体である do_sys_openat2() に渡して呼び出します． dfd には AT_FDCWD が渡されています． これは， open(2) は dfd に AT_FDCWD を指定した openat(2) と実質的に同じ動作をするからです． 2.2.4 build_open_how()inline struct open_how build_open_how(int flags, umode_t mode) { struct open_how how = { .flags = flags \u0026 VALID_OPEN_FLAGS, .mode = mode \u0026 S_IALLUGO, }; /* O_PATH beats everything else. */ if (how.flags \u0026 O_PATH) how.flags \u0026= O_PATH_FLAGS; /* Modes should only be set for create-like flags. */ if (!WILL_CREATE(how.flags)) how.mode = 0; return how; } WILL_CREATE() マクロによってファイルが作成されるかどうかのチェックが行われます． そうでない場合 struct open_how.mode は使用されません． O_PATH_FLAGS マクロは (O_DIRECTORY | O_NOFOLLOW | O_PATH | O_CLOEXEC) に展開されます． つまり， O_PATH フラグを渡した時点で，上記以外のフラグは全て無視されます． 2.2.5 do_sys_openat2()static long do_sys_openat2(int dfd, const char __user *filename, struct open_how *how) { struct open_flags op; int fd = build_open_flags(how, \u0026op); struct filename *tmp; if (fd) return fd; tmp = getname(filename); if (IS_ERR(tmp)) return PTR_ERR(tmp); fd = get_unused_fd_flags(how-\u003eflags); if (fd \u003e= 0) { struct file *f = do_filp_open(dfd, tmp, \u0026op); if (IS_ERR(f)) { put_unused_fd(fd); fd = PTR_ERR(f); } else { fsnotify_open(f); fd_install(fd, f); } } putname(tmp); return fd; } build_open_flags によって，詳細なフラグの検証と設定が行われます． その後 get_unused_fd_flags で未使用のファイルディスクリプタを探索，取得します． 実際にファイルを開くのは do_filp_open という手続きです． ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:2:2","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#syscall_define3"},{"categories":null,"content":" 2.2 “大まか\"なコードリーディングそれでは実際に open(2) の中身に入っていきます． 全ての関数を深堀りするわけではなく，あくまで大まかな理解にとどめます． 2.2.1 簡単なコールツリー簡単にコールツリーを書き起こしておきました． 私が後々本格的にコードリーディングする場合に使用するつもりで作りましたが， よかったら参考にしてください． - `sys_open` ... `open(2)` のカーネル側エントリポイント - `force_o_largefile` ... プロセスの実行ドメインを検証し，必要に応じて `O_LARGEFILE` を設定 - `do_sys_open` ... `struct open_how` の構築 - `do_sys_openat2` ... `open(2)` の実体 - `build_open_flags` ... より詳細なフラグの設定，検証 - `getname` ... ユーザプロセス空間の`filename` を取得する - `get_unused_fd_flags` ... カレントプロセスが使用していないfdを探索して返す - `do_filp_open` ... `struct file` に必要な情報を突っ込んで返す - `path_openat` ... `do_filp_open` の本筋 - `alloc_empty_file` ... `kmem_cache_zalloc()` で `struct file` を初期化 - `do_open` ... ここまでの情報をもとにファイルを開く - `vfs_open` ... 仮想ファイルシステムに問い合わせ，実際にファイルを開く - `fd_install` ... 新しい `struct file` をfdtableに登録する - `putname` ... `kmem_cache_free` を呼んで `struct filename` を開放 2.2.2 SYSCALL_DEFINE3システムコールの実装を追う場合， まずは SYSCALL_DEFINEN マクロ関数の呼び出しを見ると良いです． 例えばopen(2) であれば，/fs/open.c#1192 にあります． SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode) { if (force_o_largefile()) flags |= O_LARGEFILE; return do_sys_open(AT_FDCWD, filename, flags, mode); } force_o_largefile() マクロは，プロセスのパーソナリティを調べます． PER_LINUX32 フラグが立っていなければtrueが返るという処理で， 恐らく32bit Linuxかどうかのチェックだと思います． O_LARGEFILE フラグについては説明していませんでしたが， ファイルサイズが32bit幅で表せず，64bit幅を必要とする場合に設定します． プロセスのパーソナリティによっては，このフラグが自動で設定されるということですね． 2.2.3 do_sys_open()long do_sys_open(int dfd, const char __user *filename, int flags, umode_t mode) { struct open_how how = build_open_how(flags, mode); return do_sys_openat2(dfd, filename, \u0026how); } build_open_how() 関数はその名の通り， open(2) の挙動を制御する構造体 struct open_how を組み立てる関数です． 組み立てたあとは open(2) の実体である do_sys_openat2() に渡して呼び出します． dfd には AT_FDCWD が渡されています． これは， open(2) は dfd に AT_FDCWD を指定した openat(2) と実質的に同じ動作をするからです． 2.2.4 build_open_how()inline struct open_how build_open_how(int flags, umode_t mode) { struct open_how how = { .flags = flags \u0026 VALID_OPEN_FLAGS, .mode = mode \u0026 S_IALLUGO, }; /* O_PATH beats everything else. */ if (how.flags \u0026 O_PATH) how.flags \u0026= O_PATH_FLAGS; /* Modes should only be set for create-like flags. */ if (!WILL_CREATE(how.flags)) how.mode = 0; return how; } WILL_CREATE() マクロによってファイルが作成されるかどうかのチェックが行われます． そうでない場合 struct open_how.mode は使用されません． O_PATH_FLAGS マクロは (O_DIRECTORY | O_NOFOLLOW | O_PATH | O_CLOEXEC) に展開されます． つまり， O_PATH フラグを渡した時点で，上記以外のフラグは全て無視されます． 2.2.5 do_sys_openat2()static long do_sys_openat2(int dfd, const char __user *filename, struct open_how *how) { struct open_flags op; int fd = build_open_flags(how, \u0026op); struct filename *tmp; if (fd) return fd; tmp = getname(filename); if (IS_ERR(tmp)) return PTR_ERR(tmp); fd = get_unused_fd_flags(how-\u003eflags); if (fd \u003e= 0) { struct file *f = do_filp_open(dfd, tmp, \u0026op); if (IS_ERR(f)) { put_unused_fd(fd); fd = PTR_ERR(f); } else { fsnotify_open(f); fd_install(fd, f); } } putname(tmp); return fd; } build_open_flags によって，詳細なフラグの検証と設定が行われます． その後 get_unused_fd_flags で未使用のファイルディスクリプタを探索，取得します． 実際にファイルを開くのは do_filp_open という手続きです． ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:2:2","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#do_sys_open"},{"categories":null,"content":" 2.2 “大まか\"なコードリーディングそれでは実際に open(2) の中身に入っていきます． 全ての関数を深堀りするわけではなく，あくまで大まかな理解にとどめます． 2.2.1 簡単なコールツリー簡単にコールツリーを書き起こしておきました． 私が後々本格的にコードリーディングする場合に使用するつもりで作りましたが， よかったら参考にしてください． - `sys_open` ... `open(2)` のカーネル側エントリポイント - `force_o_largefile` ... プロセスの実行ドメインを検証し，必要に応じて `O_LARGEFILE` を設定 - `do_sys_open` ... `struct open_how` の構築 - `do_sys_openat2` ... `open(2)` の実体 - `build_open_flags` ... より詳細なフラグの設定，検証 - `getname` ... ユーザプロセス空間の`filename` を取得する - `get_unused_fd_flags` ... カレントプロセスが使用していないfdを探索して返す - `do_filp_open` ... `struct file` に必要な情報を突っ込んで返す - `path_openat` ... `do_filp_open` の本筋 - `alloc_empty_file` ... `kmem_cache_zalloc()` で `struct file` を初期化 - `do_open` ... ここまでの情報をもとにファイルを開く - `vfs_open` ... 仮想ファイルシステムに問い合わせ，実際にファイルを開く - `fd_install` ... 新しい `struct file` をfdtableに登録する - `putname` ... `kmem_cache_free` を呼んで `struct filename` を開放 2.2.2 SYSCALL_DEFINE3システムコールの実装を追う場合， まずは SYSCALL_DEFINEN マクロ関数の呼び出しを見ると良いです． 例えばopen(2) であれば，/fs/open.c#1192 にあります． SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode) { if (force_o_largefile()) flags |= O_LARGEFILE; return do_sys_open(AT_FDCWD, filename, flags, mode); } force_o_largefile() マクロは，プロセスのパーソナリティを調べます． PER_LINUX32 フラグが立っていなければtrueが返るという処理で， 恐らく32bit Linuxかどうかのチェックだと思います． O_LARGEFILE フラグについては説明していませんでしたが， ファイルサイズが32bit幅で表せず，64bit幅を必要とする場合に設定します． プロセスのパーソナリティによっては，このフラグが自動で設定されるということですね． 2.2.3 do_sys_open()long do_sys_open(int dfd, const char __user *filename, int flags, umode_t mode) { struct open_how how = build_open_how(flags, mode); return do_sys_openat2(dfd, filename, \u0026how); } build_open_how() 関数はその名の通り， open(2) の挙動を制御する構造体 struct open_how を組み立てる関数です． 組み立てたあとは open(2) の実体である do_sys_openat2() に渡して呼び出します． dfd には AT_FDCWD が渡されています． これは， open(2) は dfd に AT_FDCWD を指定した openat(2) と実質的に同じ動作をするからです． 2.2.4 build_open_how()inline struct open_how build_open_how(int flags, umode_t mode) { struct open_how how = { .flags = flags \u0026 VALID_OPEN_FLAGS, .mode = mode \u0026 S_IALLUGO, }; /* O_PATH beats everything else. */ if (how.flags \u0026 O_PATH) how.flags \u0026= O_PATH_FLAGS; /* Modes should only be set for create-like flags. */ if (!WILL_CREATE(how.flags)) how.mode = 0; return how; } WILL_CREATE() マクロによってファイルが作成されるかどうかのチェックが行われます． そうでない場合 struct open_how.mode は使用されません． O_PATH_FLAGS マクロは (O_DIRECTORY | O_NOFOLLOW | O_PATH | O_CLOEXEC) に展開されます． つまり， O_PATH フラグを渡した時点で，上記以外のフラグは全て無視されます． 2.2.5 do_sys_openat2()static long do_sys_openat2(int dfd, const char __user *filename, struct open_how *how) { struct open_flags op; int fd = build_open_flags(how, \u0026op); struct filename *tmp; if (fd) return fd; tmp = getname(filename); if (IS_ERR(tmp)) return PTR_ERR(tmp); fd = get_unused_fd_flags(how-\u003eflags); if (fd \u003e= 0) { struct file *f = do_filp_open(dfd, tmp, \u0026op); if (IS_ERR(f)) { put_unused_fd(fd); fd = PTR_ERR(f); } else { fsnotify_open(f); fd_install(fd, f); } } putname(tmp); return fd; } build_open_flags によって，詳細なフラグの検証と設定が行われます． その後 get_unused_fd_flags で未使用のファイルディスクリプタを探索，取得します． 実際にファイルを開くのは do_filp_open という手続きです． ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:2:2","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#build_open_how"},{"categories":null,"content":" 2.2 “大まか\"なコードリーディングそれでは実際に open(2) の中身に入っていきます． 全ての関数を深堀りするわけではなく，あくまで大まかな理解にとどめます． 2.2.1 簡単なコールツリー簡単にコールツリーを書き起こしておきました． 私が後々本格的にコードリーディングする場合に使用するつもりで作りましたが， よかったら参考にしてください． - `sys_open` ... `open(2)` のカーネル側エントリポイント - `force_o_largefile` ... プロセスの実行ドメインを検証し，必要に応じて `O_LARGEFILE` を設定 - `do_sys_open` ... `struct open_how` の構築 - `do_sys_openat2` ... `open(2)` の実体 - `build_open_flags` ... より詳細なフラグの設定，検証 - `getname` ... ユーザプロセス空間の`filename` を取得する - `get_unused_fd_flags` ... カレントプロセスが使用していないfdを探索して返す - `do_filp_open` ... `struct file` に必要な情報を突っ込んで返す - `path_openat` ... `do_filp_open` の本筋 - `alloc_empty_file` ... `kmem_cache_zalloc()` で `struct file` を初期化 - `do_open` ... ここまでの情報をもとにファイルを開く - `vfs_open` ... 仮想ファイルシステムに問い合わせ，実際にファイルを開く - `fd_install` ... 新しい `struct file` をfdtableに登録する - `putname` ... `kmem_cache_free` を呼んで `struct filename` を開放 2.2.2 SYSCALL_DEFINE3システムコールの実装を追う場合， まずは SYSCALL_DEFINEN マクロ関数の呼び出しを見ると良いです． 例えばopen(2) であれば，/fs/open.c#1192 にあります． SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, umode_t, mode) { if (force_o_largefile()) flags |= O_LARGEFILE; return do_sys_open(AT_FDCWD, filename, flags, mode); } force_o_largefile() マクロは，プロセスのパーソナリティを調べます． PER_LINUX32 フラグが立っていなければtrueが返るという処理で， 恐らく32bit Linuxかどうかのチェックだと思います． O_LARGEFILE フラグについては説明していませんでしたが， ファイルサイズが32bit幅で表せず，64bit幅を必要とする場合に設定します． プロセスのパーソナリティによっては，このフラグが自動で設定されるということですね． 2.2.3 do_sys_open()long do_sys_open(int dfd, const char __user *filename, int flags, umode_t mode) { struct open_how how = build_open_how(flags, mode); return do_sys_openat2(dfd, filename, \u0026how); } build_open_how() 関数はその名の通り， open(2) の挙動を制御する構造体 struct open_how を組み立てる関数です． 組み立てたあとは open(2) の実体である do_sys_openat2() に渡して呼び出します． dfd には AT_FDCWD が渡されています． これは， open(2) は dfd に AT_FDCWD を指定した openat(2) と実質的に同じ動作をするからです． 2.2.4 build_open_how()inline struct open_how build_open_how(int flags, umode_t mode) { struct open_how how = { .flags = flags \u0026 VALID_OPEN_FLAGS, .mode = mode \u0026 S_IALLUGO, }; /* O_PATH beats everything else. */ if (how.flags \u0026 O_PATH) how.flags \u0026= O_PATH_FLAGS; /* Modes should only be set for create-like flags. */ if (!WILL_CREATE(how.flags)) how.mode = 0; return how; } WILL_CREATE() マクロによってファイルが作成されるかどうかのチェックが行われます． そうでない場合 struct open_how.mode は使用されません． O_PATH_FLAGS マクロは (O_DIRECTORY | O_NOFOLLOW | O_PATH | O_CLOEXEC) に展開されます． つまり， O_PATH フラグを渡した時点で，上記以外のフラグは全て無視されます． 2.2.5 do_sys_openat2()static long do_sys_openat2(int dfd, const char __user *filename, struct open_how *how) { struct open_flags op; int fd = build_open_flags(how, \u0026op); struct filename *tmp; if (fd) return fd; tmp = getname(filename); if (IS_ERR(tmp)) return PTR_ERR(tmp); fd = get_unused_fd_flags(how-\u003eflags); if (fd \u003e= 0) { struct file *f = do_filp_open(dfd, tmp, \u0026op); if (IS_ERR(f)) { put_unused_fd(fd); fd = PTR_ERR(f); } else { fsnotify_open(f); fd_install(fd, f); } } putname(tmp); return fd; } build_open_flags によって，詳細なフラグの検証と設定が行われます． その後 get_unused_fd_flags で未使用のファイルディスクリプタを探索，取得します． 実際にファイルを開くのは do_filp_open という手続きです． ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:2:2","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#do_sys_openat2"},{"categories":null,"content":" 3 まとめこの記事では， open(2) システムコールについての知識を整理しました． カーネルのコードはすべてを紹介していませんが， 大まかに把握することで詳細なコードリーディングをする時の助けになると思います． この調子でLinuxの勉強もがんばります． ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:3:0","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#まとめ"},{"categories":null,"content":" 4 参考 open(2) — Linux manual page Man page of OPEN Virtual File System Linux source code (v5.10.9) - Bootlin ","date":"2021-01-24","objectID":"/ja/posts/recap-linux-open-syscall/:4:0","series":null,"tags":["c","linux"],"title":"Linuxのopen(2) syscallをもう一度復習する","uri":"/ja/posts/recap-linux-open-syscall/#参考"},{"categories":null,"content":"この記事は IPFactory Advent Calendar 2020 の20日目です． このネタを 言語実装 Advent Calendar 2020 の記事に採用すればよかったかもしれない． 私は1日目にも記事を上げていますので，興味があればそちらも見てください． 言語処理系の勉強をしていればほぼ100%，そうでなくても一度は目にしたことがあるであろう，“型システム” という言葉． 何やらかっこいい名前ですが，どういったものかは理解していても実装方法まで知っている人は多くありません． かくいう私も，今まで作ってきた言語達はすべて “型の明示を強制する” 言語だったので経験はありませんでした． また，そもそも型システムについて勉強しようという気になったことがありませんでした． そんな私が SecHack365 に参加していたとき， 同期の方 が サクッと実装 していて大変びっくりした記憶があります． “よくわかんなければ作りましょう，作ったことなきゃ作りましょう” が私の技術に対するモチベーションなので， 記事読んで満足するだけじゃなく，ちゃんと自分でも作ってみないとね，と思っていました(そして数千年の時が経ちました)． 本記事はあくまで “型システムを実装するための記事，実際にRustで実装した記事” になるので， 型システムについての詳細な解説等は行いません． 記事末尾に参考資料を記載しておくので， そちらをご覧いただければと思います． 当方型システムについての勉強はこれが初めてなので，間違っている点もあるかもしれません． その際は是非コメント等で教えていただけると助かります． ソースコード全体がGitHubで見れるようになっているので，そちらも是非． ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:0:0","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#"},{"categories":null,"content":" 1 対象読者 一番シンプルな型推論アルゴリズムを知りたい!という人 Rustで型システムをどうやって作るのか知りたい人 ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:1:0","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#対象読者"},{"categories":null,"content":" 2 対象でない読者読んでほしくない，というわけではなく， このレベルに該当する人にとっては退屈かもよ，という意味です． 型推論アルゴリズムの実装をしたことがある人 型システムの論文をよく読む人 関数型言語の実装をよくやる人 その他詳しい人 ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:2:0","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#対象でない読者"},{"categories":null,"content":" 3 型システムについての前提知識ここでは最低限，型システム関連の情報を整理しておきます． 後に，それら概念や知識を使って実装の解説をしていきます． また，本記事の後，型システムに関する論文や他記事を読む為の助けにもなるでしょう(なるといいな)． 興味のない人は飛ばしてください． 基本的には，こちらのページ等に書いてある知識の要約であり，n番煎じです． 間違っていたらコメントお願いします． ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:3:0","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#型システムについての前提知識"},{"categories":null,"content":" 3.1 型システムの用語まず， “型環境( type environment ) Γ 下においてプログラム中の式 e が型 T を持つ” という表現を 型判断 (type judgement ) といいます． これは，以下のように書きます． type judgement は， 型付け規則( typing rule ) というものを使って導出されます． イメージとしては， “こんなコンテキストで前提xxが全て導出できれば，結論xx” みたいな感じです． 下に示した型付け規則は， “型環境 Γ 下において x が σ 型を持つと言えるとき，そのようにして扱える” みたいな意味になります． 導出中において\"未知の変数\"を表すために 型変数 (type variable) という概念が用いられます． これにより，関数適用 ((fn x -\u003e x) 3) のような式についても推論が可能になります． a0 -\u003e a0 な関数に対し int を適用すると，最終的に int -\u003e int が導出されます． このとき， “型変数とその正体の対応関係” を 型代入( type substitution ) と呼びます． ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:3:1","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#型システムの用語"},{"categories":null,"content":" 3.2 単一化ここで，fn x -\u003e x + 3 という関数という推論について考えます ． 二項演算のオペランドである x, 3 の推論結果はそれぞれ a0, int となります． a0 と int は単純比較すると異なるように見えますが， a0 = int であればこの関数はvalidであることがわかります． これを解決するために， 単一化( unification ) という操作を行います． S(a0) = int となるような型代入を得ることができれば，上式は推論可能であることがわかります． ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:3:2","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#単一化"},{"categories":null,"content":" 3.3 単相と多相ここまでの仕組みを実装して得られる型推論器を “単相的である” と表現します． let f = fn x -\u003e x + 3 in f 4 のようなプログラムでは， f :: 'a -\u003e 'a というシグネチャが 'a = int という型代入を持ってして， “let式 let f = fn x -\u003e x + 3 in f 4 は型 int を持つ” という型判断が得られました． しかし，こちら で取り上げられているように， let f = fn x -\u003e x in if f true then f 2 else 3 のような式が推論できません． しかし，Haskellなどの関数型言語ではこのような使い方も可能です． Haskellを含むいくつかの言語は多相を実現するための言語機能を持っています． (コードの意味は特にありません) main = do let r = let f = (\\x -\u003e x) in if f True then f 3 else f 5 print r これを実現するために導入されるのが let多相( let-polymorphism ) という概念です． Haskellでは多相的なプログラムを構築することが可能です． 同様に 型変数( variable ) という概念が存在しますが， それらは 全称量化 されていると考えることができます． 推論途中の\"未知変数\"を表す型変数とは区別して， 型スキーム( type scheme ) と呼ばれます． ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:3:3","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#単相と多相"},{"categories":null,"content":" 4 型推論システムの実装型推論を実現するいくつかのアルゴリズムを紹介します． 他にもあったら教えて下さい． MinCamlの型推論 住井先生のMinCaml で用いられている型推論アルゴリズム 単相型推論となっている 今回はこちらのアルゴリズム(の一部)を実装 Basic Polymorphic Typechecking 1987年の論文 プログラミング言語における多相についての歴史から入り，型推論アルゴリズムまで紹介 パラメトリック多相には明示的/暗黙的の，2つの実現手法があると主張 例1: Zig言語のジェネリクスはexplicit polymorphismに該当? 参考: Documentation の List 関数 例2: Haskellの型変数を用いた多相はimplicit polymorphismに該当? Algorithm W 1978年に提唱された型推論アルゴリズム 構文木をトップダウン的に探索する Algorithm M 1998年に提唱された，Wの対比となるアルゴリズム Mの前に folklore と呼ばれていたアルゴリズムがあったけど，それは証明されていなかったっぽい Wを高速化したっぽい(論文にもそう書かれてる) level-based type inference 詳細はこちら Generalized HMTI Algorithm 2002年に発表された論文に記載 WやMではエラーメッセージが有益でないとし，それらを解決する為のアルゴリズムを提唱し，完全性を証明? 今回はこのうち，MinCamlコンパイラの型推論システム(っぽいもの)を実装してみます． (いずれ全部やりたいなあ) ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:4:0","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#型推論システムの実装"},{"categories":null,"content":" 5 MinCamlコンパイラの型推論を一部実装MinCamlは多くの言語機能を有していますが， ここでは言語機能をある程度制限します． また，コードすべてを載せるととんでもないことになってしまうので一部のみ取り上げます． 全体はこちらに． cargo test を動かしていただければ雰囲気はつかめると思います． //! src/expr.rs pub enum Expr { /// `x` Variable(String), /// `42` Integer(i64), /// `true` | `false` Boolean(bool), /// `x + 3` Plus(Box\u003cExpr\u003e, Box\u003cExpr\u003e), /// `\\x -\u003e x + 3` Lambda(String, Box\u003cExpr\u003e), /// `f 3` Application(Box\u003cExpr\u003e, Box\u003cExpr\u003e), /// `let x = 3 in x + 3` Let(String, Box\u003cExpr\u003e, Box\u003cExpr\u003e), } 今回の型推論器で扱う式を表します． Rustではこのような再帰的データ構造を表現する場合， Box\u003cT\u003e を使用するのが最もシンプルなので，今回はそうしています． 他には， typed-arena のようなアロケータクレートを用いるという方法もあります． 私はこのtyped-arenaを好んでよく使っています． //! src/types.rs pub enum Type { Boolean, Integer, Fn(Box\u003cType\u003e, Box\u003cType\u003e), Variable(String), } 推論結果として使用する型です． Type::Variable は型変数であり，その名前を持ちます． 実際の推論アルゴリズムを見てみましょう． fn infer( mut env: Env, mut iter: RangeInclusive\u003cchar\u003e, e: Expr, ) -\u003e Result\u003c(Env, RangeInclusive\u003cchar\u003e, Type), InferenceError\u003e { match e { Expr::Boolean(_b) =\u003e { /* stripped */ }, Expr::Integer(_v) =\u003e { /* stripped */ }, Expr::Variable(name) =\u003e { /* stripped */ }, Expr::Let(x, e1, e2) =\u003e { /* stripped */ }, Expr::Lambda(var, expr) =\u003e { /* stripped */ } Expr::Application(f, param) =\u003e { /* stripped */ } Expr::Plus(lhs, rhs) =\u003e { /* stripped */ } } } 第一引数の env: Env は，型変数から実際の型や， Expr::Lambdaに登場する束縛変数から型を導出するために使用します． MinCamlでは Type.t.Var が Type.t option を持っており， 導出結果を型自体に保存する手法を取っていますが， 今回はハッシュマップを持って取り回す方がシンプルに実装できそうだったので，そうしています． しかし，envが “関数の引数に関する型環境” と 型変数の代入 という2つの意味を持って使用されてしまうので，少し読みづらいかもしれません． 区別して読みやすくするために，単純なハッシュマップではなく，それをラップする構造体を定義しています(src/types.rsの struct Env を参照)． 第2引数の RangeInclusive\u003cchar\u003e は 'a'..='z' というrange objectを生成して渡しています． これは型変数名のジェネレータです． 他の実装では，呼び出すたびに+1される作用を持った関数を実装して， a0, a1, a2, ..., an という名前を生成する物を見つけました． Rustではイテレータを使う方が良さそうだったので，そうしています． count: RefCell\u003cusize\u003e 等の参照を infer() に渡せば同様の事が出来そうです． 第3引数は推論の対象となるノードです． パターンマッチを行って，式の種類ごとに分岐しています． トップダウン的に推論を行うアルゴリズムですが， 解説はボトムアップに行おうと思います． 実際のコードについては， src/inference.rs に定義されたテストも合わせてご覧いただければと思います． ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:5:0","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#mincamlコンパイラの型推論を一部実装"},{"categories":null,"content":" 5.1 リテラルに対する推論これは説明するまでもないですね． match e { Expr::Boolean(_b) =\u003e Ok((env, iter, Type::Boolean)), Expr::Integer(_v) =\u003e Ok((env, iter, Type::Integer)), } 対応する型をそのまま返しています． ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:5:1","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#リテラルに対する推論"},{"categories":null,"content":" 5.2 変数に対する推論match e { Expr::Variable(name) =\u003e match env.vars_in_fn.get(\u0026name) { Some(var_ty) =\u003e Ok((env.clone(), iter, var_ty.clone())), None =\u003e Err(InferenceError::NotFoundSuchAVariable { v: name.to_string() }), }, } 後に示す Expr::Lambda(var, expr) に対する推論時に， env.vars_in_fn.insert(var, new_type_var) が行われ，更新された env が渡されます． λx.x のようなラムダ式の場合， x =\u003e a のような\"変数と型変数の対応\"がマップに存在するので， その対応が存在すれば取り出し，そうでなければエラーを返しています． MinCamlでは外部変数もうまく扱えるようになっていますが(自由変数のキャプチャも推論出来る)， 今回は実装をシンプルにするためにその機能は無視しています． ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:5:2","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#変数に対する推論"},{"categories":null,"content":" 5.3 let式に対する推論match e { Expr::Let(x, e1, e2) =\u003e { let (mut env, iter, e1_t) = infer(env, iter, *e1)?; env.vars_in_fn.insert(x, e1_t); infer(env, iter, *e2) } } 変数に代入する式 e1 を推論して，変数の型が得られます． それを env に登録した状態で，変数の使用部分である e2 を推論するだけです． 実はこれだけでネストした let の推論等も動いてしまいます． ここまで説明した内容を元に，let x = 3 in let y = x in y という式を理解してみましょう． 階層構造的には， Let(\"x\", 3, Let(\"y\", \"x\", \"y\")) となっている点に注目すると良いです． 3 が推論され， env に x =\u003e int が登録される let y = x in y の推論開始 x が推論される． env をlookupして， int が返される env に y =\u003e int が登録される y が推論され，int が返される．これがinner-let-exprの型となる inner-let-exprの型がouter-let-exprの型となる 言語処理系の実装をする人にとって再帰関数は馴染み深いものですが， 何度作っても魔法のように見えますね． ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:5:3","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#let式に対する推論"},{"categories":null,"content":" 5.4 λ抽象に対する推論match e { Expr::Lambda(var, expr) =\u003e { let new_type_var = iter.next().unwrap().to_string(); let new_type_var = Type::Variable(new_type_var); env.vars_in_fn.insert(var.to_string(), new_type_var.clone()); let (env, iter, expr_ty) = infer(env, iter, *expr)?; Ok(( env, iter, Type::Fn(Box::new(new_type_var), Box::new(expr_ty)), )) } } id 関数を例に考えましょう． λx.x に対する推論は，次のようになります． 新しく型変数 a を作る env に x =\u003e a を登録する(これにより，expr に登場する束縛変数の型を推論できる) x に対する推論を行う．2番の操作により，a 型が得られる a =\u003e a な型を返す この関数型に登場する型変数aは，実際に適用されるまでわかりません． また，この関数 a はあくまでも “未知である単一の型” である点に注意です． ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:5:4","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#λ抽象に対する推論"},{"categories":null,"content":" 5.5 関数適用に対する推論match e { Expr::Application(f, param) =\u003e { let new_type_var_name = iter.next().unwrap().to_string(); let new_type_var = Type::Variable(new_type_var_name.clone()); let (env, iter, fn_ty) = infer(env, iter, *f)?; let (env, iter, param_ty) = infer(env, iter, *param)?; let env = unify( env, fn_ty, Type::Fn(Box::new(param_ty.clone()), Box::new(new_type_var)), )?; if let Some(resolved_ty) = env.type_vars..get(\u0026new_type_var_name) { return Ok((env.clone(), iter, resolved_ty.clone())); } Ok((env, iter, param_ty)) } } 少し長いので複雑に見えますが，一つ一つじっくり追っていきましょう． やはり実例がわかりやすいと思うので，((λx.x) 3) について考えます． ASTを書き下すと， Apply(Lambda(\"x\", \"x\"), 3) という感じです． まず，新たな型変数 a を作ります． そして，λx.x に対して infer() を呼び出します． 先程の解説から，この推論が b =\u003e b のような関数型を返すことがわかっています． そして，引数の3に対する推論で int が得られます． 最後に，2つの型 ((b =\u003e b), (int =\u003e a)) に対して unify() を呼び出します． このような呼び出しになっている理由は， f の推論結果である b =\u003e b の b を int で置換したとき， ((int =\u003e int), (int =\u003e a)) となるかどうかをチェックしたい為です． すぐ後に説明しますが，unify() 内部では未知の型変数に対する代入(substitution)が行われる為， a =\u003e int もすぐに判明します． int =\u003e a な関数に対する適用とわかったところで， a の型が既に判明しているかどうかenvに問い合わせます． unify 関数について定義を示します． 少しキレイな書き方ではなくなってしまったので，概要だけ説明します． 詳細に知りたい方はGitHubを御覧ください． fn unify( mut env: Env, t1: Type, t2: Type, ) -\u003e Result\u003cEnv, InferenceError\u003e { match (t1.clone(), t2.clone()) { // シンプルな比較 (Type::Integer, Type::Integer) | (Type::Boolean, Type::Boolean) =\u003e Ok(env), (Type::Fn(var_ty1, ret_ty1), Type::Fn(var_ty2, ret_ty2)) =\u003e { // 引数同士，返り値同士で型の比較 let env = unify(env, *var_ty1, *var_ty2)?; unify(env, *ret_ty1, *ret_ty2) } (Type::Variable(name1), Type::Variable(name2)) if name1 == name2 =\u003e Ok(env), // 一方が型変数の場合を調べる (Type::Variable(var), _) =\u003e { // 定義済み(割り当て済み)の場合，単純比較 if let Some(var_t) = env.type_vars.get(\u0026var) { return unify(env.clone(), var_t.clone(), t2); } // 未定義(未知)の場合，occur check後代入 if occur(\u0026var, \u0026t2) { return Err(InferenceError::FoundOccurrence); } env.type_vars.insert(var.to_string(), t2); Ok(env) } (_, Type::Variable(var)) =\u003e { /* 上記と同様にチェック */ } _ =\u003e Err(InferenceError::CannotUnify), } } 渡された2つの型が等しいかチェック どちらか一方が型変数の場合，occur checkを行った後代入 occur checkとは， t1: int =\u003e a, t2: a な場合等で a =\u003e (int =\u003e a)としてしまうと無限ループに陥ってしまうので，そういったケースの検出をする手続き をする関数だということだけ理解していただければ問題ありません． ちょっとぐちゃっとなってしまったので，まとめましょう ((λx.x) 3) に対する推論はじめ この関数適用の結果 a が得られるとして型変数を持っておく λx.x の型が b =\u003e b だとわかる 3 の型が int だとわかる ここまでで， int =\u003e a という関数に対する適用だとわかる unify((b =\u003e b), (int =\u003e a)) を呼び出す unify(b, int) が呼ばれ， b =\u003e int がenvに登録される unify(b, a) が呼ばれ，b =\u003e int がわかっているので， unify(int, a) としてもっかい呼ぶ a =\u003e int が登録される int =\u003e int として導出できたので，返り値型である int を返す という感じです． かなり複雑でしたが，実装することで理解が深まりました． ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:5:5","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#関数適用に対する推論"},{"categories":null,"content":" 5.6 二項演算に対する推論おまけmatch e { Expr::Plus(lhs, rhs) =\u003e { let (env, iter, lhs_ty) = infer(env, iter, *lhs)?; let (env, iter, rhs_ty) = infer(env, iter, *rhs)?; let env = unify(env, Type::Integer, lhs_ty.clone())?; let env = unify(env, Type::Integer, rhs_ty.clone())?; if let (Type::Variable(var), _) = (\u0026lhs_ty, \u0026rhs_ty) { let resolved_ty = env.type_vars.get(var).unwrap().clone(); return Ok((env, iter, resolved_ty)); } if let (_, Type::Variable(var)) = (\u0026lhs_ty, \u0026rhs_ty) { let resolved_ty = env.type_vars.get(var).unwrap().clone(); return Ok((env, iter, resolved_ty)); } Ok((env, iter, lhs_ty)) } } + 演算子を，2つの引数を取る中置関数だと考えると，ほぼ同じことをしているとわかります． しかし + は今回想定する言語では int しか引数を持たないので， unify の呼び出し回数は少なくて済みます． ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:5:6","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#二項演算に対する推論おまけ"},{"categories":null,"content":" 5.7 MinCamlコンパイラの型推論を一部実装所感150行程度の実装でしたが比較的複雑で，実装にもある程度時間がかかってしまいました． OCamlで実装されたコードをRustに変換するとき， Rustの知識が足りないせいであまりキレイじゃない書き方になってしまい，若干悔しい思いをしています． Rustでもっと関数型っぽい書き方に慣れていきたいところですね． 型推論アルゴリズムというとあれですが， 自作言語では let x : i64 = x + 3; みたいな代入に対して， “右辺がちゃんと宣言通りの型を持っているか\"みたいな型検査の実装をしたことがあったので，少し親近感はありました． しかし unify() はやはり複雑でしたし，ちゃんとテストが通ったときは凄いびっくりしました． ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:5:7","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#mincamlコンパイラの型推論を一部実装所感"},{"categories":null,"content":" 6 WIP: 多相型推論システムの実装時間が足りずできませんでした(無念…) 後日別記事にまとめてあげようかな，なんて思っていたりします． ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:6:0","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#wip-多相型推論システムの実装"},{"categories":null,"content":" 7 まとめ今回はRustで単相型推論アルゴリズムを実装しつつ，お勉強してみました． 本当は多相型推論も実装しようとしたんですが何分記事のアイデアを思いついたのが期日ギリギリだったので厳しかったです． なんだかなあなあになってしまった感じがあるので，後日記事書きたいなあ，なんて思っています(できたら)． 先程も述べましたが，Rustで関数型っぽく書く力をもっと身につけたいなあ，と感じることができましたね． ここらへんRustのベストプラクティスも調べながら知っていきたい． OCamlの Map.add のように，エントリ挿入後のマップが返るようなAPIが std::collections::HashMapにもほしいなと思います(おもいませんか?) とはいえ，効率を考えたら \u0026mut HashMap\u003cK, V\u003e でごにょごにょするほうがいい気もします． うーん，難しい． ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:7:0","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#まとめ"},{"categories":null,"content":" 8 参考資料 Wikipedia 単相型/多相型についての解説も記載 具体的な定義とかが書いてある このページの参考資料にある論文とか読むと良さそう(私は読んでません) Hindley-Milner型推論をCで実装した話 記事が比較的新しめ ラムダ計算の知識も説明されているので，前提知識のない人におすすめ let f = λx -\u003e x in ((pair (f 200)) (f true)) サンプルからもわかるように，多相が動いている C言語で実装されているので，他言語よりも敷居が低い Haskell/Scala読める人よりC読める人の方が多いんじゃないか?という予想からの発言です 型システムを学ぼう！ Haskell実装が掲載 typeOf が推論のエントリポイントなのでそこから読むといいです OCaml でも採用されているレベルベースの多相型型推論とは 発展的な話題 多相的型推論アルゴリズムのうち，実用されている高パフォーマンスな手法の解説 第16章 Hindley/Milner型推論 Scala By Example の16章の実装 HMTS自体の解説は0なので，ドメイン知識を得たい場合はほか記事を読んでから実装だけ参照すると良さそう Algorithm W Step By Stepを読んだ \u0026 実装した Algorithm Wの実装 Algorithm W Step By Step自体は このPDF だと思われる 型推論機構の実装 京都大学の講義資料? 単相型推論に始まる型システムの基礎から詳しく説明されている めちゃくちゃ読みました ","date":"2020-12-20","objectID":"/ja/posts/type-system1/:8:0","series":null,"tags":["type-system","rust"],"title":"Rustでつくる単相型システムもどき","uri":"/ja/posts/type-system1/#参考資料"},{"categories":null,"content":"この記事は 言語実装 Advent Calendar 2020 の10日目です． 昨日は @kimiyuki さんの記事でした． 明日は @fukkun さんの記事です． Twitter等で rustcのコードリーディングを助ける為の記事 を書くみたいなこと言っていましたが， ある理由により実現できませんでした． ","date":"2020-12-10","objectID":"/ja/posts/dive-into-combine/:0:0","series":null,"tags":["parser-combinator","code-reading","combine","rust"],"title":"Rust製のパーサコンビネータcombine v4.4.0を覗き見する","uri":"/ja/posts/dive-into-combine/#"},{"categories":null,"content":" 1 Motivationつい先日，このような記事を上げました． nomという比較的よく使われるパーサコンビネータについて解析し， パーサコンビネータとRustに詳しくなろう，みたいな目的の記事です． 思ったより多くの方にご覧頂けたようで，大変嬉しく思っております． この記事の冒頭で言っていた，まさにそれです． combineも同様に理解することで，更にRustに詳しくなろうと考えています． パーサコンビネータについての解説等は特にしないですし， ｢nomと比較してxxな実装なんですね｣という切り口で解説したいと思っているので， 是非nom解説の記事もご覧頂ければと思います． 実際にnom解説を理解したあとこちらの記事に戻ってくると， あまり理解するのに時間はかからないんじゃないかなと思います． nom解説の記事は2,3週間練って作ったものなので出来がいいのもあります 2つのプロジェクトの規模感を把握するために， clocを使って比較してみました． $ cloc nom/src/ 32 text files. 32 unique files. 0 files ignored. github.com/AlDanial/cloc v 1.82 T=0.04 s (853.9 files/s, 581289.6 lines/s) ------------------------------------------------------------------------------- Language files blank comment code ------------------------------------------------------------------------------- Rust 32 1480 7348 12957 ------------------------------------------------------------------------------- SUM: 32 1480 7348 12957 ------------------------------------------------------------------------------- $ cloc combine/src/ 23 text files. 23 unique files. 0 files ignored. github.com/AlDanial/cloc v 1.82 T=0.03 s (778.9 files/s, 551101.4 lines/s) ------------------------------------------------------------------------------- Language files blank comment code ------------------------------------------------------------------------------- Rust 23 1401 3594 11278 ------------------------------------------------------------------------------- SUM: 23 1401 3594 11278 ------------------------------------------------------------------------------- こうしてみると，そこまで大きな差はなさそうですね． 今回は v4.4.0 を対象に読み進めていきます． ","date":"2020-12-10","objectID":"/ja/posts/dive-into-combine/:1:0","series":null,"tags":["parser-combinator","code-reading","combine","rust"],"title":"Rust製のパーサコンビネータcombine v4.4.0を覗き見する","uri":"/ja/posts/dive-into-combine/#motivation"},{"categories":null,"content":" 2 サンプルから読み始めるREADMEに書いてあるサンプルから読み始めて， サンプルで使われている各関数の実装を見てみることにしましょう． 適宜コメントを加えているので，combineを使ったことない人も理解できると思います． nomの記事もそうでしたし今回もそうですが，私はほぼ使ったことないまま記事を書いています use combine::{many1, Parser, sep_by}; use combine::parser::char::{letter, space}; // parser::char::letter はchar型の入力がstd::char::is_alphabeticを満たせばeatして返す // many1 は引数に受け取ったパーサが一回以上適用できればパース成功とする // ここで得られるwordという変数ももちろんパーサである(後述) let word = many1(letter()); // sep_by は2つ目の引数に渡したパーサをseparatorとする構造をパースする // 今回で言えばparser::char::space なので，スペース区切りの文字列をパースすると考えてくれれば良い // 後述するが，各パーサはmap()メソッドを持つ． // sep_by の返り値が Vec\u003cString\u003e であると注釈することで， // rustcは parser.parse() の成果物がStringだとわかる let mut parser = sep_by(word, space()) .map(|mut words: Vec\u003cString\u003e| words.pop()); // parse() メソッドを呼び出すことで，実際にパースを実行する // 今回は入力文字列としてとして \u0026'static strを渡している let result = parser.parse(\"Pick up that word!\"); // パース成功時，(パーサの成果物， eatされた文字列)というタプルが返る // タプルの順番がnomと真逆なので注意 assert_eq!(result, Ok((Some(\"word\".to_string()), \"!\"))); 私の解説では(個人的好みにつき)一切触れませんでしたが， 一般的には “マクロのnomと関数のcombine” みたいな比較をされることが多いです． 但し前回の記事で散々説明したようにnomには関数APIも存在するので， この比較は現在あまり意味がありません． ","date":"2020-12-10","objectID":"/ja/posts/dive-into-combine/:2:0","series":null,"tags":["parser-combinator","code-reading","combine","rust"],"title":"Rust製のパーサコンビネータcombine v4.4.0を覗き見する","uri":"/ja/posts/dive-into-combine/#サンプルから読み始める"},{"categories":null,"content":" 2.1 parser::char::letter非常にシンプルな機能を提供する関数だとわかったので， この関数を見ていくことでcombineの設計を覗き見ることにします． 実際のコードを見てみると， 提供する機能と同じく非常にシンプルな実装になっていることがわかります． /// Parses an alphabet letter according to [`std::char::is_alphabetic`]. /// /// [`std::char::is_alphabetic`]: https://doc.rust-lang.org/std/primitive.char.html#method.is_alphabetic /// /// ``` /// use combine::Parser; /// use combine::parser::char::letter; /// assert_eq!(letter().parse(\"a\"), Ok(('a', \"\"))); /// assert_eq!(letter().parse(\"A\"), Ok(('A', \"\"))); /// assert!(letter().parse(\"9\").is_err()); /// ``` pub fn letter\u003cInput\u003e() -\u003e impl Parser\u003cInput, Output = char, PartialState = ()\u003e where Input: Stream\u003cToken = char\u003e, Input::Error: ParseError\u003cInput::Token, Input::Range, Input::Position\u003e, { satisfy(|ch: char| ch.is_alphabetic()).expected(\"letter\") } where 句で設けられているトレイト境界ですが，nomの時と似たような感じになっていますね． この関数を ボトムアップ 的に理解することにしましょう． ボトムアップ式理解の欠点として， ずっと不安感を覚えながら読みすすめる必要がある というのがあるので， ここで簡潔に letter() の理解の方針を示します． まず letter() の定義で用いられているトレイトや型を簡潔に説明します これによって，letter() が生成するパーサはどんな入力を受け取れて何を返すかがわかります 次に関数内部を理解します 今回で言えば satisfy() が何をするのか，ということです 2.1.1 letter() で使われている各種トレイトや型まず気になるのは返り値の型とされているimpl Objectです． これはParserトレイトを実装する型を返すようです． nomでは Fn(Input) -\u003e IResult\u003cInput, Output, Error\u003e のように Fn トレイトがそのまま使われていましたが， combineでは少し異なる実装がされているようですね． ドキュメントを見てみましょう． いくつかのメソッドがありますが， とりあえずは fn parse(\u0026mut self, input: Input) -\u003e Result\u003c(Self::Output, Input), \u003cInput as StreamOnce\u003e::Error\u003e だけ理解できていれば大丈夫です． そしてこれ，まさしく先程言っていたように， nomにおける Fn(Input) -\u003e IResult\u003cInput, Output, Error\u003e に似たものを感じます． ドキュメントの内容と合わせるとかなり多くの型が出てきたので，整理したいと思います． Parser\u003cInput: Stream\u003e は各関数が返す\"パーサが実装するトレイト\" associated typeとして Output を持ち，これはパーサの成果物の型を指定する 今回でいえば Output = char fn parse() はパース実行のエントリポイント Stream は3つのトレイトの集合的存在 StreamOnce Positioned ResetStream ParseError はそのままパーサエラーの定義 と，ここまで言われても\"ナンノコッチャ\"ってなってると思います， nomと同様，トレイトとジェネリクスを最大限活用してゼロコスト抽象化を実現している点は変わらないようです． 今回も同じく \u0026str に限定して考えます． まずは以下のコードを見てみましょう． impl\u003c'a\u003e StreamOnce for \u0026'a str { type Token = char; type Range = \u0026'a str; type Position = PointerOffset\u003cstr\u003e; type Error = StringStreamError; #[inline] fn uncons(\u0026mut self) -\u003e Result\u003cchar, StreamErrorFor\u003cSelf\u003e\u003e { let mut chars = self.chars(); match chars.next() { Some(c) =\u003e { *self = chars.as_str(); Ok(c) } None =\u003e Err(StringStreamError::Eoi), } } } impl\u003c'a, T\u003e Positioned for \u0026'a [T] where T: Clone + PartialEq, { #[inline] fn position(\u0026self) -\u003e Self::Position { PointerOffset::new(self.as_ptr() as usize) } } impl\u003c'a\u003e Positioned for \u0026'a str { #[inline] fn position(\u0026self) -\u003e Self::Position { PointerOffset::new(self.as_bytes().position().0) } } #[doc(hidden)] #[macro_export] macro_rules! clone_resetable { (( $($params: tt)* ) $ty: ty) =\u003e { impl\u003c$($params)*\u003e ResetStream for $ty where Self: StreamOnce { type Checkpoint = Self; fn checkpoint(\u0026self) -\u003e Self { self.clone() } #[inline] fn reset(\u0026mut self, checkpoint: Self) -\u003e Result\u003c(), Self::Error\u003e { *self = checkpoint; Ok(()) } } } } clone_resetable! {('a) \u0026'a str} やっぱりこうして一つのトレイト実装を見てみると非常にわかりやすいですね． StreamOnce::uncons() を見ると，std::collections::VecDeque::pop_front() のような動作に見えます． Positioned::position() は， impl\u003c'a, T\u003e Positioned for \u0026'a [T] によってスライスに対する定義が行われ， それを流用する形で \u003c\u0026[u8]\u003e::position() を呼び出しています． ResetPosition に関してはマクロによって実装されていますが， impl\u003c'a\u003e ResetStream for \u0026'a str のような展開がされることがわかれば，あとは普通のimplブロックです． 私は普段Rustを書くとき一切マクロを使わないですが， Rustのマクロは比較的分かりやすい上に ドキュメント もあるので， あまり読むのに困ったことはありません． 3つのトレイトの実装について大まかにわかったところで， Stream トレイトも見てみます． とはいっても，3つのトレイトを頑張って理解した私達にとって難しいことは特にありません． pub trait Stream: StreamOnce + ResetStream + Positioned {} impl\u003cInput\u003e Stream for Input where Input: StreamOnce + Positioned + ResetStream, Input::Error: ParseError\u003cInput::Token, Input::Range, Input::Position\u003e, { } \u0026str に限定して考えたとき， Input::Token = char, Input::Range = \u0026str, Input::Position = PointerOffset\u003cstr\u003e であるとわかっています． また先程説明を省略しましたが， Input::Error = StringStreamError であり， impl ParseError for StringStreamError はデフォルトのものが存在します． ここまでの説明を経て，やっと letter() が生成するパーサに \u0026str を渡せることがわかったのです． 2.1.2 letter() 内部ここでもう一度 parser::char::letter() の定義を","date":"2020-12-10","objectID":"/ja/posts/dive-into-combine/:2:1","series":null,"tags":["parser-combinator","code-reading","combine","rust"],"title":"Rust製のパーサコンビネータcombine v4.4.0を覗き見する","uri":"/ja/posts/dive-into-combine/#parsercharletter"},{"categories":null,"content":" 2.1 parser::char::letter非常にシンプルな機能を提供する関数だとわかったので， この関数を見ていくことでcombineの設計を覗き見ることにします． 実際のコードを見てみると， 提供する機能と同じく非常にシンプルな実装になっていることがわかります． /// Parses an alphabet letter according to [`std::char::is_alphabetic`]. /// /// [`std::char::is_alphabetic`]: https://doc.rust-lang.org/std/primitive.char.html#method.is_alphabetic /// /// ``` /// use combine::Parser; /// use combine::parser::char::letter; /// assert_eq!(letter().parse(\"a\"), Ok(('a', \"\"))); /// assert_eq!(letter().parse(\"A\"), Ok(('A', \"\"))); /// assert!(letter().parse(\"9\").is_err()); /// ``` pub fn letter() -\u003e impl Parser where Input: Stream","date":"2020-12-10","objectID":"/ja/posts/dive-into-combine/:2:1","series":null,"tags":["parser-combinator","code-reading","combine","rust"],"title":"Rust製のパーサコンビネータcombine v4.4.0を覗き見する","uri":"/ja/posts/dive-into-combine/#letter-で使われている各種トレイトや型"},{"categories":null,"content":" 2.1 parser::char::letter非常にシンプルな機能を提供する関数だとわかったので， この関数を見ていくことでcombineの設計を覗き見ることにします． 実際のコードを見てみると， 提供する機能と同じく非常にシンプルな実装になっていることがわかります． /// Parses an alphabet letter according to [`std::char::is_alphabetic`]. /// /// [`std::char::is_alphabetic`]: https://doc.rust-lang.org/std/primitive.char.html#method.is_alphabetic /// /// ``` /// use combine::Parser; /// use combine::parser::char::letter; /// assert_eq!(letter().parse(\"a\"), Ok(('a', \"\"))); /// assert_eq!(letter().parse(\"A\"), Ok(('A', \"\"))); /// assert!(letter().parse(\"9\").is_err()); /// ``` pub fn letter() -\u003e impl Parser where Input: Stream","date":"2020-12-10","objectID":"/ja/posts/dive-into-combine/:2:1","series":null,"tags":["parser-combinator","code-reading","combine","rust"],"title":"Rust製のパーサコンビネータcombine v4.4.0を覗き見する","uri":"/ja/posts/dive-into-combine/#letter-内部"},{"categories":null,"content":" 3 まとめ combineの提供する関数は Parser トレイトを実装する構造体を返す 返す構造体はAPI関数によって異なるが， Parser トレイトを実装するという点で共通している Parser トレイトは Stream トレイトを実装する入力を受け付ける 返り値は Result\u003c(Self::Output, Input), \u003cInput as StreamOnce\u003e::Error\u003e である ここまでの道のりで分かる通り， nomと同じく\"汎用性の高いパーサコンビネータライブラリ\" を実現するための設計がされていることがわかります． Rustの型システムを最大限に活用するコードは見ていて楽しいですし，学びもありますね． 急ピッチで作成したので少しごちゃっとした，自分用メモみたいな記事になってしまいました． 本当は Dockerfileの静的解析ツール みたいなやつを作って発表しようかなーなんて構想もありましたが，後の祭り． 来年も絶対に参加するつもりなので，その時はちゃんと既存記事がないかどうかチェックしてから書くことにします． ","date":"2020-12-10","objectID":"/ja/posts/dive-into-combine/:3:0","series":null,"tags":["parser-combinator","code-reading","combine","rust"],"title":"Rust製のパーサコンビネータcombine v4.4.0を覗き見する","uri":"/ja/posts/dive-into-combine/#まとめ"},{"categories":null,"content":" 4 余談: any について最もシンプルなパーサはこれだったんじゃないかと，今になって思い始めました． とにかくeatできればなんでもいい，なパーサを作ってくれます． pub fn any\u003cInput\u003e() -\u003e Any\u003cInput\u003e where Input: Stream, { Any(PhantomData) } #[derive(Copy, Clone)] pub struct Any\u003cInput\u003e(PhantomData\u003cfn(Input) -\u003e Input\u003e); impl\u003cInput\u003e Parser\u003cInput\u003e for Any\u003cInput\u003e where Input: Stream, { type Output = Input::Token; type PartialState = (); #[inline] fn parse_lazy(\u0026mut self, input: \u0026mut Input) -\u003e ParseResult\u003cInput::Token, Input::Error\u003e { uncons(input) } } fn stream::uncons() に渡しているだけなのがわかります． 要は先頭アイテムが切り取れればそれでいい，な実装です． わかりやすい． ","date":"2020-12-10","objectID":"/ja/posts/dive-into-combine/:4:0","series":null,"tags":["parser-combinator","code-reading","combine","rust"],"title":"Rust製のパーサコンビネータcombine v4.4.0を覗き見する","uri":"/ja/posts/dive-into-combine/#余談-anyhttpsdocsrscombine440combinefnanyhtml-について"},{"categories":null,"content":"この記事は IPFactory Advent Calendar 2020 の1日目です． IPFactoryという技術サークルについては 余談 を御覧ください． 普段コンパイラ自作をしていて，LLパーサを手書きすることが多い私ですが， ｢そういえばパーサライブラリ使ったことないな｣と思い，現在 自作コンパイラプロジェクト のパーサを書き換えています． そこで使っているのが， Geal/nom というパーサコンビネータのライブラリです． Rustの強力な型システムの上でジェネリックなパーサ関数を実装しているということで， きっと様々なRustのエッセンスが含まれていると思い，コードリーディングしようと感じました． また，いずれパーサライブラリも自作したいなと思っているので， その前段階として既存実装を調べる必要があると思いました． Rustのパーサライブラリとしてもう一つ有名なものに Marwes/combine がありますが， これも記事上げようかなーなんて考えています． IPFカレンダーにあげようかなー，他メンバーの記事で埋まらなかったら上がるのでお楽しみに． 今回はv6.0.0を対象とします． 投稿前日に見返したら6.0.1がリリースされていたけど気にしない ","date":"2020-12-01","objectID":"/ja/posts/dive-into-nom/:0:0","series":null,"tags":["parser-combinator","code-reading","nom","rust"],"title":"Rust製のパーサコンビネータnom v6.0.0を解剖する","uri":"/ja/posts/dive-into-nom/#"},{"categories":null,"content":" 1 前提: nomとはnom内部の解説を前に，nom自体の解説をしておきます． README.mdを読むと，次のように書いてあります． nomはRustで書かれたパーサコンビネータのライブラリです． nomは速度やメモリ消費を犠牲にすることなく安全なパーサを構築することを支援します． 高速で正確なパーサを実現するために，Rustの強力な型システムやメモリ安全性を活用しています． バグを生みやすい\"パイプライン\"を抽象化するための関数やマクロ，トレイトなども提供しています． 実際の設計は後述しますが， nomの設計はコンパイル時に決まるような “静的ディスパッチ” の集合で実現されています． (実行時に型を解決する\"動的ディスパッチ\"ではなく) パーサコンビネータを使用するときにパーサ同士を連携させる部分のバグが起こりやすい，みたいな話なのでしょうか． 確かに暗黙的で動的な型システムの上で動かそうと思ったらヒヤヒヤしてしまいますね． ","date":"2020-12-01","objectID":"/ja/posts/dive-into-nom/:1:0","series":null,"tags":["parser-combinator","code-reading","nom","rust"],"title":"Rust製のパーサコンビネータnom v6.0.0を解剖する","uri":"/ja/posts/dive-into-nom/#前提-nomとは"},{"categories":null,"content":" 1.1 サンプルREADME.mdに記載のサンプルを見てみます． コメントで簡単に解説を加えているので，参考にしてください． extern crate nom; use nom::{ IResult, bytes::complete::{tag, take_while_m_n}, combinator::map_res, sequence::tuple }; #[derive(Debug,PartialEq)] pub struct Color { pub red: u8, pub green: u8, pub blue: u8, } fn from_hex(input: \u0026str) -\u003e Result\u003cu8, std::num::ParseIntError\u003e { u8::from_str_radix(input, 16) } fn is_hex_digit(c: char) -\u003e bool { c.is_digit(16) } fn hex_primary(input: \u0026str) -\u003e IResult\u003c\u0026str, u8\u003e { // take_while_m_n(min, max, condition) は， // 入力列の各要素に対しconditionを適用し，それがtrueを返した回数が[min..max]に含まれるかどうか判別する． // map_res(parser, f) は，parserの返す値に関数fを適用して返す． // 今回はtake_while_m_nが二桁の16進数(の文字列)を返すはずなので，それに対してfrom_hexを適用し，u8型を返すという感じ． map_res( take_while_m_n(2, 2, is_hex_digit), from_hex )(input) } fn hex_color(input: \u0026str) -\u003e IResult\u003c\u0026str, Color\u003e { // tag() はシンプルに引数のパターンと合致するかチェック． // カラーコードにおいて \"#\" は意味を持たないので，パーサの成果物は捨ててしまう． let (input, _) = tag(\"#\")(input)?; // tupleは関数に渡したタプル内のパーサを順次適用して，その結果をタプルに格納して返す． // カラーコードは先頭から順にRGBとなっているため，その数値をパターン代入で受け取り， // Color構造体を構築して返す，ということをしている． let (input, (red, green, blue)) = tuple((hex_primary, hex_primary, hex_primary))(input)?; Ok((input, Color { red, green, blue })) } fn main() {} #[test] fn parse_color() { // パース関数の返り値チェック // hex_color のシグネチャを見ると分かる通り，返り値はResultになっている． // タプルの第一要素には\"パース成功後のeatされた文字列\"が， // 第二要素には\"パース関数が成功した時に返す成果物\"が格納されている． assert_eq!(hex_color(\"#2F14DF\"), Ok((\"\", Color { red: 47, green: 20, blue: 223, }))); } ","date":"2020-12-01","objectID":"/ja/posts/dive-into-nom/:1:1","series":null,"tags":["parser-combinator","code-reading","nom","rust"],"title":"Rust製のパーサコンビネータnom v6.0.0を解剖する","uri":"/ja/posts/dive-into-nom/#サンプル"},{"categories":null,"content":" 2 nomを解剖する3ステップnomのパーサ定義は次のような手順で読みすすめるとシンプルに理解できます． nomのパーサ関数のほとんどは nom::IResult\u003cI, O, E\u003e を返す パーサ関数が受け取る型はnomが定義するトレイトのいくつかを実装している必要がある nom::bytes や nom::character はこれらトレイト境界を利用して制約付きジェネリクスを実現している 例えば nom::bytes はバイトストリームを扱う関数群が定義するが，引数にはバイトストリームの振る舞いを定義したトレイト境界を設けている パーサ関数の実装は，これらトレイトが持つ関数を組み合わせて実現している ","date":"2020-12-01","objectID":"/ja/posts/dive-into-nom/:2:0","series":null,"tags":["parser-combinator","code-reading","nom","rust"],"title":"Rust製のパーサコンビネータnom v6.0.0を解剖する","uri":"/ja/posts/dive-into-nom/#nomを解剖する3ステップ"},{"categories":null,"content":" 2.1 nom::IResult\u003cI, O, E\u003eまずは基本的な型から見ていきましょう． nomでは殆どすべてのパーサ関数が IResult という型を返すようになっています． type IResult\u003cI, O, E = Error\u003cI\u003e\u003e = Result\u003c(I, O), Err\u003cE\u003e\u003e; I は Input を表し，パーサ関数に入力するパース対象の型を示します． O は Output を表し，パース成功時に返す型を示します． パーサは パース成功後のeatされた文字列 を返すので， Result::Ok には (eatされた文字列, パース成功時に生成される成果物の型) という値の組が入ります． コンパイラのコードでは fn integer_literal(i: \u0026str) -\u003e nom::IResult\u003c\u0026str, ast::Node\u003e のように， パース結果をASTノードにすることが多いです． ここで Error は std::error::Error ではなく nom::error::Error であり， Err は nom::Err です． #[derive(Debug, PartialEq)] pub struct Error\u003cI\u003e { /// position of the error in the input data pub input: I, /// nom error code pub code: ErrorKind, } pub enum ErrorKind { Tag, MapRes, MapOpt, // stripped } pub enum Err\u003cE\u003e { Incomplete(Needed), Error(E), Failure(E), } pub enum Needed { Unknown, Size(NonZeroUsize), } まず，nomのパーサが返す可能性のあるエラーは Incomplete … パースが完了しなかった Error … 回復可能なエラー Failure … 回復不可能なエラー の三種類です． それぞれがどのような場面で使われるのかについては後述します． デフォルトでは nom::IResult\u003cI, O, E = Error\u003cI\u003e\u003e となっているので， IResult\u003c\u0026str, ast::Node\u003e とすると Error.input: \u0026str のようになります． nom::Err は std::error::Error を実装しているので， Result\u003cT, Box\u003cdyn std::error::Error\u003e\u003e のような関数内で ? 演算子を使用することも可能です． ","date":"2020-12-01","objectID":"/ja/posts/dive-into-nom/:2:1","series":null,"tags":["parser-combinator","code-reading","nom","rust"],"title":"Rust製のパーサコンビネータnom v6.0.0を解剖する","uri":"/ja/posts/dive-into-nom/#nomiresulti-o-e"},{"categories":null,"content":" 2.2 パーサ関数とトレイト境界先程 nom::IResult\u003cI, O, E\u003e について紹介しましたが， パーサ関数内での I は，traits.rs に記載されているトレイト群(のいくつか)を実装していることを強制します． nomでは \u0026str や \u0026[u8] など，これらのトレイトが既に実装済みの型を使うこともできますが， ユーザが自身で定義した型にこれらトレイトを実装することで任意の型が扱えるようになっています． 実際に一つパーサ関数を見てみましょう． nom::bytes::complete::is_a という， パターンに適合する先頭部分文字列をeatして返す関数を例にあげます． pub fn is_a\u003cT, Input, Error: ParseError\u003cInput\u003e\u003e( arr: T, ) -\u003e impl Fn(Input) -\u003e IResult\u003cInput, Input, Error\u003e where Input: InputTakeAtPosition, T: FindToken\u003c\u003cInput as InputTakeAtPosition\u003e::Item\u003e, { move |i: Input| { let e: ErrorKind = ErrorKind::IsA; i.split_at_position1_complete(|c| !arr.find_token(c), e) } } まず取り上げるべきなのは， T: FindToken\u003c\u003cInput as InputTakeAtPosition\u003e::Item\u003e という構文です． これは Fully Qualified Syntax というもので， 今回の例で言えば， Input 型が実装する InputTakeAtPosition トレイト その関連型である Item をパラメータとして渡した FindToken トレイト それを実装する 型 T という意味です． Fully Qualified Syntaxについてはこちらのサンプルも合わせてご覧ください． 例えば Input = \u0026str とします． impl\u003c'a\u003e InputTakeAtPosition for \u0026'a str を見ると， type Item = char; と宣言されています． また，impl\u003c'a\u003e FindToken\u003cchar\u003e for \u0026'a str が実装されているので， この関数に \u0026str を渡すことができるとわかります． つまり， \"\u0026str 型のパーサを書くとき，is_a の引数に指定するパターンも \u0026str でかけるよ\" という意味になるのです． 返り値はクロージャですが，クロージャを返す関数では impl trait 型を指定するのが一般的です(Rust by Example を参照)． Rustにおけるクロージャについてはこちらの記事がとてもわかりやすいです． トレイト定義を見るとわかりますが， 第一引数に指定した P: Fn(Self::Item) -\u003e bool を str::find に渡し， Some(i) if i != 0 ならばパース成功とする，というようなイメージです． 追うのが中々大変だと思うので，まずは Input = \u0026str, Item = char に限定した実装を用意しました． 下記サンプルはGitHubにも置いてあります． /// 適当なエラー型 #[derive(Debug)] enum Error { Failed, } /// impl FindToken\u003cchar\u003e for \u0026strの簡易実装 /// 単にnomの実装の中身を持ってきただけ fn find_char\u003c'a\u003e(arr: \u0026'a str, c: char) -\u003e bool { arr.chars().any(|i| i == c) } /// impl InputTokenAtPosition for \u0026str の簡易実装 /// これも実際の中身を取り出した fn split_at_position1_complete\u003c'a, P\u003e(s: \u0026'a str, predicate: P) -\u003e Result\u003c(\u0026'a str, \u0026'a str), Error\u003e where P: Fn(char) -\u003e bool, { match s.find(predicate) { Some(0) =\u003e Err(Error::Failed), Some(i) =\u003e Ok((\u0026s[i..], \u0026s[..i])), None =\u003e { if s.is_empty() { Err(Error::Failed) } else { Ok((\u0026s[s.len()..], \u0026s[..s.len()])) } } } } /// nom::bytes::complete::is_a の実装ほぼそのまま fn specialized_is_a\u003c'a\u003e(arr: \u0026'a str) -\u003e impl Fn(\u0026'a str) -\u003e Result\u003c(\u0026'a str, \u0026'a str), Error\u003e { move |i: \u0026str| split_at_position1_complete(i, |c| !find_char(arr, c)) } fn main() { let digit = specialized_is_a(\"1234567890\"); eprintln!(\"is_a(\\\"1234567890\\\")(\\\"123abc\\\") =\u003e {:?}\", digit(\"123abc\")); //=\u003e is_a(\"1234567890\")(\"123abc\") =\u003e Ok((\"abc\", \"123\")) eprintln!( \"is_a(\\\"1234567890\\\")(\\\"Drumato\\\") =\u003e {:?}\", digit(\"Drumato\") ); //=\u003e is_a(\"1234567890\")(\"Drumato\") =\u003e Err(Failed) eprintln!(\"is_a(\\\"1234567890\\\")(\\\"\\\") =\u003e {:?}\", digit(\"\")); //=\u003e is_a(\"1234567890\")(\"\") =\u003e Err(Failed) } これだけであれば，文字列操作で実現された単なる(関数を返す)関数です． これを Rustの強力な静的ディスパッチ によって汎用的にし, specialized_is_a の引数や，返すクロージャの引数や返り値をジェネリクスによって実現したものがnomというわけです． かなり複雑な型パズルになっているので初見は分かりづらいかもしれませんが， これらは全てコンパイル時に決定し Box\u003cdyn trait\u003e 等の動的ディスパッチによるコストもない，非常にキレイな設計になっていると言えます． ここまでで，nomのパーサを読む前提知識が身につきました． パーサの殆どは IResult\u003cI, O, E = Error\u003cI\u003e\u003e = Result\u003c(I, O), Err\u003cE\u003e\u003e という型を返す パーサ関数はnomが定義するトレイトを用いて制約付きジェネリクスを実現している \u0026[u8] や \u0026str 等の関してはデフォルト実装が存在し，すぐに使い始められる それ以外の型にトレイトを実装することで，任意の型をパーサ関数に入力できる パーサ関数は各トレイトに定義された振る舞いの関数を組み合わせて実現されている 例えば nom::bytes::complete::is_a は FindToken::find_token や InputTakeAtPosition::split_at_position1_complete など ","date":"2020-12-01","objectID":"/ja/posts/dive-into-nom/:2:2","series":null,"tags":["parser-combinator","code-reading","nom","rust"],"title":"Rust製のパーサコンビネータnom v6.0.0を解剖する","uri":"/ja/posts/dive-into-nom/#パーサ関数とトレイト境界"},{"categories":null,"content":" 3 nomのモジュール，パーサ紹介nomは以下のモジュールを公開しています． モジュール名 説明 nom::bits ビットレベルのパーサ nom::bytes バイトストリームのパーサ nom::character char 単位のパーサ nom::combinator パーサコンビネータ群 nom::error nomのエラー管理 nom::multi パーサを引数に受け取って，複数回適用したりみたいなことに使う nom::number 数値関連のパーサ nom::regexp 正規表現を用いたパーサ定義 nom:sequence (a, b, c)など，シーケンス構造のパーサ これらの関数をすべて紹介していたら大変なことになっていますので， いくつか取り上げて実装を見てみる，ということをやってみましょう． ","date":"2020-12-01","objectID":"/ja/posts/dive-into-nom/:3:0","series":null,"tags":["parser-combinator","code-reading","nom","rust"],"title":"Rust製のパーサコンビネータnom v6.0.0を解剖する","uri":"/ja/posts/dive-into-nom/#nomのモジュールパーサ紹介"},{"categories":null,"content":" 3.1 nom::bytes::complete::tag以下のようにして使うことができる，シンプルなパターンマッチングのパーサです． 下記サンプルコードはGitHub にもおいてあります． fn elf_magic_number(i: \u0026[u8]) -\u003e nom::IResult\u003c\u0026[u8], \u0026[u8]\u003e { nom::bytes::complete::tag(\u0026[0x7f, 0x45, 0x4c, 0x46])(i) } fn drumato(i: \u0026str) -\u003e nom::IResult\u003c\u0026str, \u0026str\u003e { nom::bytes::complete::tag(\"drumato\")(i) } #[test] fn drumato_test() { assert_eq!(Ok((\";\", \"drumato\")), drumato(\"drumato;\")); assert!(drumato(\"not_drumato;\").is_err()); } #[test] fn elf_magic_number_test() { assert_eq!( Ok((\u0026[0x00][..], \u0026[0x7f, 0x45, 0x4c, 0x46][..])), elf_magic_number(\u0026[0x7f, 0x45, 0x4c, 0x46, 0x00]) ); assert!(elf_magic_number(\u0026[0x01, 0x02, 0x03, 0x04, 0x05]).is_err()); } ELFバイナリのパーサもバイトストリームパーサを構築することで実装できますし， \u0026str のような型も勿論使うことが出来ます． 今回はELFバイナリのマジックナンバーをパースする関数を作っています． 非常にシンプルな関数なので，その実装も直感的です． 実際の定義 を見てみます． pub fn tag\u003cT, Input, Error: ParseError\u003cInput\u003e\u003e( tag: T, ) -\u003e impl Fn(Input) -\u003e IResult\u003cInput, Input, Error\u003e where Input: InputTake + Compare\u003cT\u003e, T: InputLength + Clone, { move |i: Input| { let tag_len = tag.input_len(); let t = tag.clone(); let res: IResult\u003c_, _, Error\u003e = match i.compare(t) { CompareResult::Ok =\u003e Ok(i.take_split(tag_len)), _ =\u003e { let e: ErrorKind = ErrorKind::Tag; Err(Err::Error(Error::from_error_kind(i, e))) } }; res } } 入力文字列とパターン tag を比較して，成功すればパターンの長さで分割して返す 後述しますが， take_split() はある一点で区切って，“区切りより前の列” をタプルの第一引数にして返すので注意です \"Drumato\".take_split(4) =\u003e (\"ato\", \"Drum\") という感じ これはまさに序盤で説明した (eatされた文字列, パーサ関数の成果物) という構成． 失敗すれば，適切に ErrorKind を設定して返します． Err(Err::Error(Error::from_error_kind(i, e))) と非常に分かりづらくなっています 冗長に書くと Result::Err(nom::Err::Error(nom::error::Error::from_error_kind(i, e))) となる 先程nom::bytes::complete::is_aを見たのと同じように， クロージャを返す関数になっています． 先程詳しくは説明しませんでしたが，引数の型 T はあくまでマッチを判断するパターンのために使われます． パーサに入力される型は Input であり， T と同じではない点に注意してください． ここでは T == Input == \u0026str と限定して， この関数を読むために必要な部分を持ってきました． といっても，メソッド名からある程度予想ができますが． // Tに必要な実装 impl\u003c'a\u003e InputLength for \u0026'a str { #[inline] fn input_len(\u0026self) -\u003e usize { self.len() } } // Inputに必要な実装 impl\u003c'a\u003e InputTake for \u0026'a str { #[inline] fn take(\u0026self, count: usize) -\u003e Self { /* stripped */ } // return byte index #[inline] fn take_split(\u0026self, count: usize) -\u003e (Self, Self) { (\u0026self[count..], \u0026self[..count]) } } // Inputに必要な実装 impl\u003c'a, 'b\u003e Compare\u003c\u0026'b str\u003e for \u0026'a str { #[inline(always)] fn compare(\u0026self, t: \u0026'b str) -\u003e CompareResult { self.as_bytes().compare(t.as_bytes()) } #[inline(always)] fn compare_no_case(\u0026self, t: \u0026'b str) -\u003e CompareResult { /* stripped */ } } これだけ見ると素朴な実装に見えますね． 実際にはこれらがコンパイル時にガチャガチャと組み合わされて実現されます． なんだか魔法のように見えますが，これこそRustの実現する型システムの恩恵，という感じがしますよね． ","date":"2020-12-01","objectID":"/ja/posts/dive-into-nom/:3:1","series":null,"tags":["parser-combinator","code-reading","nom","rust"],"title":"Rust製のパーサコンビネータnom v6.0.0を解剖する","uri":"/ja/posts/dive-into-nom/#nombytescompletetag"},{"categories":null,"content":" 3.2 nom::character::complete::satisfysatisfy に渡した関数 fn(char) -\u003e bool が真を返す場合，その文字をパースして返すという関数です． これ単体で用いることは少なく，以下のように他のパーサ関数と組み合わせて使う事が多いです． use nom::character::complete::satisfy; use nom::combinator::map; use nom::multi::many1; fn bin(i: \u0026str) -\u003e nom::IResult\u003c\u0026str, String\u003e { map( many1(satisfy(|c| c == '0' || c == '1')), |chars: Vec\u003cchar\u003e| chars.into_iter().collect(), )(i) } fn main() {} #[test] fn bin_test() { assert_eq!(Ok((\"\", \"0\".to_string())), bin(\"0\")); assert_eq!(Ok((\"\", \"11010\".to_string())), bin(\"11010\")); assert!(bin(\"\").is_err()); } これも 実際の定義を見てみましょう． pub fn satisfy\u003cF, I, Error: ParseError\u003cI\u003e\u003e(cond: F) -\u003e impl Fn(I) -\u003e IResult\u003cI, char, Error\u003e where I: Slice\u003cRangeFrom\u003cusize\u003e\u003e + InputIter, \u003cI as InputIter\u003e::Item: AsChar, F: Fn(char) -\u003e bool, { move |i: I| match (i).iter_elements().next().map(|t| { let c = t.as_char(); let b = cond(c); (c, b) }) { Some((c, true)) =\u003e Ok((i.slice(c.len()..), c)), _ =\u003e Err(Err::Error(Error::from_error_kind(i, ErrorKind::Satisfy))), } } InputIter::next() が返す，シーケンスの最初の要素に対して cond を適用し， その返り値が真であった場合のみパース成功としています． 非常にシンプルで分かりやすい実装なので特に取り上げることもありませんが， iter_elements() で返ってくるのはイテレータそのものという点に注意です． \u0026str であれば chars() を呼び出しています． ","date":"2020-12-01","objectID":"/ja/posts/dive-into-nom/:3:2","series":null,"tags":["parser-combinator","code-reading","nom","rust"],"title":"Rust製のパーサコンビネータnom v6.0.0を解剖する","uri":"/ja/posts/dive-into-nom/#nomcharactercompletesatisfy"},{"categories":null,"content":" 4 まとめ今回はnomを徹底解剖することで，Rustの強力な言語機能を使ったパーサライブラリの実装方法を学びました． トレイトやジェネリクスの機能等の機能を活用することでRustっぽいキレイなコードを書くことができますが， 意識せず惰性で作っているとごちゃっとしたコードが出来上がってしまいがちです． 今回のコードリーディングはそのような自分のプログラミングに喝を入れるという意味でとても勉強になりました． 今回取り上げた関数以外にも非常に多くの機能が実現されているnomですが， 本記事で身につけた知識を活かせば，実装を読むことはそう難しくないはずです． 使い方のサンプルが紹介されていない関数についても，今回踏んだ流れで読んでいけば使えそうですね． ","date":"2020-12-01","objectID":"/ja/posts/dive-into-nom/:4:0","series":null,"tags":["parser-combinator","code-reading","nom","rust"],"title":"Rust製のパーサコンビネータnom v6.0.0を解剖する","uri":"/ja/posts/dive-into-nom/#まとめ"},{"categories":null,"content":" 5 余談: IPFactoryについてIPFactoryとは，情報科学専門学校という学校に存在する学内サークルです． IPFに在籍するメンバーがそれぞれ自身の専門分野についての勉強やアウトプット活動を行っています． サークル内有志メンバーで CTFを主催 したりもしています． IPFに所属する各メンバーのTwitterやブログ等は こちらのページ を参照してください． ","date":"2020-12-01","objectID":"/ja/posts/dive-into-nom/:5:0","series":null,"tags":["parser-combinator","code-reading","nom","rust"],"title":"Rust製のパーサコンビネータnom v6.0.0を解剖する","uri":"/ja/posts/dive-into-nom/#余談-ipfactoryについて"},{"categories":null,"content":" 6 余談2: nom::branch::alt についてここらへんのツイートが関連しています． [https://twitter.com/drumato/status/1329698474049249280:embed] 先述したように， nom::branch::alt は引数に nom::branch::Alt トレイトを実装することを強制しています． nom::branch::Alt は パーサのリスト に対して実装されていて， リストの要素である各パーサは nom::Parser トレイトが実装されていることになっています． 実際の定義 を見てみると， FnMut(I) -\u003e nom::IResult\u003cI, O, E\u003e トレイトを実装する引数は全て受け取れるようになっていますが， nom::branch::alt にインプット以外の資源を渡したいことがあります． わたしが実際に取り組んでいる 自作コンパイラ では， ASTの実装に typed-arena を使用している為に， アリーナアロケータを使いまわしたい，というモチベーションがあります． Rustの型システムの限界か?(というかそれこそが型システムの恩恵なので当たり前なんですが)と思いとても困っていたのですが， そこで keen さんの webml という実装を見つけました． この実装を見ると，構造体メソッドとして実装された各パーサ関数が FnMut(I) -\u003e nom::IResult\u003cI, O, E\u003e を実現するクロージャを返す ようになっています． クロージャ内部では構造体のメンバやパーサ関数に渡される引数を利用しても上手くキャプチャされ， 返されるクロージャは上記トレイトを返すので nom::branch::alt に渡すことができるという仕組みです． 非常に感動し，すぐに私の実装にも取り入れました． こちら などが参考になるかと． Keenさんの実装にはとても助けられました． ここで改めて紹介しておきたいとおもいます． ","date":"2020-12-01","objectID":"/ja/posts/dive-into-nom/:6:0","series":null,"tags":["parser-combinator","code-reading","nom","rust"],"title":"Rust製のパーサコンビネータnom v6.0.0を解剖する","uri":"/ja/posts/dive-into-nom/#余談2-nombranchalt-について"},{"categories":null,"content":"私は 2020年4月に サイボウズ･ラボユース という人材育成プロジェクトに採択されてから, 複数アーキテクチャを対象にするコンパイラドライバのスクラッチ開発 に挑戦しています. その過程で,Rustで便利に使えるELF周辺のユーティリティがほしいなと思って, このようなライブラリを作り始めました. ELFの読み込みやその他操作が簡単に,Rustのコア機能を使って行えるもので, イメージとしては /usr/include/elf.h のRust実装+αみたいな感じです. このような 純粋なライブラリ を作っていると, やはり ライブラリを使うツール を作る必要が出てきます. 実装しているライブラリのバグや完成度,使いやすさを自分の手で確かめられるからです. そのために作り始めたのが, TUIベースのELF解析ツール , elfpeach です. 既存のTUIのELF解析ツールが見受けられなかった GNU readelfの出力に不満があった(後述) という点から開発を決意しました. 結果として,良いものを作れているのではないかと考えています. 本記事では,elfpeachの解説と今後の展望をベースに, ELF解析ツールについての考えを取り上げたいと思います. ","date":"2020-10-10","objectID":"/ja/posts/tuipeach/:0:0","series":null,"tags":["elf","rust"],"title":"TUI Based ELF Analyzer in Rust","uri":"/ja/posts/tuipeach/#"},{"categories":null,"content":" 1 elfpeachの主な機能ELF解析の標準ツールである GNU readelfで -a オプションを渡した時に表示される情報と, 現在のelfpeachで実装されている機能を比較してみます.. x が実装済み, - が未実装を表します. 機能 readelf elfpeach 64bit ELF x x 32bit ELF x - ( elf-utilities の不備) x86_64以外のアーキテクチャ x - ( elf-utilities の不備) ELFヘッダ x x セクションヘッダテーブル x x sh_link/sh_info のより詳細な情報 - x プログラムヘッダテーブル x x セクションとセグメントのマッピング表示 x - 動的情報 x x d_un のより詳細な情報 - x 再配置テーブル x - セクショングループ x - バージョン x - ヒストグラム x - note x - こうしてみると,まだまだ実装すべき部分が残っていることがわかりますが, 一番のコア機能であるヘッダ/セクション/セグメント/シンボルが解析できるので, ここで一区切りとして,記事にすることにしました. それぞれの概要と,特徴的な機能について触れます. ","date":"2020-10-10","objectID":"/ja/posts/tuipeach/:1:0","series":null,"tags":["elf","rust"],"title":"TUI Based ELF Analyzer in Rust","uri":"/ja/posts/tuipeach/#elfpeachの主な機能"},{"categories":null,"content":" 1.1 ELFヘッダについて実際に解析したスクショを載せます. 殆どの出力はGNU readelfに似ていますが, GNU readelf よりこだわった部分の紹介をします. どのような種類のELFであれ,ELFヘッダは必ず存在します. ELFヘッダから得られる情報はかなり多いですが, 多くの人が見るのは Ehdr.e_entry の情報でしょう. プログラムのエントリポイントとなるアドレスが格納されているフィールドですが, /bin/ls のようにシンボルテーブルがstripされていなければ, Sym.st_value の値から,エントリポイントとなるシンボルが見つかるはずです. そこでelfpeachでは, Ehdr.e_entry の値でシンボルテーブルを探索し, 該当するシンボルがあれば,そのシンボル名を表示するようにしています. (上記例では _start が表示されていますね. ) 一般的に用いられるコンパイラドライバを使っている場合, 大体固定のシンボルが指定されていますし,気にする必要はありません. しかし,組み込みやベアメタル,その他特殊な状況ではこのエントリポイントを気にする必要が出てきます. 実際私はコンパイラドライバを自作しており,スタートアップルーチンも 独自のもの を使用しているので, この機能がELF解析ツールにあると非常に助かります. 自作アセンブラやリンカのデバッグに役立つからですね. また, Ehdr.e_shstrndx というメンバには, セクションヘッダテーブルが用いる セクション名テーブル のインデックスを指定しますが, これも同様にセクション名を表示するようにしました. ほぼすべての場合で .shstrtab が指定されていると思いますが. ","date":"2020-10-10","objectID":"/ja/posts/tuipeach/:1:1","series":null,"tags":["elf","rust"],"title":"TUI Based ELF Analyzer in Rust","uri":"/ja/posts/tuipeach/#elfヘッダについて"},{"categories":null,"content":" 1.2 セクションヘッダテーブルについて これも殆どreadelfに即したものになっています. elfpeachのこだわった点として, readelfよりリッチな情報を提供する というものがあります. セクションヘッダには, sh_link/sh_info というフィールドが存在します. これらは Shdr.sh_type によって異なる意味を持ち, その意味はフィールドの数値だけを見てもすぐにわかりません. しかしreadelfでは単にその値が出力されているだけです. elfpeachでは数値をそのまま出力するのに加えて, sh_type ごとに異なる出力 をするようにしました. 例えば上記画像であれば, Shdr.sh_type == SHT_RELA より, sh_link … 対応するシンボルテーブルのセクションインデックス sh_info … 再配置対象となるセクションのインデックス というような情報が格納されているはずです. これらをより見やすくするための工夫を施しています. 実装をお見せします. pub fn section_information\u003c'a\u003e( elf_file: \u0026'a file::ELF64, sct: \u0026'a section::Section64, ) -\u003e Paragraph\u003c'a\u003e { let sct_info = match sct.header.get_type() { section::Type::Dynamic =\u003e dynamic_info(elf_file, sct), section::Type::Hash | section::Type::SymTabShNdx =\u003e hash_info(elf_file, sct), section::Type::SymTab | section::Type::DynSym =\u003e symtab_info(elf_file, sct), section::Type::Group =\u003e group_info(elf_file, sct), section::Type::Rel | section::Type::Rela =\u003e relocation_info(elf_file, sct), _ =\u003e common_section_info(sct), }; Paragraph::new(sct_info).block(Block::default().borders(Borders::ALL).title(\"Sections\")) } このようにして描画する情報を区別しています. ","date":"2020-10-10","objectID":"/ja/posts/tuipeach/:1:2","series":null,"tags":["elf","rust"],"title":"TUI Based ELF Analyzer in Rust","uri":"/ja/posts/tuipeach/#セクションヘッダテーブルについて"},{"categories":null,"content":" 1.3 プログラムヘッダテーブルについて 現状はGNU readelfと同等の機能しか提供できていませんが, これも INTERP ならファイルパスを表示する等の機能を作成してもいいかもしれません. ","date":"2020-10-10","objectID":"/ja/posts/tuipeach/:1:3","series":null,"tags":["elf","rust"],"title":"TUI Based ELF Analyzer in Rust","uri":"/ja/posts/tuipeach/#プログラムヘッダテーブルについて"},{"categories":null,"content":" 1.4 シンボルテーブルについて 基本情報の出力は問題なく実装できています. お気づきの通り 重大な欠陥 が存在するのですが, それは 今後の展望 でお話したいと思います. ","date":"2020-10-10","objectID":"/ja/posts/tuipeach/:1:4","series":null,"tags":["elf","rust"],"title":"TUI Based ELF Analyzer in Rust","uri":"/ja/posts/tuipeach/#シンボルテーブルについて"},{"categories":null,"content":" 1.5 動的情報についてこれも基本的には同様ですが, Dyn.d_un の情報をreadelfよりリッチに提供しています. 非常に長いので,詳細はこちらをご覧ください. ","date":"2020-10-10","objectID":"/ja/posts/tuipeach/:1:5","series":null,"tags":["elf","rust"],"title":"TUI Based ELF Analyzer in Rust","uri":"/ja/posts/tuipeach/#動的情報について"},{"categories":null,"content":" 2 今後実装しようと思っている機能","date":"2020-10-10","objectID":"/ja/posts/tuipeach/:2:0","series":null,"tags":["elf","rust"],"title":"TUI Based ELF Analyzer in Rust","uri":"/ja/posts/tuipeach/#今後実装しようと思っている機能"},{"categories":null,"content":" 2.1 テーブル構造のフィルタリング機能ELFには多くの テーブル形式 が存在します. ヘッダテーブルやシンボルテーブル,再配置情報に動的情報などがその例です. これらは大きなバイナリであれば非常に多くのエントリ数を持つでしょう. readelfだと grep \u003caddress\u003e とかで見たいシンボルのみ見れるというのに対して, 現状 ひたすら ↓ を入力するしかない という残念な仕様になっています. よって,このままでは使い物になりません. これをTUIツールっぽく解決するために, フィルタリング機能 を導入しようと考えています. イメージとしては次のような感じ. あるキーの入力でウィンドウがポップアップ(modal windowというんだったかな) 構造体のメンバと検索したい値を指定する 合致するエントリのみ表示される これにより多くの問題は解決すると思います. ","date":"2020-10-10","objectID":"/ja/posts/tuipeach/:2:1","series":null,"tags":["elf","rust"],"title":"TUI Based ELF Analyzer in Rust","uri":"/ja/posts/tuipeach/#テーブル構造のフィルタリング機能"},{"categories":null,"content":" 2.2 32bit ELFの読み込みこれはelf-utilitiesの問題になりますが, 現状32bit版のELFに関わるAPIが全く存在しない,という欠点が存在します. readelfでは問題なく解析可能なので,これは早急に取り組む必要がありそうです. elf-utilitiesに沢山のトレイトを実装すれば,キレイな感じで実装できそうな感じはします. のでやるだけなんですが,やっていないので,やらなければいけませんね. ","date":"2020-10-10","objectID":"/ja/posts/tuipeach/:2:2","series":null,"tags":["elf","rust"],"title":"TUI Based ELF Analyzer in Rust","uri":"/ja/posts/tuipeach/#32bit-elfの読み込み"},{"categories":null,"content":" 2.3 hexdumpセクションの部 を見て頂ければわかりますが, 右下に余らせているスペースがありますよね. このスペースを使用して,セクション内部のhexdumpを表示したら便利なんじゃないかと思っています. 例えば .interp など, sh_type は SHT_PROGBITS であるが, 内容はファイルパスのascii文字列であり,hexdumpによって簡単に解析可能である場合, それが表示されていると結構便利です. あとは .comment とかもそうですね. なんかRustでいい感じのツールあるんでしょうか. 教えて頂ければ助かります. ","date":"2020-10-10","objectID":"/ja/posts/tuipeach/:2:3","series":null,"tags":["elf","rust"],"title":"TUI Based ELF Analyzer in Rust","uri":"/ja/posts/tuipeach/#hexdump"},{"categories":null,"content":" 3 まとめまだまだ発展途上のelfpeachですが, ELF解析ツールの選択肢の一つに入れるようなものを目指して,これからもほそぼそと続けていきます. 一応サイボウズ･ラボユースの成果物に含まれているので基本的には一人で開発していきますが, OSS活動の範囲内であれば支援していただけると大変助かります. 是非使って,あら探しして,Issueに上げてください. ","date":"2020-10-10","objectID":"/ja/posts/tuipeach/:3:0","series":null,"tags":["elf","rust"],"title":"TUI Based ELF Analyzer in Rust","uri":"/ja/posts/tuipeach/#まとめ"},{"categories":null,"content":"お久しぶりです． 2020/8/31 ~ 9/4の5日間に渡って開催された株式会社メルカリ様のインターンに参加しました． 詳細はこちらののページを御覧ください． また，本インターンで使用されていた講義資料については， 既に 完全公開 されています(ありがとうございます)． 今回は本インターンに参加して勉強になったことや所感， 講義資料に記載の課題や，自身の活動をまとめたいと思います． ","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:0:0","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#"},{"categories":null,"content":" 1 インターン概要Golangで学ぶ静的解析 を主なテーマに， 前半2日間 … Golangで静的解析を行う上で必要な知識のインプットやコード課題 何故 静的解析ツールを開発するのか 実用的な静的解析ツールの例 Golangによる静的解析で頻繁に用いられる標準(準標準)パッケージの解説 字句解析や構文解析などによって得られるデータ構造等，静的解析に必要な前提知識の解説 Golangに精通しているメンターさんだからこその言語仕様に関連した話題の紹介 後半3日間 … 学んだことを生かして，実際に静的解析ツールを開発する 自身でテーマを設定して取り組む コーディング等で困ったことがあればメンターさんからサポートしていただける という内容の予定でした． 実際にはメンターさんの都合もあって開発の日にちが変更になりましたので， インプットとアウトプットの2つに分けて振り返りたいと思います． ","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:0","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#インターン概要"},{"categories":null,"content":" 1.1 座学記事冒頭に記載した資料をもとに，静的解析関連の知識を勉強しました． 私は普段コンパイラ理論の勉強をしているので字句解析や構文解析，型検査やSSAについては多少の前提知識がありましたが， 静的解析という目線ではただ｢データ構造を構築すること｣だけでなく， ツールの目的 に応じてデータ構造を柔軟に利用することが求められることがわかり，勉強になりました． 講義の具体的な内容についてはここでは触れません(講義資料が本当に素晴らしいので，そちらを参照ください)が， いくつか個人的に感じたことについて触れておきます． 見たくない人は 開発セクション まで読み飛ばしていただければ． 1.1.1 Golangの良さ私は最近，基本的にRust以外のプログラミング言語を触っていません． Atcoder等の解答にPythonを用いたり，ちょっとしたコード例としてHaskellを使うみたいなことはありますが， Golangをガッツリ触ったのは久しぶりなので，新鮮で楽しかったです． Golangの良さは既に語り尽くされていて，多くの人がブログ等で言及しているので， ここでは｢静的解析ツールを作る上で｣の利点について述べようと思っています． 1.1.1.1 標準パッケージの豊富さ･扱いやすさそんなことはもう知ってるよ! と思うかもしれませんが，改めて考えてみても優秀です． 講義資料にも書いてありますが，静的解析ツールには フラットな “ソースコード” というテキストから 階層/親子構造 を抽出したり， 識別子が参照する･される等の 依存関係 も解析する必要があります． そのために字句解析，構文解析，パッケージ間の依存グラフの構築等も必要になります． 言語処理系をスクラッチで書いたことがある人はイメージしやすいと思いますが， これらアルゴリズムの実装には 多くのコスト がかかり， かなりの労力を要します． しかし，GolangにはトークンやAST,構文解析や型関連の標準パッケージが存在します． また， x/tools/go 以下には静的解析系ツールやコールグラフ，CFGやSSA等の更に便利な機能が存在します． 例えば，Golangのソースコードに対し構文解析を行い，インポートしているパッケージを表示するプログラムを見てみましょう． 引用: Example - ParseFile package main import ( \"fmt\" \"go/parser\" \"go/token\" ) func main() { fset := token.NewFileSet() // positions are relative to fset src := `package foo import ( \"fmt\" \"time\" ) func bar() { fmt.Println(time.Now()) }` // Parse src but stop after processing the imports. f, err := parser.ParseFile(fset, \"\", src, parser.ImportsOnly) if err != nil { fmt.Println(err) return } // Print the imports from the file's AST. for _, s := range f.Imports { fmt.Println(s.Path.Value) } } このように非常にシンプルに解析資源を得られるので， 静的解析ツールの書きやすさはかなり高いと思います． x/tools/go/analysis がその最たる例で， 静的解析ツールを開発するためのパッケージ が用意されています． これについては上記講義資料に加えて，こちらの記事も参考になると思います． このように，標準･準標準パッケージが豊富であり扱いやすいところからも， Golangで静的解析ツールが書きやすい と言えそうです． 1.1.1.2 静的解析ツールの開発しやすさ実際に作り始めるにはいくつかのお作法が存在する静的解析ツールですが， 本インターンのメンターである @tenntennさん も在籍する Org の GoStaticAnalysis が 雛形ジェネレータ を公開しています． 使い方はREADMEや講義資料を参照してください． skeleton を利用すればすぐに静的解析ツールの開発を始める事ができます，非常に便利です． このあと紹介する課題を解く際に使ってみます． 1.1.2 ｢ルールを作る場合はツールも作る｣本インターンで一番共感した講義内容の一つです． 講義資料の23ページに書かれています． コーディングルールやベストプラクティス，アンチパターンなど， プログラミング中に意識し，開発チーム内で注意しあわなければいけないことはいくつか存在します． (これは個人開発でも同様に重要だと考えています) これらを意識し続けることは簡単ではありません． コンパイルエラーが発生するのであれば特に大きな問題にはなりませんが， そういう訳ではないので｢無意識のうちに良くない書き方をする｣ということが考えられます． このようなミスを エンジニアリングによって解決する というのは非常にスマートで美しい考え方だと思います． GitのコミットやPRごとにCIが回るようにして，ルールに即した静的解析ツールが動く． そして自動的にアンチパターンを防げるようになれば，生産性は大幅に向上するはずです． 学生で，かつ(長期インターン等を除き)個人の学習の範囲でしか開発していないような私からすると， このような 実際に業務で開発している人 の考え方に触れることができて，凄い勉強になりました． 1.1.3 課題まとめここからは，講義資料に記載されている課題について考え，実装してみたいと思います． 本インターンの活動中には時間が足りず解答しきれなかったので，夜の時間等にちまちま解き進めていました． 1.1.3.1 構文解析でわかること，わからないこと パッケージ変数の数 ファイル中に定義されている構造体のフィールド名 インポートしているファイルのパス は構文解析で検出することができます． go/ast のAPIでは(おそらく)以下のように対応しています． パッケージ変数の数 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.VAR を数える ファイル中に定義されている構造体のフィールド名 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.TYPE を検査する ast.GenDecl.Specs のエントリを *ast.TypeSpec にキャストし， .Type を *ast.StructType にキャストする go/types から Info.Defs のマップを探索し，types.Object.Type() を *types.Struct にキャストして NumFields() 分ループ インポートしているファイルのパス ast.File.Imports のエントリのそれぞれが， $GOPATH/src もしくは $GOROOT/src に存在するかどうかを調べれば良い $GOPATH/src にはサードパーティ製パッケージが， $GOROOT/src には標準パッケージが存在 逆に，context.Context 型のフィールドを持つ構造体の検出は構文解析だけではできません． 型チェックの機構やパッケージ解析を用いて， std/context.Context であるかどうか判断しないといけません． (別パッケージの Context 構造体をフィールドに持っている場合も，構文上は全く同じであり，ASTを探索するだけで検出できない) 1.1.3.2 型チェックでわかること，わからないことまず ｢context.Context 型のフィールドを持つ構造体の検出｣は型チェックで検出することができます． これについてはこちらのリポジトリにそのまま同じ用途の静的解析ツールが存在するので，参考にすると良いでしょう． 次に ｢使われていない非公開なパッケージ関数｣ ですが，これも構文解析や型チェックの情報で検出可能です． ASTを探索し， 関数呼び出し 関数ポインタ代入 返り値に指定されているか のいずれも行われていない場合を検出すれば良さそうです． 逆に， ｢100 + 200 などの定数式の評価｣ は型チェックだけでは不可能です． go/constants のように定数評価器のパッケージを利用する等が必要です． 講義資料には go/types や go/constant によって定数式の評価が可能と書いてあります． Golangのパッケージはとても優秀ですね． また， ｢インタフェース型のメソッドを呼び出した場合のレシーバの実際の型｣ も型チェックだけでは難しいと思います． インタフェース型メソッドの呼び出しに渡される引数から ポインタ解析( golang.org/x/tools/go/pointer ) を行う必要がありそうです． 1.1.3.3 不要なパッケージ関数の判別パッケージ外部に公開されておらず，またどの関数からも呼び出されていないような関数の検出です． こちらの課題で解き方を簡単に考察してみたので，これの通り実装","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:1","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#座学"},{"categories":null,"content":" 1.1 座学記事冒頭に記載した資料をもとに，静的解析関連の知識を勉強しました． 私は普段コンパイラ理論の勉強をしているので字句解析や構文解析，型検査やSSAについては多少の前提知識がありましたが， 静的解析という目線ではただ｢データ構造を構築すること｣だけでなく， ツールの目的 に応じてデータ構造を柔軟に利用することが求められることがわかり，勉強になりました． 講義の具体的な内容についてはここでは触れません(講義資料が本当に素晴らしいので，そちらを参照ください)が， いくつか個人的に感じたことについて触れておきます． 見たくない人は 開発セクション まで読み飛ばしていただければ． 1.1.1 Golangの良さ私は最近，基本的にRust以外のプログラミング言語を触っていません． Atcoder等の解答にPythonを用いたり，ちょっとしたコード例としてHaskellを使うみたいなことはありますが， Golangをガッツリ触ったのは久しぶりなので，新鮮で楽しかったです． Golangの良さは既に語り尽くされていて，多くの人がブログ等で言及しているので， ここでは｢静的解析ツールを作る上で｣の利点について述べようと思っています． 1.1.1.1 標準パッケージの豊富さ･扱いやすさそんなことはもう知ってるよ! と思うかもしれませんが，改めて考えてみても優秀です． 講義資料にも書いてありますが，静的解析ツールには フラットな “ソースコード” というテキストから 階層/親子構造 を抽出したり， 識別子が参照する･される等の 依存関係 も解析する必要があります． そのために字句解析，構文解析，パッケージ間の依存グラフの構築等も必要になります． 言語処理系をスクラッチで書いたことがある人はイメージしやすいと思いますが， これらアルゴリズムの実装には 多くのコスト がかかり， かなりの労力を要します． しかし，GolangにはトークンやAST,構文解析や型関連の標準パッケージが存在します． また， x/tools/go 以下には静的解析系ツールやコールグラフ，CFGやSSA等の更に便利な機能が存在します． 例えば，Golangのソースコードに対し構文解析を行い，インポートしているパッケージを表示するプログラムを見てみましょう． 引用: Example - ParseFile package main import ( \"fmt\" \"go/parser\" \"go/token\" ) func main() { fset := token.NewFileSet() // positions are relative to fset src := `package foo import ( \"fmt\" \"time\" ) func bar() { fmt.Println(time.Now()) }` // Parse src but stop after processing the imports. f, err := parser.ParseFile(fset, \"\", src, parser.ImportsOnly) if err != nil { fmt.Println(err) return } // Print the imports from the file's AST. for _, s := range f.Imports { fmt.Println(s.Path.Value) } } このように非常にシンプルに解析資源を得られるので， 静的解析ツールの書きやすさはかなり高いと思います． x/tools/go/analysis がその最たる例で， 静的解析ツールを開発するためのパッケージ が用意されています． これについては上記講義資料に加えて，こちらの記事も参考になると思います． このように，標準･準標準パッケージが豊富であり扱いやすいところからも， Golangで静的解析ツールが書きやすい と言えそうです． 1.1.1.2 静的解析ツールの開発しやすさ実際に作り始めるにはいくつかのお作法が存在する静的解析ツールですが， 本インターンのメンターである @tenntennさん も在籍する Org の GoStaticAnalysis が 雛形ジェネレータ を公開しています． 使い方はREADMEや講義資料を参照してください． skeleton を利用すればすぐに静的解析ツールの開発を始める事ができます，非常に便利です． このあと紹介する課題を解く際に使ってみます． 1.1.2 ｢ルールを作る場合はツールも作る｣本インターンで一番共感した講義内容の一つです． 講義資料の23ページに書かれています． コーディングルールやベストプラクティス，アンチパターンなど， プログラミング中に意識し，開発チーム内で注意しあわなければいけないことはいくつか存在します． (これは個人開発でも同様に重要だと考えています) これらを意識し続けることは簡単ではありません． コンパイルエラーが発生するのであれば特に大きな問題にはなりませんが， そういう訳ではないので｢無意識のうちに良くない書き方をする｣ということが考えられます． このようなミスを エンジニアリングによって解決する というのは非常にスマートで美しい考え方だと思います． GitのコミットやPRごとにCIが回るようにして，ルールに即した静的解析ツールが動く． そして自動的にアンチパターンを防げるようになれば，生産性は大幅に向上するはずです． 学生で，かつ(長期インターン等を除き)個人の学習の範囲でしか開発していないような私からすると， このような 実際に業務で開発している人 の考え方に触れることができて，凄い勉強になりました． 1.1.3 課題まとめここからは，講義資料に記載されている課題について考え，実装してみたいと思います． 本インターンの活動中には時間が足りず解答しきれなかったので，夜の時間等にちまちま解き進めていました． 1.1.3.1 構文解析でわかること，わからないこと パッケージ変数の数 ファイル中に定義されている構造体のフィールド名 インポートしているファイルのパス は構文解析で検出することができます． go/ast のAPIでは(おそらく)以下のように対応しています． パッケージ変数の数 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.VAR を数える ファイル中に定義されている構造体のフィールド名 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.TYPE を検査する ast.GenDecl.Specs のエントリを *ast.TypeSpec にキャストし， .Type を *ast.StructType にキャストする go/types から Info.Defs のマップを探索し，types.Object.Type() を *types.Struct にキャストして NumFields() 分ループ インポートしているファイルのパス ast.File.Imports のエントリのそれぞれが， $GOPATH/src もしくは $GOROOT/src に存在するかどうかを調べれば良い $GOPATH/src にはサードパーティ製パッケージが， $GOROOT/src には標準パッケージが存在 逆に，context.Context 型のフィールドを持つ構造体の検出は構文解析だけではできません． 型チェックの機構やパッケージ解析を用いて， std/context.Context であるかどうか判断しないといけません． (別パッケージの Context 構造体をフィールドに持っている場合も，構文上は全く同じであり，ASTを探索するだけで検出できない) 1.1.3.2 型チェックでわかること，わからないことまず ｢context.Context 型のフィールドを持つ構造体の検出｣は型チェックで検出することができます． これについてはこちらのリポジトリにそのまま同じ用途の静的解析ツールが存在するので，参考にすると良いでしょう． 次に ｢使われていない非公開なパッケージ関数｣ ですが，これも構文解析や型チェックの情報で検出可能です． ASTを探索し， 関数呼び出し 関数ポインタ代入 返り値に指定されているか のいずれも行われていない場合を検出すれば良さそうです． 逆に， ｢100 + 200 などの定数式の評価｣ は型チェックだけでは不可能です． go/constants のように定数評価器のパッケージを利用する等が必要です． 講義資料には go/types や go/constant によって定数式の評価が可能と書いてあります． Golangのパッケージはとても優秀ですね． また， ｢インタフェース型のメソッドを呼び出した場合のレシーバの実際の型｣ も型チェックだけでは難しいと思います． インタフェース型メソッドの呼び出しに渡される引数から ポインタ解析( golang.org/x/tools/go/pointer ) を行う必要がありそうです． 1.1.3.3 不要なパッケージ関数の判別パッケージ外部に公開されておらず，またどの関数からも呼び出されていないような関数の検出です． こちらの課題で解き方を簡単に考察してみたので，これの通り実装","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:1","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#golangの良さ"},{"categories":null,"content":" 1.1 座学記事冒頭に記載した資料をもとに，静的解析関連の知識を勉強しました． 私は普段コンパイラ理論の勉強をしているので字句解析や構文解析，型検査やSSAについては多少の前提知識がありましたが， 静的解析という目線ではただ｢データ構造を構築すること｣だけでなく， ツールの目的 に応じてデータ構造を柔軟に利用することが求められることがわかり，勉強になりました． 講義の具体的な内容についてはここでは触れません(講義資料が本当に素晴らしいので，そちらを参照ください)が， いくつか個人的に感じたことについて触れておきます． 見たくない人は 開発セクション まで読み飛ばしていただければ． 1.1.1 Golangの良さ私は最近，基本的にRust以外のプログラミング言語を触っていません． Atcoder等の解答にPythonを用いたり，ちょっとしたコード例としてHaskellを使うみたいなことはありますが， Golangをガッツリ触ったのは久しぶりなので，新鮮で楽しかったです． Golangの良さは既に語り尽くされていて，多くの人がブログ等で言及しているので， ここでは｢静的解析ツールを作る上で｣の利点について述べようと思っています． 1.1.1.1 標準パッケージの豊富さ･扱いやすさそんなことはもう知ってるよ! と思うかもしれませんが，改めて考えてみても優秀です． 講義資料にも書いてありますが，静的解析ツールには フラットな “ソースコード” というテキストから 階層/親子構造 を抽出したり， 識別子が参照する･される等の 依存関係 も解析する必要があります． そのために字句解析，構文解析，パッケージ間の依存グラフの構築等も必要になります． 言語処理系をスクラッチで書いたことがある人はイメージしやすいと思いますが， これらアルゴリズムの実装には 多くのコスト がかかり， かなりの労力を要します． しかし，GolangにはトークンやAST,構文解析や型関連の標準パッケージが存在します． また， x/tools/go 以下には静的解析系ツールやコールグラフ，CFGやSSA等の更に便利な機能が存在します． 例えば，Golangのソースコードに対し構文解析を行い，インポートしているパッケージを表示するプログラムを見てみましょう． 引用: Example - ParseFile package main import ( \"fmt\" \"go/parser\" \"go/token\" ) func main() { fset := token.NewFileSet() // positions are relative to fset src := `package foo import ( \"fmt\" \"time\" ) func bar() { fmt.Println(time.Now()) }` // Parse src but stop after processing the imports. f, err := parser.ParseFile(fset, \"\", src, parser.ImportsOnly) if err != nil { fmt.Println(err) return } // Print the imports from the file's AST. for _, s := range f.Imports { fmt.Println(s.Path.Value) } } このように非常にシンプルに解析資源を得られるので， 静的解析ツールの書きやすさはかなり高いと思います． x/tools/go/analysis がその最たる例で， 静的解析ツールを開発するためのパッケージ が用意されています． これについては上記講義資料に加えて，こちらの記事も参考になると思います． このように，標準･準標準パッケージが豊富であり扱いやすいところからも， Golangで静的解析ツールが書きやすい と言えそうです． 1.1.1.2 静的解析ツールの開発しやすさ実際に作り始めるにはいくつかのお作法が存在する静的解析ツールですが， 本インターンのメンターである @tenntennさん も在籍する Org の GoStaticAnalysis が 雛形ジェネレータ を公開しています． 使い方はREADMEや講義資料を参照してください． skeleton を利用すればすぐに静的解析ツールの開発を始める事ができます，非常に便利です． このあと紹介する課題を解く際に使ってみます． 1.1.2 ｢ルールを作る場合はツールも作る｣本インターンで一番共感した講義内容の一つです． 講義資料の23ページに書かれています． コーディングルールやベストプラクティス，アンチパターンなど， プログラミング中に意識し，開発チーム内で注意しあわなければいけないことはいくつか存在します． (これは個人開発でも同様に重要だと考えています) これらを意識し続けることは簡単ではありません． コンパイルエラーが発生するのであれば特に大きな問題にはなりませんが， そういう訳ではないので｢無意識のうちに良くない書き方をする｣ということが考えられます． このようなミスを エンジニアリングによって解決する というのは非常にスマートで美しい考え方だと思います． GitのコミットやPRごとにCIが回るようにして，ルールに即した静的解析ツールが動く． そして自動的にアンチパターンを防げるようになれば，生産性は大幅に向上するはずです． 学生で，かつ(長期インターン等を除き)個人の学習の範囲でしか開発していないような私からすると， このような 実際に業務で開発している人 の考え方に触れることができて，凄い勉強になりました． 1.1.3 課題まとめここからは，講義資料に記載されている課題について考え，実装してみたいと思います． 本インターンの活動中には時間が足りず解答しきれなかったので，夜の時間等にちまちま解き進めていました． 1.1.3.1 構文解析でわかること，わからないこと パッケージ変数の数 ファイル中に定義されている構造体のフィールド名 インポートしているファイルのパス は構文解析で検出することができます． go/ast のAPIでは(おそらく)以下のように対応しています． パッケージ変数の数 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.VAR を数える ファイル中に定義されている構造体のフィールド名 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.TYPE を検査する ast.GenDecl.Specs のエントリを *ast.TypeSpec にキャストし， .Type を *ast.StructType にキャストする go/types から Info.Defs のマップを探索し，types.Object.Type() を *types.Struct にキャストして NumFields() 分ループ インポートしているファイルのパス ast.File.Imports のエントリのそれぞれが， $GOPATH/src もしくは $GOROOT/src に存在するかどうかを調べれば良い $GOPATH/src にはサードパーティ製パッケージが， $GOROOT/src には標準パッケージが存在 逆に，context.Context 型のフィールドを持つ構造体の検出は構文解析だけではできません． 型チェックの機構やパッケージ解析を用いて， std/context.Context であるかどうか判断しないといけません． (別パッケージの Context 構造体をフィールドに持っている場合も，構文上は全く同じであり，ASTを探索するだけで検出できない) 1.1.3.2 型チェックでわかること，わからないことまず ｢context.Context 型のフィールドを持つ構造体の検出｣は型チェックで検出することができます． これについてはこちらのリポジトリにそのまま同じ用途の静的解析ツールが存在するので，参考にすると良いでしょう． 次に ｢使われていない非公開なパッケージ関数｣ ですが，これも構文解析や型チェックの情報で検出可能です． ASTを探索し， 関数呼び出し 関数ポインタ代入 返り値に指定されているか のいずれも行われていない場合を検出すれば良さそうです． 逆に， ｢100 + 200 などの定数式の評価｣ は型チェックだけでは不可能です． go/constants のように定数評価器のパッケージを利用する等が必要です． 講義資料には go/types や go/constant によって定数式の評価が可能と書いてあります． Golangのパッケージはとても優秀ですね． また， ｢インタフェース型のメソッドを呼び出した場合のレシーバの実際の型｣ も型チェックだけでは難しいと思います． インタフェース型メソッドの呼び出しに渡される引数から ポインタ解析( golang.org/x/tools/go/pointer ) を行う必要がありそうです． 1.1.3.3 不要なパッケージ関数の判別パッケージ外部に公開されておらず，またどの関数からも呼び出されていないような関数の検出です． こちらの課題で解き方を簡単に考察してみたので，これの通り実装","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:1","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#標準パッケージの豊富さ扱いやすさ"},{"categories":null,"content":" 1.1 座学記事冒頭に記載した資料をもとに，静的解析関連の知識を勉強しました． 私は普段コンパイラ理論の勉強をしているので字句解析や構文解析，型検査やSSAについては多少の前提知識がありましたが， 静的解析という目線ではただ｢データ構造を構築すること｣だけでなく， ツールの目的 に応じてデータ構造を柔軟に利用することが求められることがわかり，勉強になりました． 講義の具体的な内容についてはここでは触れません(講義資料が本当に素晴らしいので，そちらを参照ください)が， いくつか個人的に感じたことについて触れておきます． 見たくない人は 開発セクション まで読み飛ばしていただければ． 1.1.1 Golangの良さ私は最近，基本的にRust以外のプログラミング言語を触っていません． Atcoder等の解答にPythonを用いたり，ちょっとしたコード例としてHaskellを使うみたいなことはありますが， Golangをガッツリ触ったのは久しぶりなので，新鮮で楽しかったです． Golangの良さは既に語り尽くされていて，多くの人がブログ等で言及しているので， ここでは｢静的解析ツールを作る上で｣の利点について述べようと思っています． 1.1.1.1 標準パッケージの豊富さ･扱いやすさそんなことはもう知ってるよ! と思うかもしれませんが，改めて考えてみても優秀です． 講義資料にも書いてありますが，静的解析ツールには フラットな “ソースコード” というテキストから 階層/親子構造 を抽出したり， 識別子が参照する･される等の 依存関係 も解析する必要があります． そのために字句解析，構文解析，パッケージ間の依存グラフの構築等も必要になります． 言語処理系をスクラッチで書いたことがある人はイメージしやすいと思いますが， これらアルゴリズムの実装には 多くのコスト がかかり， かなりの労力を要します． しかし，GolangにはトークンやAST,構文解析や型関連の標準パッケージが存在します． また， x/tools/go 以下には静的解析系ツールやコールグラフ，CFGやSSA等の更に便利な機能が存在します． 例えば，Golangのソースコードに対し構文解析を行い，インポートしているパッケージを表示するプログラムを見てみましょう． 引用: Example - ParseFile package main import ( \"fmt\" \"go/parser\" \"go/token\" ) func main() { fset := token.NewFileSet() // positions are relative to fset src := `package foo import ( \"fmt\" \"time\" ) func bar() { fmt.Println(time.Now()) }` // Parse src but stop after processing the imports. f, err := parser.ParseFile(fset, \"\", src, parser.ImportsOnly) if err != nil { fmt.Println(err) return } // Print the imports from the file's AST. for _, s := range f.Imports { fmt.Println(s.Path.Value) } } このように非常にシンプルに解析資源を得られるので， 静的解析ツールの書きやすさはかなり高いと思います． x/tools/go/analysis がその最たる例で， 静的解析ツールを開発するためのパッケージ が用意されています． これについては上記講義資料に加えて，こちらの記事も参考になると思います． このように，標準･準標準パッケージが豊富であり扱いやすいところからも， Golangで静的解析ツールが書きやすい と言えそうです． 1.1.1.2 静的解析ツールの開発しやすさ実際に作り始めるにはいくつかのお作法が存在する静的解析ツールですが， 本インターンのメンターである @tenntennさん も在籍する Org の GoStaticAnalysis が 雛形ジェネレータ を公開しています． 使い方はREADMEや講義資料を参照してください． skeleton を利用すればすぐに静的解析ツールの開発を始める事ができます，非常に便利です． このあと紹介する課題を解く際に使ってみます． 1.1.2 ｢ルールを作る場合はツールも作る｣本インターンで一番共感した講義内容の一つです． 講義資料の23ページに書かれています． コーディングルールやベストプラクティス，アンチパターンなど， プログラミング中に意識し，開発チーム内で注意しあわなければいけないことはいくつか存在します． (これは個人開発でも同様に重要だと考えています) これらを意識し続けることは簡単ではありません． コンパイルエラーが発生するのであれば特に大きな問題にはなりませんが， そういう訳ではないので｢無意識のうちに良くない書き方をする｣ということが考えられます． このようなミスを エンジニアリングによって解決する というのは非常にスマートで美しい考え方だと思います． GitのコミットやPRごとにCIが回るようにして，ルールに即した静的解析ツールが動く． そして自動的にアンチパターンを防げるようになれば，生産性は大幅に向上するはずです． 学生で，かつ(長期インターン等を除き)個人の学習の範囲でしか開発していないような私からすると， このような 実際に業務で開発している人 の考え方に触れることができて，凄い勉強になりました． 1.1.3 課題まとめここからは，講義資料に記載されている課題について考え，実装してみたいと思います． 本インターンの活動中には時間が足りず解答しきれなかったので，夜の時間等にちまちま解き進めていました． 1.1.3.1 構文解析でわかること，わからないこと パッケージ変数の数 ファイル中に定義されている構造体のフィールド名 インポートしているファイルのパス は構文解析で検出することができます． go/ast のAPIでは(おそらく)以下のように対応しています． パッケージ変数の数 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.VAR を数える ファイル中に定義されている構造体のフィールド名 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.TYPE を検査する ast.GenDecl.Specs のエントリを *ast.TypeSpec にキャストし， .Type を *ast.StructType にキャストする go/types から Info.Defs のマップを探索し，types.Object.Type() を *types.Struct にキャストして NumFields() 分ループ インポートしているファイルのパス ast.File.Imports のエントリのそれぞれが， $GOPATH/src もしくは $GOROOT/src に存在するかどうかを調べれば良い $GOPATH/src にはサードパーティ製パッケージが， $GOROOT/src には標準パッケージが存在 逆に，context.Context 型のフィールドを持つ構造体の検出は構文解析だけではできません． 型チェックの機構やパッケージ解析を用いて， std/context.Context であるかどうか判断しないといけません． (別パッケージの Context 構造体をフィールドに持っている場合も，構文上は全く同じであり，ASTを探索するだけで検出できない) 1.1.3.2 型チェックでわかること，わからないことまず ｢context.Context 型のフィールドを持つ構造体の検出｣は型チェックで検出することができます． これについてはこちらのリポジトリにそのまま同じ用途の静的解析ツールが存在するので，参考にすると良いでしょう． 次に ｢使われていない非公開なパッケージ関数｣ ですが，これも構文解析や型チェックの情報で検出可能です． ASTを探索し， 関数呼び出し 関数ポインタ代入 返り値に指定されているか のいずれも行われていない場合を検出すれば良さそうです． 逆に， ｢100 + 200 などの定数式の評価｣ は型チェックだけでは不可能です． go/constants のように定数評価器のパッケージを利用する等が必要です． 講義資料には go/types や go/constant によって定数式の評価が可能と書いてあります． Golangのパッケージはとても優秀ですね． また， ｢インタフェース型のメソッドを呼び出した場合のレシーバの実際の型｣ も型チェックだけでは難しいと思います． インタフェース型メソッドの呼び出しに渡される引数から ポインタ解析( golang.org/x/tools/go/pointer ) を行う必要がありそうです． 1.1.3.3 不要なパッケージ関数の判別パッケージ外部に公開されておらず，またどの関数からも呼び出されていないような関数の検出です． こちらの課題で解き方を簡単に考察してみたので，これの通り実装","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:1","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#静的解析ツールの開発しやすさ"},{"categories":null,"content":" 1.1 座学記事冒頭に記載した資料をもとに，静的解析関連の知識を勉強しました． 私は普段コンパイラ理論の勉強をしているので字句解析や構文解析，型検査やSSAについては多少の前提知識がありましたが， 静的解析という目線ではただ｢データ構造を構築すること｣だけでなく， ツールの目的 に応じてデータ構造を柔軟に利用することが求められることがわかり，勉強になりました． 講義の具体的な内容についてはここでは触れません(講義資料が本当に素晴らしいので，そちらを参照ください)が， いくつか個人的に感じたことについて触れておきます． 見たくない人は 開発セクション まで読み飛ばしていただければ． 1.1.1 Golangの良さ私は最近，基本的にRust以外のプログラミング言語を触っていません． Atcoder等の解答にPythonを用いたり，ちょっとしたコード例としてHaskellを使うみたいなことはありますが， Golangをガッツリ触ったのは久しぶりなので，新鮮で楽しかったです． Golangの良さは既に語り尽くされていて，多くの人がブログ等で言及しているので， ここでは｢静的解析ツールを作る上で｣の利点について述べようと思っています． 1.1.1.1 標準パッケージの豊富さ･扱いやすさそんなことはもう知ってるよ! と思うかもしれませんが，改めて考えてみても優秀です． 講義資料にも書いてありますが，静的解析ツールには フラットな “ソースコード” というテキストから 階層/親子構造 を抽出したり， 識別子が参照する･される等の 依存関係 も解析する必要があります． そのために字句解析，構文解析，パッケージ間の依存グラフの構築等も必要になります． 言語処理系をスクラッチで書いたことがある人はイメージしやすいと思いますが， これらアルゴリズムの実装には 多くのコスト がかかり， かなりの労力を要します． しかし，GolangにはトークンやAST,構文解析や型関連の標準パッケージが存在します． また， x/tools/go 以下には静的解析系ツールやコールグラフ，CFGやSSA等の更に便利な機能が存在します． 例えば，Golangのソースコードに対し構文解析を行い，インポートしているパッケージを表示するプログラムを見てみましょう． 引用: Example - ParseFile package main import ( \"fmt\" \"go/parser\" \"go/token\" ) func main() { fset := token.NewFileSet() // positions are relative to fset src := `package foo import ( \"fmt\" \"time\" ) func bar() { fmt.Println(time.Now()) }` // Parse src but stop after processing the imports. f, err := parser.ParseFile(fset, \"\", src, parser.ImportsOnly) if err != nil { fmt.Println(err) return } // Print the imports from the file's AST. for _, s := range f.Imports { fmt.Println(s.Path.Value) } } このように非常にシンプルに解析資源を得られるので， 静的解析ツールの書きやすさはかなり高いと思います． x/tools/go/analysis がその最たる例で， 静的解析ツールを開発するためのパッケージ が用意されています． これについては上記講義資料に加えて，こちらの記事も参考になると思います． このように，標準･準標準パッケージが豊富であり扱いやすいところからも， Golangで静的解析ツールが書きやすい と言えそうです． 1.1.1.2 静的解析ツールの開発しやすさ実際に作り始めるにはいくつかのお作法が存在する静的解析ツールですが， 本インターンのメンターである @tenntennさん も在籍する Org の GoStaticAnalysis が 雛形ジェネレータ を公開しています． 使い方はREADMEや講義資料を参照してください． skeleton を利用すればすぐに静的解析ツールの開発を始める事ができます，非常に便利です． このあと紹介する課題を解く際に使ってみます． 1.1.2 ｢ルールを作る場合はツールも作る｣本インターンで一番共感した講義内容の一つです． 講義資料の23ページに書かれています． コーディングルールやベストプラクティス，アンチパターンなど， プログラミング中に意識し，開発チーム内で注意しあわなければいけないことはいくつか存在します． (これは個人開発でも同様に重要だと考えています) これらを意識し続けることは簡単ではありません． コンパイルエラーが発生するのであれば特に大きな問題にはなりませんが， そういう訳ではないので｢無意識のうちに良くない書き方をする｣ということが考えられます． このようなミスを エンジニアリングによって解決する というのは非常にスマートで美しい考え方だと思います． GitのコミットやPRごとにCIが回るようにして，ルールに即した静的解析ツールが動く． そして自動的にアンチパターンを防げるようになれば，生産性は大幅に向上するはずです． 学生で，かつ(長期インターン等を除き)個人の学習の範囲でしか開発していないような私からすると， このような 実際に業務で開発している人 の考え方に触れることができて，凄い勉強になりました． 1.1.3 課題まとめここからは，講義資料に記載されている課題について考え，実装してみたいと思います． 本インターンの活動中には時間が足りず解答しきれなかったので，夜の時間等にちまちま解き進めていました． 1.1.3.1 構文解析でわかること，わからないこと パッケージ変数の数 ファイル中に定義されている構造体のフィールド名 インポートしているファイルのパス は構文解析で検出することができます． go/ast のAPIでは(おそらく)以下のように対応しています． パッケージ変数の数 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.VAR を数える ファイル中に定義されている構造体のフィールド名 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.TYPE を検査する ast.GenDecl.Specs のエントリを *ast.TypeSpec にキャストし， .Type を *ast.StructType にキャストする go/types から Info.Defs のマップを探索し，types.Object.Type() を *types.Struct にキャストして NumFields() 分ループ インポートしているファイルのパス ast.File.Imports のエントリのそれぞれが， $GOPATH/src もしくは $GOROOT/src に存在するかどうかを調べれば良い $GOPATH/src にはサードパーティ製パッケージが， $GOROOT/src には標準パッケージが存在 逆に，context.Context 型のフィールドを持つ構造体の検出は構文解析だけではできません． 型チェックの機構やパッケージ解析を用いて， std/context.Context であるかどうか判断しないといけません． (別パッケージの Context 構造体をフィールドに持っている場合も，構文上は全く同じであり，ASTを探索するだけで検出できない) 1.1.3.2 型チェックでわかること，わからないことまず ｢context.Context 型のフィールドを持つ構造体の検出｣は型チェックで検出することができます． これについてはこちらのリポジトリにそのまま同じ用途の静的解析ツールが存在するので，参考にすると良いでしょう． 次に ｢使われていない非公開なパッケージ関数｣ ですが，これも構文解析や型チェックの情報で検出可能です． ASTを探索し， 関数呼び出し 関数ポインタ代入 返り値に指定されているか のいずれも行われていない場合を検出すれば良さそうです． 逆に， ｢100 + 200 などの定数式の評価｣ は型チェックだけでは不可能です． go/constants のように定数評価器のパッケージを利用する等が必要です． 講義資料には go/types や go/constant によって定数式の評価が可能と書いてあります． Golangのパッケージはとても優秀ですね． また， ｢インタフェース型のメソッドを呼び出した場合のレシーバの実際の型｣ も型チェックだけでは難しいと思います． インタフェース型メソッドの呼び出しに渡される引数から ポインタ解析( golang.org/x/tools/go/pointer ) を行う必要がありそうです． 1.1.3.3 不要なパッケージ関数の判別パッケージ外部に公開されておらず，またどの関数からも呼び出されていないような関数の検出です． こちらの課題で解き方を簡単に考察してみたので，これの通り実装","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:1","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#ルールを作る場合はツールも作る"},{"categories":null,"content":" 1.1 座学記事冒頭に記載した資料をもとに，静的解析関連の知識を勉強しました． 私は普段コンパイラ理論の勉強をしているので字句解析や構文解析，型検査やSSAについては多少の前提知識がありましたが， 静的解析という目線ではただ｢データ構造を構築すること｣だけでなく， ツールの目的 に応じてデータ構造を柔軟に利用することが求められることがわかり，勉強になりました． 講義の具体的な内容についてはここでは触れません(講義資料が本当に素晴らしいので，そちらを参照ください)が， いくつか個人的に感じたことについて触れておきます． 見たくない人は 開発セクション まで読み飛ばしていただければ． 1.1.1 Golangの良さ私は最近，基本的にRust以外のプログラミング言語を触っていません． Atcoder等の解答にPythonを用いたり，ちょっとしたコード例としてHaskellを使うみたいなことはありますが， Golangをガッツリ触ったのは久しぶりなので，新鮮で楽しかったです． Golangの良さは既に語り尽くされていて，多くの人がブログ等で言及しているので， ここでは｢静的解析ツールを作る上で｣の利点について述べようと思っています． 1.1.1.1 標準パッケージの豊富さ･扱いやすさそんなことはもう知ってるよ! と思うかもしれませんが，改めて考えてみても優秀です． 講義資料にも書いてありますが，静的解析ツールには フラットな “ソースコード” というテキストから 階層/親子構造 を抽出したり， 識別子が参照する･される等の 依存関係 も解析する必要があります． そのために字句解析，構文解析，パッケージ間の依存グラフの構築等も必要になります． 言語処理系をスクラッチで書いたことがある人はイメージしやすいと思いますが， これらアルゴリズムの実装には 多くのコスト がかかり， かなりの労力を要します． しかし，GolangにはトークンやAST,構文解析や型関連の標準パッケージが存在します． また， x/tools/go 以下には静的解析系ツールやコールグラフ，CFGやSSA等の更に便利な機能が存在します． 例えば，Golangのソースコードに対し構文解析を行い，インポートしているパッケージを表示するプログラムを見てみましょう． 引用: Example - ParseFile package main import ( \"fmt\" \"go/parser\" \"go/token\" ) func main() { fset := token.NewFileSet() // positions are relative to fset src := `package foo import ( \"fmt\" \"time\" ) func bar() { fmt.Println(time.Now()) }` // Parse src but stop after processing the imports. f, err := parser.ParseFile(fset, \"\", src, parser.ImportsOnly) if err != nil { fmt.Println(err) return } // Print the imports from the file's AST. for _, s := range f.Imports { fmt.Println(s.Path.Value) } } このように非常にシンプルに解析資源を得られるので， 静的解析ツールの書きやすさはかなり高いと思います． x/tools/go/analysis がその最たる例で， 静的解析ツールを開発するためのパッケージ が用意されています． これについては上記講義資料に加えて，こちらの記事も参考になると思います． このように，標準･準標準パッケージが豊富であり扱いやすいところからも， Golangで静的解析ツールが書きやすい と言えそうです． 1.1.1.2 静的解析ツールの開発しやすさ実際に作り始めるにはいくつかのお作法が存在する静的解析ツールですが， 本インターンのメンターである @tenntennさん も在籍する Org の GoStaticAnalysis が 雛形ジェネレータ を公開しています． 使い方はREADMEや講義資料を参照してください． skeleton を利用すればすぐに静的解析ツールの開発を始める事ができます，非常に便利です． このあと紹介する課題を解く際に使ってみます． 1.1.2 ｢ルールを作る場合はツールも作る｣本インターンで一番共感した講義内容の一つです． 講義資料の23ページに書かれています． コーディングルールやベストプラクティス，アンチパターンなど， プログラミング中に意識し，開発チーム内で注意しあわなければいけないことはいくつか存在します． (これは個人開発でも同様に重要だと考えています) これらを意識し続けることは簡単ではありません． コンパイルエラーが発生するのであれば特に大きな問題にはなりませんが， そういう訳ではないので｢無意識のうちに良くない書き方をする｣ということが考えられます． このようなミスを エンジニアリングによって解決する というのは非常にスマートで美しい考え方だと思います． GitのコミットやPRごとにCIが回るようにして，ルールに即した静的解析ツールが動く． そして自動的にアンチパターンを防げるようになれば，生産性は大幅に向上するはずです． 学生で，かつ(長期インターン等を除き)個人の学習の範囲でしか開発していないような私からすると， このような 実際に業務で開発している人 の考え方に触れることができて，凄い勉強になりました． 1.1.3 課題まとめここからは，講義資料に記載されている課題について考え，実装してみたいと思います． 本インターンの活動中には時間が足りず解答しきれなかったので，夜の時間等にちまちま解き進めていました． 1.1.3.1 構文解析でわかること，わからないこと パッケージ変数の数 ファイル中に定義されている構造体のフィールド名 インポートしているファイルのパス は構文解析で検出することができます． go/ast のAPIでは(おそらく)以下のように対応しています． パッケージ変数の数 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.VAR を数える ファイル中に定義されている構造体のフィールド名 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.TYPE を検査する ast.GenDecl.Specs のエントリを *ast.TypeSpec にキャストし， .Type を *ast.StructType にキャストする go/types から Info.Defs のマップを探索し，types.Object.Type() を *types.Struct にキャストして NumFields() 分ループ インポートしているファイルのパス ast.File.Imports のエントリのそれぞれが， $GOPATH/src もしくは $GOROOT/src に存在するかどうかを調べれば良い $GOPATH/src にはサードパーティ製パッケージが， $GOROOT/src には標準パッケージが存在 逆に，context.Context 型のフィールドを持つ構造体の検出は構文解析だけではできません． 型チェックの機構やパッケージ解析を用いて， std/context.Context であるかどうか判断しないといけません． (別パッケージの Context 構造体をフィールドに持っている場合も，構文上は全く同じであり，ASTを探索するだけで検出できない) 1.1.3.2 型チェックでわかること，わからないことまず ｢context.Context 型のフィールドを持つ構造体の検出｣は型チェックで検出することができます． これについてはこちらのリポジトリにそのまま同じ用途の静的解析ツールが存在するので，参考にすると良いでしょう． 次に ｢使われていない非公開なパッケージ関数｣ ですが，これも構文解析や型チェックの情報で検出可能です． ASTを探索し， 関数呼び出し 関数ポインタ代入 返り値に指定されているか のいずれも行われていない場合を検出すれば良さそうです． 逆に， ｢100 + 200 などの定数式の評価｣ は型チェックだけでは不可能です． go/constants のように定数評価器のパッケージを利用する等が必要です． 講義資料には go/types や go/constant によって定数式の評価が可能と書いてあります． Golangのパッケージはとても優秀ですね． また， ｢インタフェース型のメソッドを呼び出した場合のレシーバの実際の型｣ も型チェックだけでは難しいと思います． インタフェース型メソッドの呼び出しに渡される引数から ポインタ解析( golang.org/x/tools/go/pointer ) を行う必要がありそうです． 1.1.3.3 不要なパッケージ関数の判別パッケージ外部に公開されておらず，またどの関数からも呼び出されていないような関数の検出です． こちらの課題で解き方を簡単に考察してみたので，これの通り実装","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:1","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#課題まとめ"},{"categories":null,"content":" 1.1 座学記事冒頭に記載した資料をもとに，静的解析関連の知識を勉強しました． 私は普段コンパイラ理論の勉強をしているので字句解析や構文解析，型検査やSSAについては多少の前提知識がありましたが， 静的解析という目線ではただ｢データ構造を構築すること｣だけでなく， ツールの目的 に応じてデータ構造を柔軟に利用することが求められることがわかり，勉強になりました． 講義の具体的な内容についてはここでは触れません(講義資料が本当に素晴らしいので，そちらを参照ください)が， いくつか個人的に感じたことについて触れておきます． 見たくない人は 開発セクション まで読み飛ばしていただければ． 1.1.1 Golangの良さ私は最近，基本的にRust以外のプログラミング言語を触っていません． Atcoder等の解答にPythonを用いたり，ちょっとしたコード例としてHaskellを使うみたいなことはありますが， Golangをガッツリ触ったのは久しぶりなので，新鮮で楽しかったです． Golangの良さは既に語り尽くされていて，多くの人がブログ等で言及しているので， ここでは｢静的解析ツールを作る上で｣の利点について述べようと思っています． 1.1.1.1 標準パッケージの豊富さ･扱いやすさそんなことはもう知ってるよ! と思うかもしれませんが，改めて考えてみても優秀です． 講義資料にも書いてありますが，静的解析ツールには フラットな “ソースコード” というテキストから 階層/親子構造 を抽出したり， 識別子が参照する･される等の 依存関係 も解析する必要があります． そのために字句解析，構文解析，パッケージ間の依存グラフの構築等も必要になります． 言語処理系をスクラッチで書いたことがある人はイメージしやすいと思いますが， これらアルゴリズムの実装には 多くのコスト がかかり， かなりの労力を要します． しかし，GolangにはトークンやAST,構文解析や型関連の標準パッケージが存在します． また， x/tools/go 以下には静的解析系ツールやコールグラフ，CFGやSSA等の更に便利な機能が存在します． 例えば，Golangのソースコードに対し構文解析を行い，インポートしているパッケージを表示するプログラムを見てみましょう． 引用: Example - ParseFile package main import ( \"fmt\" \"go/parser\" \"go/token\" ) func main() { fset := token.NewFileSet() // positions are relative to fset src := `package foo import ( \"fmt\" \"time\" ) func bar() { fmt.Println(time.Now()) }` // Parse src but stop after processing the imports. f, err := parser.ParseFile(fset, \"\", src, parser.ImportsOnly) if err != nil { fmt.Println(err) return } // Print the imports from the file's AST. for _, s := range f.Imports { fmt.Println(s.Path.Value) } } このように非常にシンプルに解析資源を得られるので， 静的解析ツールの書きやすさはかなり高いと思います． x/tools/go/analysis がその最たる例で， 静的解析ツールを開発するためのパッケージ が用意されています． これについては上記講義資料に加えて，こちらの記事も参考になると思います． このように，標準･準標準パッケージが豊富であり扱いやすいところからも， Golangで静的解析ツールが書きやすい と言えそうです． 1.1.1.2 静的解析ツールの開発しやすさ実際に作り始めるにはいくつかのお作法が存在する静的解析ツールですが， 本インターンのメンターである @tenntennさん も在籍する Org の GoStaticAnalysis が 雛形ジェネレータ を公開しています． 使い方はREADMEや講義資料を参照してください． skeleton を利用すればすぐに静的解析ツールの開発を始める事ができます，非常に便利です． このあと紹介する課題を解く際に使ってみます． 1.1.2 ｢ルールを作る場合はツールも作る｣本インターンで一番共感した講義内容の一つです． 講義資料の23ページに書かれています． コーディングルールやベストプラクティス，アンチパターンなど， プログラミング中に意識し，開発チーム内で注意しあわなければいけないことはいくつか存在します． (これは個人開発でも同様に重要だと考えています) これらを意識し続けることは簡単ではありません． コンパイルエラーが発生するのであれば特に大きな問題にはなりませんが， そういう訳ではないので｢無意識のうちに良くない書き方をする｣ということが考えられます． このようなミスを エンジニアリングによって解決する というのは非常にスマートで美しい考え方だと思います． GitのコミットやPRごとにCIが回るようにして，ルールに即した静的解析ツールが動く． そして自動的にアンチパターンを防げるようになれば，生産性は大幅に向上するはずです． 学生で，かつ(長期インターン等を除き)個人の学習の範囲でしか開発していないような私からすると， このような 実際に業務で開発している人 の考え方に触れることができて，凄い勉強になりました． 1.1.3 課題まとめここからは，講義資料に記載されている課題について考え，実装してみたいと思います． 本インターンの活動中には時間が足りず解答しきれなかったので，夜の時間等にちまちま解き進めていました． 1.1.3.1 構文解析でわかること，わからないこと パッケージ変数の数 ファイル中に定義されている構造体のフィールド名 インポートしているファイルのパス は構文解析で検出することができます． go/ast のAPIでは(おそらく)以下のように対応しています． パッケージ変数の数 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.VAR を数える ファイル中に定義されている構造体のフィールド名 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.TYPE を検査する ast.GenDecl.Specs のエントリを *ast.TypeSpec にキャストし， .Type を *ast.StructType にキャストする go/types から Info.Defs のマップを探索し，types.Object.Type() を *types.Struct にキャストして NumFields() 分ループ インポートしているファイルのパス ast.File.Imports のエントリのそれぞれが， $GOPATH/src もしくは $GOROOT/src に存在するかどうかを調べれば良い $GOPATH/src にはサードパーティ製パッケージが， $GOROOT/src には標準パッケージが存在 逆に，context.Context 型のフィールドを持つ構造体の検出は構文解析だけではできません． 型チェックの機構やパッケージ解析を用いて， std/context.Context であるかどうか判断しないといけません． (別パッケージの Context 構造体をフィールドに持っている場合も，構文上は全く同じであり，ASTを探索するだけで検出できない) 1.1.3.2 型チェックでわかること，わからないことまず ｢context.Context 型のフィールドを持つ構造体の検出｣は型チェックで検出することができます． これについてはこちらのリポジトリにそのまま同じ用途の静的解析ツールが存在するので，参考にすると良いでしょう． 次に ｢使われていない非公開なパッケージ関数｣ ですが，これも構文解析や型チェックの情報で検出可能です． ASTを探索し， 関数呼び出し 関数ポインタ代入 返り値に指定されているか のいずれも行われていない場合を検出すれば良さそうです． 逆に， ｢100 + 200 などの定数式の評価｣ は型チェックだけでは不可能です． go/constants のように定数評価器のパッケージを利用する等が必要です． 講義資料には go/types や go/constant によって定数式の評価が可能と書いてあります． Golangのパッケージはとても優秀ですね． また， ｢インタフェース型のメソッドを呼び出した場合のレシーバの実際の型｣ も型チェックだけでは難しいと思います． インタフェース型メソッドの呼び出しに渡される引数から ポインタ解析( golang.org/x/tools/go/pointer ) を行う必要がありそうです． 1.1.3.3 不要なパッケージ関数の判別パッケージ外部に公開されておらず，またどの関数からも呼び出されていないような関数の検出です． こちらの課題で解き方を簡単に考察してみたので，これの通り実装","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:1","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#構文解析でわかることわからないことhttpsdocsgooglecompresentationd1i4phnzv2dfombrcpa-xd0talcx6pbkpls6wxghomjogeditslideidg6298a0e67590aa1e_30"},{"categories":null,"content":" 1.1 座学記事冒頭に記載した資料をもとに，静的解析関連の知識を勉強しました． 私は普段コンパイラ理論の勉強をしているので字句解析や構文解析，型検査やSSAについては多少の前提知識がありましたが， 静的解析という目線ではただ｢データ構造を構築すること｣だけでなく， ツールの目的 に応じてデータ構造を柔軟に利用することが求められることがわかり，勉強になりました． 講義の具体的な内容についてはここでは触れません(講義資料が本当に素晴らしいので，そちらを参照ください)が， いくつか個人的に感じたことについて触れておきます． 見たくない人は 開発セクション まで読み飛ばしていただければ． 1.1.1 Golangの良さ私は最近，基本的にRust以外のプログラミング言語を触っていません． Atcoder等の解答にPythonを用いたり，ちょっとしたコード例としてHaskellを使うみたいなことはありますが， Golangをガッツリ触ったのは久しぶりなので，新鮮で楽しかったです． Golangの良さは既に語り尽くされていて，多くの人がブログ等で言及しているので， ここでは｢静的解析ツールを作る上で｣の利点について述べようと思っています． 1.1.1.1 標準パッケージの豊富さ･扱いやすさそんなことはもう知ってるよ! と思うかもしれませんが，改めて考えてみても優秀です． 講義資料にも書いてありますが，静的解析ツールには フラットな “ソースコード” というテキストから 階層/親子構造 を抽出したり， 識別子が参照する･される等の 依存関係 も解析する必要があります． そのために字句解析，構文解析，パッケージ間の依存グラフの構築等も必要になります． 言語処理系をスクラッチで書いたことがある人はイメージしやすいと思いますが， これらアルゴリズムの実装には 多くのコスト がかかり， かなりの労力を要します． しかし，GolangにはトークンやAST,構文解析や型関連の標準パッケージが存在します． また， x/tools/go 以下には静的解析系ツールやコールグラフ，CFGやSSA等の更に便利な機能が存在します． 例えば，Golangのソースコードに対し構文解析を行い，インポートしているパッケージを表示するプログラムを見てみましょう． 引用: Example - ParseFile package main import ( \"fmt\" \"go/parser\" \"go/token\" ) func main() { fset := token.NewFileSet() // positions are relative to fset src := `package foo import ( \"fmt\" \"time\" ) func bar() { fmt.Println(time.Now()) }` // Parse src but stop after processing the imports. f, err := parser.ParseFile(fset, \"\", src, parser.ImportsOnly) if err != nil { fmt.Println(err) return } // Print the imports from the file's AST. for _, s := range f.Imports { fmt.Println(s.Path.Value) } } このように非常にシンプルに解析資源を得られるので， 静的解析ツールの書きやすさはかなり高いと思います． x/tools/go/analysis がその最たる例で， 静的解析ツールを開発するためのパッケージ が用意されています． これについては上記講義資料に加えて，こちらの記事も参考になると思います． このように，標準･準標準パッケージが豊富であり扱いやすいところからも， Golangで静的解析ツールが書きやすい と言えそうです． 1.1.1.2 静的解析ツールの開発しやすさ実際に作り始めるにはいくつかのお作法が存在する静的解析ツールですが， 本インターンのメンターである @tenntennさん も在籍する Org の GoStaticAnalysis が 雛形ジェネレータ を公開しています． 使い方はREADMEや講義資料を参照してください． skeleton を利用すればすぐに静的解析ツールの開発を始める事ができます，非常に便利です． このあと紹介する課題を解く際に使ってみます． 1.1.2 ｢ルールを作る場合はツールも作る｣本インターンで一番共感した講義内容の一つです． 講義資料の23ページに書かれています． コーディングルールやベストプラクティス，アンチパターンなど， プログラミング中に意識し，開発チーム内で注意しあわなければいけないことはいくつか存在します． (これは個人開発でも同様に重要だと考えています) これらを意識し続けることは簡単ではありません． コンパイルエラーが発生するのであれば特に大きな問題にはなりませんが， そういう訳ではないので｢無意識のうちに良くない書き方をする｣ということが考えられます． このようなミスを エンジニアリングによって解決する というのは非常にスマートで美しい考え方だと思います． GitのコミットやPRごとにCIが回るようにして，ルールに即した静的解析ツールが動く． そして自動的にアンチパターンを防げるようになれば，生産性は大幅に向上するはずです． 学生で，かつ(長期インターン等を除き)個人の学習の範囲でしか開発していないような私からすると， このような 実際に業務で開発している人 の考え方に触れることができて，凄い勉強になりました． 1.1.3 課題まとめここからは，講義資料に記載されている課題について考え，実装してみたいと思います． 本インターンの活動中には時間が足りず解答しきれなかったので，夜の時間等にちまちま解き進めていました． 1.1.3.1 構文解析でわかること，わからないこと パッケージ変数の数 ファイル中に定義されている構造体のフィールド名 インポートしているファイルのパス は構文解析で検出することができます． go/ast のAPIでは(おそらく)以下のように対応しています． パッケージ変数の数 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.VAR を数える ファイル中に定義されている構造体のフィールド名 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.TYPE を検査する ast.GenDecl.Specs のエントリを *ast.TypeSpec にキャストし， .Type を *ast.StructType にキャストする go/types から Info.Defs のマップを探索し，types.Object.Type() を *types.Struct にキャストして NumFields() 分ループ インポートしているファイルのパス ast.File.Imports のエントリのそれぞれが， $GOPATH/src もしくは $GOROOT/src に存在するかどうかを調べれば良い $GOPATH/src にはサードパーティ製パッケージが， $GOROOT/src には標準パッケージが存在 逆に，context.Context 型のフィールドを持つ構造体の検出は構文解析だけではできません． 型チェックの機構やパッケージ解析を用いて， std/context.Context であるかどうか判断しないといけません． (別パッケージの Context 構造体をフィールドに持っている場合も，構文上は全く同じであり，ASTを探索するだけで検出できない) 1.1.3.2 型チェックでわかること，わからないことまず ｢context.Context 型のフィールドを持つ構造体の検出｣は型チェックで検出することができます． これについてはこちらのリポジトリにそのまま同じ用途の静的解析ツールが存在するので，参考にすると良いでしょう． 次に ｢使われていない非公開なパッケージ関数｣ ですが，これも構文解析や型チェックの情報で検出可能です． ASTを探索し， 関数呼び出し 関数ポインタ代入 返り値に指定されているか のいずれも行われていない場合を検出すれば良さそうです． 逆に， ｢100 + 200 などの定数式の評価｣ は型チェックだけでは不可能です． go/constants のように定数評価器のパッケージを利用する等が必要です． 講義資料には go/types や go/constant によって定数式の評価が可能と書いてあります． Golangのパッケージはとても優秀ですね． また， ｢インタフェース型のメソッドを呼び出した場合のレシーバの実際の型｣ も型チェックだけでは難しいと思います． インタフェース型メソッドの呼び出しに渡される引数から ポインタ解析( golang.org/x/tools/go/pointer ) を行う必要がありそうです． 1.1.3.3 不要なパッケージ関数の判別パッケージ外部に公開されておらず，またどの関数からも呼び出されていないような関数の検出です． こちらの課題で解き方を簡単に考察してみたので，これの通り実装","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:1","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#型チェックでわかることわからないことhttpsdocsgooglecompresentationd1i4phnzv2dfombrcpa-xd0talcx6pbkpls6wxghomjogeditslideidg6298a0e67590aa1e_36"},{"categories":null,"content":" 1.1 座学記事冒頭に記載した資料をもとに，静的解析関連の知識を勉強しました． 私は普段コンパイラ理論の勉強をしているので字句解析や構文解析，型検査やSSAについては多少の前提知識がありましたが， 静的解析という目線ではただ｢データ構造を構築すること｣だけでなく， ツールの目的 に応じてデータ構造を柔軟に利用することが求められることがわかり，勉強になりました． 講義の具体的な内容についてはここでは触れません(講義資料が本当に素晴らしいので，そちらを参照ください)が， いくつか個人的に感じたことについて触れておきます． 見たくない人は 開発セクション まで読み飛ばしていただければ． 1.1.1 Golangの良さ私は最近，基本的にRust以外のプログラミング言語を触っていません． Atcoder等の解答にPythonを用いたり，ちょっとしたコード例としてHaskellを使うみたいなことはありますが， Golangをガッツリ触ったのは久しぶりなので，新鮮で楽しかったです． Golangの良さは既に語り尽くされていて，多くの人がブログ等で言及しているので， ここでは｢静的解析ツールを作る上で｣の利点について述べようと思っています． 1.1.1.1 標準パッケージの豊富さ･扱いやすさそんなことはもう知ってるよ! と思うかもしれませんが，改めて考えてみても優秀です． 講義資料にも書いてありますが，静的解析ツールには フラットな “ソースコード” というテキストから 階層/親子構造 を抽出したり， 識別子が参照する･される等の 依存関係 も解析する必要があります． そのために字句解析，構文解析，パッケージ間の依存グラフの構築等も必要になります． 言語処理系をスクラッチで書いたことがある人はイメージしやすいと思いますが， これらアルゴリズムの実装には 多くのコスト がかかり， かなりの労力を要します． しかし，GolangにはトークンやAST,構文解析や型関連の標準パッケージが存在します． また， x/tools/go 以下には静的解析系ツールやコールグラフ，CFGやSSA等の更に便利な機能が存在します． 例えば，Golangのソースコードに対し構文解析を行い，インポートしているパッケージを表示するプログラムを見てみましょう． 引用: Example - ParseFile package main import ( \"fmt\" \"go/parser\" \"go/token\" ) func main() { fset := token.NewFileSet() // positions are relative to fset src := `package foo import ( \"fmt\" \"time\" ) func bar() { fmt.Println(time.Now()) }` // Parse src but stop after processing the imports. f, err := parser.ParseFile(fset, \"\", src, parser.ImportsOnly) if err != nil { fmt.Println(err) return } // Print the imports from the file's AST. for _, s := range f.Imports { fmt.Println(s.Path.Value) } } このように非常にシンプルに解析資源を得られるので， 静的解析ツールの書きやすさはかなり高いと思います． x/tools/go/analysis がその最たる例で， 静的解析ツールを開発するためのパッケージ が用意されています． これについては上記講義資料に加えて，こちらの記事も参考になると思います． このように，標準･準標準パッケージが豊富であり扱いやすいところからも， Golangで静的解析ツールが書きやすい と言えそうです． 1.1.1.2 静的解析ツールの開発しやすさ実際に作り始めるにはいくつかのお作法が存在する静的解析ツールですが， 本インターンのメンターである @tenntennさん も在籍する Org の GoStaticAnalysis が 雛形ジェネレータ を公開しています． 使い方はREADMEや講義資料を参照してください． skeleton を利用すればすぐに静的解析ツールの開発を始める事ができます，非常に便利です． このあと紹介する課題を解く際に使ってみます． 1.1.2 ｢ルールを作る場合はツールも作る｣本インターンで一番共感した講義内容の一つです． 講義資料の23ページに書かれています． コーディングルールやベストプラクティス，アンチパターンなど， プログラミング中に意識し，開発チーム内で注意しあわなければいけないことはいくつか存在します． (これは個人開発でも同様に重要だと考えています) これらを意識し続けることは簡単ではありません． コンパイルエラーが発生するのであれば特に大きな問題にはなりませんが， そういう訳ではないので｢無意識のうちに良くない書き方をする｣ということが考えられます． このようなミスを エンジニアリングによって解決する というのは非常にスマートで美しい考え方だと思います． GitのコミットやPRごとにCIが回るようにして，ルールに即した静的解析ツールが動く． そして自動的にアンチパターンを防げるようになれば，生産性は大幅に向上するはずです． 学生で，かつ(長期インターン等を除き)個人の学習の範囲でしか開発していないような私からすると， このような 実際に業務で開発している人 の考え方に触れることができて，凄い勉強になりました． 1.1.3 課題まとめここからは，講義資料に記載されている課題について考え，実装してみたいと思います． 本インターンの活動中には時間が足りず解答しきれなかったので，夜の時間等にちまちま解き進めていました． 1.1.3.1 構文解析でわかること，わからないこと パッケージ変数の数 ファイル中に定義されている構造体のフィールド名 インポートしているファイルのパス は構文解析で検出することができます． go/ast のAPIでは(おそらく)以下のように対応しています． パッケージ変数の数 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.VAR を数える ファイル中に定義されている構造体のフィールド名 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.TYPE を検査する ast.GenDecl.Specs のエントリを *ast.TypeSpec にキャストし， .Type を *ast.StructType にキャストする go/types から Info.Defs のマップを探索し，types.Object.Type() を *types.Struct にキャストして NumFields() 分ループ インポートしているファイルのパス ast.File.Imports のエントリのそれぞれが， $GOPATH/src もしくは $GOROOT/src に存在するかどうかを調べれば良い $GOPATH/src にはサードパーティ製パッケージが， $GOROOT/src には標準パッケージが存在 逆に，context.Context 型のフィールドを持つ構造体の検出は構文解析だけではできません． 型チェックの機構やパッケージ解析を用いて， std/context.Context であるかどうか判断しないといけません． (別パッケージの Context 構造体をフィールドに持っている場合も，構文上は全く同じであり，ASTを探索するだけで検出できない) 1.1.3.2 型チェックでわかること，わからないことまず ｢context.Context 型のフィールドを持つ構造体の検出｣は型チェックで検出することができます． これについてはこちらのリポジトリにそのまま同じ用途の静的解析ツールが存在するので，参考にすると良いでしょう． 次に ｢使われていない非公開なパッケージ関数｣ ですが，これも構文解析や型チェックの情報で検出可能です． ASTを探索し， 関数呼び出し 関数ポインタ代入 返り値に指定されているか のいずれも行われていない場合を検出すれば良さそうです． 逆に， ｢100 + 200 などの定数式の評価｣ は型チェックだけでは不可能です． go/constants のように定数評価器のパッケージを利用する等が必要です． 講義資料には go/types や go/constant によって定数式の評価が可能と書いてあります． Golangのパッケージはとても優秀ですね． また， ｢インタフェース型のメソッドを呼び出した場合のレシーバの実際の型｣ も型チェックだけでは難しいと思います． インタフェース型メソッドの呼び出しに渡される引数から ポインタ解析( golang.org/x/tools/go/pointer ) を行う必要がありそうです． 1.1.3.3 不要なパッケージ関数の判別パッケージ外部に公開されておらず，またどの関数からも呼び出されていないような関数の検出です． こちらの課題で解き方を簡単に考察してみたので，これの通り実装","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:1","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#不要なパッケージ関数の判別httpsdocsgooglecompresentationd1i4phnzv2dfombrcpa-xd0talcx6pbkpls6wxghomjogeditslideidg80c1410104_5_281"},{"categories":null,"content":" 1.1 座学記事冒頭に記載した資料をもとに，静的解析関連の知識を勉強しました． 私は普段コンパイラ理論の勉強をしているので字句解析や構文解析，型検査やSSAについては多少の前提知識がありましたが， 静的解析という目線ではただ｢データ構造を構築すること｣だけでなく， ツールの目的 に応じてデータ構造を柔軟に利用することが求められることがわかり，勉強になりました． 講義の具体的な内容についてはここでは触れません(講義資料が本当に素晴らしいので，そちらを参照ください)が， いくつか個人的に感じたことについて触れておきます． 見たくない人は 開発セクション まで読み飛ばしていただければ． 1.1.1 Golangの良さ私は最近，基本的にRust以外のプログラミング言語を触っていません． Atcoder等の解答にPythonを用いたり，ちょっとしたコード例としてHaskellを使うみたいなことはありますが， Golangをガッツリ触ったのは久しぶりなので，新鮮で楽しかったです． Golangの良さは既に語り尽くされていて，多くの人がブログ等で言及しているので， ここでは｢静的解析ツールを作る上で｣の利点について述べようと思っています． 1.1.1.1 標準パッケージの豊富さ･扱いやすさそんなことはもう知ってるよ! と思うかもしれませんが，改めて考えてみても優秀です． 講義資料にも書いてありますが，静的解析ツールには フラットな “ソースコード” というテキストから 階層/親子構造 を抽出したり， 識別子が参照する･される等の 依存関係 も解析する必要があります． そのために字句解析，構文解析，パッケージ間の依存グラフの構築等も必要になります． 言語処理系をスクラッチで書いたことがある人はイメージしやすいと思いますが， これらアルゴリズムの実装には 多くのコスト がかかり， かなりの労力を要します． しかし，GolangにはトークンやAST,構文解析や型関連の標準パッケージが存在します． また， x/tools/go 以下には静的解析系ツールやコールグラフ，CFGやSSA等の更に便利な機能が存在します． 例えば，Golangのソースコードに対し構文解析を行い，インポートしているパッケージを表示するプログラムを見てみましょう． 引用: Example - ParseFile package main import ( \"fmt\" \"go/parser\" \"go/token\" ) func main() { fset := token.NewFileSet() // positions are relative to fset src := `package foo import ( \"fmt\" \"time\" ) func bar() { fmt.Println(time.Now()) }` // Parse src but stop after processing the imports. f, err := parser.ParseFile(fset, \"\", src, parser.ImportsOnly) if err != nil { fmt.Println(err) return } // Print the imports from the file's AST. for _, s := range f.Imports { fmt.Println(s.Path.Value) } } このように非常にシンプルに解析資源を得られるので， 静的解析ツールの書きやすさはかなり高いと思います． x/tools/go/analysis がその最たる例で， 静的解析ツールを開発するためのパッケージ が用意されています． これについては上記講義資料に加えて，こちらの記事も参考になると思います． このように，標準･準標準パッケージが豊富であり扱いやすいところからも， Golangで静的解析ツールが書きやすい と言えそうです． 1.1.1.2 静的解析ツールの開発しやすさ実際に作り始めるにはいくつかのお作法が存在する静的解析ツールですが， 本インターンのメンターである @tenntennさん も在籍する Org の GoStaticAnalysis が 雛形ジェネレータ を公開しています． 使い方はREADMEや講義資料を参照してください． skeleton を利用すればすぐに静的解析ツールの開発を始める事ができます，非常に便利です． このあと紹介する課題を解く際に使ってみます． 1.1.2 ｢ルールを作る場合はツールも作る｣本インターンで一番共感した講義内容の一つです． 講義資料の23ページに書かれています． コーディングルールやベストプラクティス，アンチパターンなど， プログラミング中に意識し，開発チーム内で注意しあわなければいけないことはいくつか存在します． (これは個人開発でも同様に重要だと考えています) これらを意識し続けることは簡単ではありません． コンパイルエラーが発生するのであれば特に大きな問題にはなりませんが， そういう訳ではないので｢無意識のうちに良くない書き方をする｣ということが考えられます． このようなミスを エンジニアリングによって解決する というのは非常にスマートで美しい考え方だと思います． GitのコミットやPRごとにCIが回るようにして，ルールに即した静的解析ツールが動く． そして自動的にアンチパターンを防げるようになれば，生産性は大幅に向上するはずです． 学生で，かつ(長期インターン等を除き)個人の学習の範囲でしか開発していないような私からすると， このような 実際に業務で開発している人 の考え方に触れることができて，凄い勉強になりました． 1.1.3 課題まとめここからは，講義資料に記載されている課題について考え，実装してみたいと思います． 本インターンの活動中には時間が足りず解答しきれなかったので，夜の時間等にちまちま解き進めていました． 1.1.3.1 構文解析でわかること，わからないこと パッケージ変数の数 ファイル中に定義されている構造体のフィールド名 インポートしているファイルのパス は構文解析で検出することができます． go/ast のAPIでは(おそらく)以下のように対応しています． パッケージ変数の数 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.VAR を数える ファイル中に定義されている構造体のフィールド名 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.TYPE を検査する ast.GenDecl.Specs のエントリを *ast.TypeSpec にキャストし， .Type を *ast.StructType にキャストする go/types から Info.Defs のマップを探索し，types.Object.Type() を *types.Struct にキャストして NumFields() 分ループ インポートしているファイルのパス ast.File.Imports のエントリのそれぞれが， $GOPATH/src もしくは $GOROOT/src に存在するかどうかを調べれば良い $GOPATH/src にはサードパーティ製パッケージが， $GOROOT/src には標準パッケージが存在 逆に，context.Context 型のフィールドを持つ構造体の検出は構文解析だけではできません． 型チェックの機構やパッケージ解析を用いて， std/context.Context であるかどうか判断しないといけません． (別パッケージの Context 構造体をフィールドに持っている場合も，構文上は全く同じであり，ASTを探索するだけで検出できない) 1.1.3.2 型チェックでわかること，わからないことまず ｢context.Context 型のフィールドを持つ構造体の検出｣は型チェックで検出することができます． これについてはこちらのリポジトリにそのまま同じ用途の静的解析ツールが存在するので，参考にすると良いでしょう． 次に ｢使われていない非公開なパッケージ関数｣ ですが，これも構文解析や型チェックの情報で検出可能です． ASTを探索し， 関数呼び出し 関数ポインタ代入 返り値に指定されているか のいずれも行われていない場合を検出すれば良さそうです． 逆に， ｢100 + 200 などの定数式の評価｣ は型チェックだけでは不可能です． go/constants のように定数評価器のパッケージを利用する等が必要です． 講義資料には go/types や go/constant によって定数式の評価が可能と書いてあります． Golangのパッケージはとても優秀ですね． また， ｢インタフェース型のメソッドを呼び出した場合のレシーバの実際の型｣ も型チェックだけでは難しいと思います． インタフェース型メソッドの呼び出しに渡される引数から ポインタ解析( golang.org/x/tools/go/pointer ) を行う必要がありそうです． 1.1.3.3 不要なパッケージ関数の判別パッケージ外部に公開されておらず，またどの関数からも呼び出されていないような関数の検出です． こちらの課題で解き方を簡単に考察してみたので，これの通り実装","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:1","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#自作のanalyzerコレクションを作るhttpsdocsgooglecompresentationd1i4phnzv2dfombrcpa-xd0talcx6pbkpls6wxghomjogeditslideidg6298a0e67590aa1e_12"},{"categories":null,"content":" 1.1 座学記事冒頭に記載した資料をもとに，静的解析関連の知識を勉強しました． 私は普段コンパイラ理論の勉強をしているので字句解析や構文解析，型検査やSSAについては多少の前提知識がありましたが， 静的解析という目線ではただ｢データ構造を構築すること｣だけでなく， ツールの目的 に応じてデータ構造を柔軟に利用することが求められることがわかり，勉強になりました． 講義の具体的な内容についてはここでは触れません(講義資料が本当に素晴らしいので，そちらを参照ください)が， いくつか個人的に感じたことについて触れておきます． 見たくない人は 開発セクション まで読み飛ばしていただければ． 1.1.1 Golangの良さ私は最近，基本的にRust以外のプログラミング言語を触っていません． Atcoder等の解答にPythonを用いたり，ちょっとしたコード例としてHaskellを使うみたいなことはありますが， Golangをガッツリ触ったのは久しぶりなので，新鮮で楽しかったです． Golangの良さは既に語り尽くされていて，多くの人がブログ等で言及しているので， ここでは｢静的解析ツールを作る上で｣の利点について述べようと思っています． 1.1.1.1 標準パッケージの豊富さ･扱いやすさそんなことはもう知ってるよ! と思うかもしれませんが，改めて考えてみても優秀です． 講義資料にも書いてありますが，静的解析ツールには フラットな “ソースコード” というテキストから 階層/親子構造 を抽出したり， 識別子が参照する･される等の 依存関係 も解析する必要があります． そのために字句解析，構文解析，パッケージ間の依存グラフの構築等も必要になります． 言語処理系をスクラッチで書いたことがある人はイメージしやすいと思いますが， これらアルゴリズムの実装には 多くのコスト がかかり， かなりの労力を要します． しかし，GolangにはトークンやAST,構文解析や型関連の標準パッケージが存在します． また， x/tools/go 以下には静的解析系ツールやコールグラフ，CFGやSSA等の更に便利な機能が存在します． 例えば，Golangのソースコードに対し構文解析を行い，インポートしているパッケージを表示するプログラムを見てみましょう． 引用: Example - ParseFile package main import ( \"fmt\" \"go/parser\" \"go/token\" ) func main() { fset := token.NewFileSet() // positions are relative to fset src := `package foo import ( \"fmt\" \"time\" ) func bar() { fmt.Println(time.Now()) }` // Parse src but stop after processing the imports. f, err := parser.ParseFile(fset, \"\", src, parser.ImportsOnly) if err != nil { fmt.Println(err) return } // Print the imports from the file's AST. for _, s := range f.Imports { fmt.Println(s.Path.Value) } } このように非常にシンプルに解析資源を得られるので， 静的解析ツールの書きやすさはかなり高いと思います． x/tools/go/analysis がその最たる例で， 静的解析ツールを開発するためのパッケージ が用意されています． これについては上記講義資料に加えて，こちらの記事も参考になると思います． このように，標準･準標準パッケージが豊富であり扱いやすいところからも， Golangで静的解析ツールが書きやすい と言えそうです． 1.1.1.2 静的解析ツールの開発しやすさ実際に作り始めるにはいくつかのお作法が存在する静的解析ツールですが， 本インターンのメンターである @tenntennさん も在籍する Org の GoStaticAnalysis が 雛形ジェネレータ を公開しています． 使い方はREADMEや講義資料を参照してください． skeleton を利用すればすぐに静的解析ツールの開発を始める事ができます，非常に便利です． このあと紹介する課題を解く際に使ってみます． 1.1.2 ｢ルールを作る場合はツールも作る｣本インターンで一番共感した講義内容の一つです． 講義資料の23ページに書かれています． コーディングルールやベストプラクティス，アンチパターンなど， プログラミング中に意識し，開発チーム内で注意しあわなければいけないことはいくつか存在します． (これは個人開発でも同様に重要だと考えています) これらを意識し続けることは簡単ではありません． コンパイルエラーが発生するのであれば特に大きな問題にはなりませんが， そういう訳ではないので｢無意識のうちに良くない書き方をする｣ということが考えられます． このようなミスを エンジニアリングによって解決する というのは非常にスマートで美しい考え方だと思います． GitのコミットやPRごとにCIが回るようにして，ルールに即した静的解析ツールが動く． そして自動的にアンチパターンを防げるようになれば，生産性は大幅に向上するはずです． 学生で，かつ(長期インターン等を除き)個人の学習の範囲でしか開発していないような私からすると， このような 実際に業務で開発している人 の考え方に触れることができて，凄い勉強になりました． 1.1.3 課題まとめここからは，講義資料に記載されている課題について考え，実装してみたいと思います． 本インターンの活動中には時間が足りず解答しきれなかったので，夜の時間等にちまちま解き進めていました． 1.1.3.1 構文解析でわかること，わからないこと パッケージ変数の数 ファイル中に定義されている構造体のフィールド名 インポートしているファイルのパス は構文解析で検出することができます． go/ast のAPIでは(おそらく)以下のように対応しています． パッケージ変数の数 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.VAR を数える ファイル中に定義されている構造体のフィールド名 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.TYPE を検査する ast.GenDecl.Specs のエントリを *ast.TypeSpec にキャストし， .Type を *ast.StructType にキャストする go/types から Info.Defs のマップを探索し，types.Object.Type() を *types.Struct にキャストして NumFields() 分ループ インポートしているファイルのパス ast.File.Imports のエントリのそれぞれが， $GOPATH/src もしくは $GOROOT/src に存在するかどうかを調べれば良い $GOPATH/src にはサードパーティ製パッケージが， $GOROOT/src には標準パッケージが存在 逆に，context.Context 型のフィールドを持つ構造体の検出は構文解析だけではできません． 型チェックの機構やパッケージ解析を用いて， std/context.Context であるかどうか判断しないといけません． (別パッケージの Context 構造体をフィールドに持っている場合も，構文上は全く同じであり，ASTを探索するだけで検出できない) 1.1.3.2 型チェックでわかること，わからないことまず ｢context.Context 型のフィールドを持つ構造体の検出｣は型チェックで検出することができます． これについてはこちらのリポジトリにそのまま同じ用途の静的解析ツールが存在するので，参考にすると良いでしょう． 次に ｢使われていない非公開なパッケージ関数｣ ですが，これも構文解析や型チェックの情報で検出可能です． ASTを探索し， 関数呼び出し 関数ポインタ代入 返り値に指定されているか のいずれも行われていない場合を検出すれば良さそうです． 逆に， ｢100 + 200 などの定数式の評価｣ は型チェックだけでは不可能です． go/constants のように定数評価器のパッケージを利用する等が必要です． 講義資料には go/types や go/constant によって定数式の評価が可能と書いてあります． Golangのパッケージはとても優秀ですね． また， ｢インタフェース型のメソッドを呼び出した場合のレシーバの実際の型｣ も型チェックだけでは難しいと思います． インタフェース型メソッドの呼び出しに渡される引数から ポインタ解析( golang.org/x/tools/go/pointer ) を行う必要がありそうです． 1.1.3.3 不要なパッケージ関数の判別パッケージ外部に公開されておらず，またどの関数からも呼び出されていないような関数の検出です． こちらの課題で解き方を簡単に考察してみたので，これの通り実装","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:1","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#skeletonのインストールhttpsdocsgooglecompresentationd1i4phnzv2dfombrcpa-xd0talcx6pbkpls6wxghomjogeditslideidg6298a0e67590aa1e_18"},{"categories":null,"content":" 1.1 座学記事冒頭に記載した資料をもとに，静的解析関連の知識を勉強しました． 私は普段コンパイラ理論の勉強をしているので字句解析や構文解析，型検査やSSAについては多少の前提知識がありましたが， 静的解析という目線ではただ｢データ構造を構築すること｣だけでなく， ツールの目的 に応じてデータ構造を柔軟に利用することが求められることがわかり，勉強になりました． 講義の具体的な内容についてはここでは触れません(講義資料が本当に素晴らしいので，そちらを参照ください)が， いくつか個人的に感じたことについて触れておきます． 見たくない人は 開発セクション まで読み飛ばしていただければ． 1.1.1 Golangの良さ私は最近，基本的にRust以外のプログラミング言語を触っていません． Atcoder等の解答にPythonを用いたり，ちょっとしたコード例としてHaskellを使うみたいなことはありますが， Golangをガッツリ触ったのは久しぶりなので，新鮮で楽しかったです． Golangの良さは既に語り尽くされていて，多くの人がブログ等で言及しているので， ここでは｢静的解析ツールを作る上で｣の利点について述べようと思っています． 1.1.1.1 標準パッケージの豊富さ･扱いやすさそんなことはもう知ってるよ! と思うかもしれませんが，改めて考えてみても優秀です． 講義資料にも書いてありますが，静的解析ツールには フラットな “ソースコード” というテキストから 階層/親子構造 を抽出したり， 識別子が参照する･される等の 依存関係 も解析する必要があります． そのために字句解析，構文解析，パッケージ間の依存グラフの構築等も必要になります． 言語処理系をスクラッチで書いたことがある人はイメージしやすいと思いますが， これらアルゴリズムの実装には 多くのコスト がかかり， かなりの労力を要します． しかし，GolangにはトークンやAST,構文解析や型関連の標準パッケージが存在します． また， x/tools/go 以下には静的解析系ツールやコールグラフ，CFGやSSA等の更に便利な機能が存在します． 例えば，Golangのソースコードに対し構文解析を行い，インポートしているパッケージを表示するプログラムを見てみましょう． 引用: Example - ParseFile package main import ( \"fmt\" \"go/parser\" \"go/token\" ) func main() { fset := token.NewFileSet() // positions are relative to fset src := `package foo import ( \"fmt\" \"time\" ) func bar() { fmt.Println(time.Now()) }` // Parse src but stop after processing the imports. f, err := parser.ParseFile(fset, \"\", src, parser.ImportsOnly) if err != nil { fmt.Println(err) return } // Print the imports from the file's AST. for _, s := range f.Imports { fmt.Println(s.Path.Value) } } このように非常にシンプルに解析資源を得られるので， 静的解析ツールの書きやすさはかなり高いと思います． x/tools/go/analysis がその最たる例で， 静的解析ツールを開発するためのパッケージ が用意されています． これについては上記講義資料に加えて，こちらの記事も参考になると思います． このように，標準･準標準パッケージが豊富であり扱いやすいところからも， Golangで静的解析ツールが書きやすい と言えそうです． 1.1.1.2 静的解析ツールの開発しやすさ実際に作り始めるにはいくつかのお作法が存在する静的解析ツールですが， 本インターンのメンターである @tenntennさん も在籍する Org の GoStaticAnalysis が 雛形ジェネレータ を公開しています． 使い方はREADMEや講義資料を参照してください． skeleton を利用すればすぐに静的解析ツールの開発を始める事ができます，非常に便利です． このあと紹介する課題を解く際に使ってみます． 1.1.2 ｢ルールを作る場合はツールも作る｣本インターンで一番共感した講義内容の一つです． 講義資料の23ページに書かれています． コーディングルールやベストプラクティス，アンチパターンなど， プログラミング中に意識し，開発チーム内で注意しあわなければいけないことはいくつか存在します． (これは個人開発でも同様に重要だと考えています) これらを意識し続けることは簡単ではありません． コンパイルエラーが発生するのであれば特に大きな問題にはなりませんが， そういう訳ではないので｢無意識のうちに良くない書き方をする｣ということが考えられます． このようなミスを エンジニアリングによって解決する というのは非常にスマートで美しい考え方だと思います． GitのコミットやPRごとにCIが回るようにして，ルールに即した静的解析ツールが動く． そして自動的にアンチパターンを防げるようになれば，生産性は大幅に向上するはずです． 学生で，かつ(長期インターン等を除き)個人の学習の範囲でしか開発していないような私からすると， このような 実際に業務で開発している人 の考え方に触れることができて，凄い勉強になりました． 1.1.3 課題まとめここからは，講義資料に記載されている課題について考え，実装してみたいと思います． 本インターンの活動中には時間が足りず解答しきれなかったので，夜の時間等にちまちま解き進めていました． 1.1.3.1 構文解析でわかること，わからないこと パッケージ変数の数 ファイル中に定義されている構造体のフィールド名 インポートしているファイルのパス は構文解析で検出することができます． go/ast のAPIでは(おそらく)以下のように対応しています． パッケージ変数の数 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.VAR を数える ファイル中に定義されている構造体のフィールド名 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.TYPE を検査する ast.GenDecl.Specs のエントリを *ast.TypeSpec にキャストし， .Type を *ast.StructType にキャストする go/types から Info.Defs のマップを探索し，types.Object.Type() を *types.Struct にキャストして NumFields() 分ループ インポートしているファイルのパス ast.File.Imports のエントリのそれぞれが， $GOPATH/src もしくは $GOROOT/src に存在するかどうかを調べれば良い $GOPATH/src にはサードパーティ製パッケージが， $GOROOT/src には標準パッケージが存在 逆に，context.Context 型のフィールドを持つ構造体の検出は構文解析だけではできません． 型チェックの機構やパッケージ解析を用いて， std/context.Context であるかどうか判断しないといけません． (別パッケージの Context 構造体をフィールドに持っている場合も，構文上は全く同じであり，ASTを探索するだけで検出できない) 1.1.3.2 型チェックでわかること，わからないことまず ｢context.Context 型のフィールドを持つ構造体の検出｣は型チェックで検出することができます． これについてはこちらのリポジトリにそのまま同じ用途の静的解析ツールが存在するので，参考にすると良いでしょう． 次に ｢使われていない非公開なパッケージ関数｣ ですが，これも構文解析や型チェックの情報で検出可能です． ASTを探索し， 関数呼び出し 関数ポインタ代入 返り値に指定されているか のいずれも行われていない場合を検出すれば良さそうです． 逆に， ｢100 + 200 などの定数式の評価｣ は型チェックだけでは不可能です． go/constants のように定数評価器のパッケージを利用する等が必要です． 講義資料には go/types や go/constant によって定数式の評価が可能と書いてあります． Golangのパッケージはとても優秀ですね． また， ｢インタフェース型のメソッドを呼び出した場合のレシーバの実際の型｣ も型チェックだけでは難しいと思います． インタフェース型メソッドの呼び出しに渡される引数から ポインタ解析( golang.org/x/tools/go/pointer ) を行う必要がありそうです． 1.1.3.3 不要なパッケージ関数の判別パッケージ外部に公開されておらず，またどの関数からも呼び出されていないような関数の検出です． こちらの課題で解き方を簡単に考察してみたので，これの通り実装","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:1","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#インポート文の重複を検出しようhttpsdocsgooglecompresentationd1i4phnzv2dfombrcpa-xd0talcx6pbkpls6wxghomjogeditslideidg870cb4ff5f_0_989"},{"categories":null,"content":" 1.1 座学記事冒頭に記載した資料をもとに，静的解析関連の知識を勉強しました． 私は普段コンパイラ理論の勉強をしているので字句解析や構文解析，型検査やSSAについては多少の前提知識がありましたが， 静的解析という目線ではただ｢データ構造を構築すること｣だけでなく， ツールの目的 に応じてデータ構造を柔軟に利用することが求められることがわかり，勉強になりました． 講義の具体的な内容についてはここでは触れません(講義資料が本当に素晴らしいので，そちらを参照ください)が， いくつか個人的に感じたことについて触れておきます． 見たくない人は 開発セクション まで読み飛ばしていただければ． 1.1.1 Golangの良さ私は最近，基本的にRust以外のプログラミング言語を触っていません． Atcoder等の解答にPythonを用いたり，ちょっとしたコード例としてHaskellを使うみたいなことはありますが， Golangをガッツリ触ったのは久しぶりなので，新鮮で楽しかったです． Golangの良さは既に語り尽くされていて，多くの人がブログ等で言及しているので， ここでは｢静的解析ツールを作る上で｣の利点について述べようと思っています． 1.1.1.1 標準パッケージの豊富さ･扱いやすさそんなことはもう知ってるよ! と思うかもしれませんが，改めて考えてみても優秀です． 講義資料にも書いてありますが，静的解析ツールには フラットな “ソースコード” というテキストから 階層/親子構造 を抽出したり， 識別子が参照する･される等の 依存関係 も解析する必要があります． そのために字句解析，構文解析，パッケージ間の依存グラフの構築等も必要になります． 言語処理系をスクラッチで書いたことがある人はイメージしやすいと思いますが， これらアルゴリズムの実装には 多くのコスト がかかり， かなりの労力を要します． しかし，GolangにはトークンやAST,構文解析や型関連の標準パッケージが存在します． また， x/tools/go 以下には静的解析系ツールやコールグラフ，CFGやSSA等の更に便利な機能が存在します． 例えば，Golangのソースコードに対し構文解析を行い，インポートしているパッケージを表示するプログラムを見てみましょう． 引用: Example - ParseFile package main import ( \"fmt\" \"go/parser\" \"go/token\" ) func main() { fset := token.NewFileSet() // positions are relative to fset src := `package foo import ( \"fmt\" \"time\" ) func bar() { fmt.Println(time.Now()) }` // Parse src but stop after processing the imports. f, err := parser.ParseFile(fset, \"\", src, parser.ImportsOnly) if err != nil { fmt.Println(err) return } // Print the imports from the file's AST. for _, s := range f.Imports { fmt.Println(s.Path.Value) } } このように非常にシンプルに解析資源を得られるので， 静的解析ツールの書きやすさはかなり高いと思います． x/tools/go/analysis がその最たる例で， 静的解析ツールを開発するためのパッケージ が用意されています． これについては上記講義資料に加えて，こちらの記事も参考になると思います． このように，標準･準標準パッケージが豊富であり扱いやすいところからも， Golangで静的解析ツールが書きやすい と言えそうです． 1.1.1.2 静的解析ツールの開発しやすさ実際に作り始めるにはいくつかのお作法が存在する静的解析ツールですが， 本インターンのメンターである @tenntennさん も在籍する Org の GoStaticAnalysis が 雛形ジェネレータ を公開しています． 使い方はREADMEや講義資料を参照してください． skeleton を利用すればすぐに静的解析ツールの開発を始める事ができます，非常に便利です． このあと紹介する課題を解く際に使ってみます． 1.1.2 ｢ルールを作る場合はツールも作る｣本インターンで一番共感した講義内容の一つです． 講義資料の23ページに書かれています． コーディングルールやベストプラクティス，アンチパターンなど， プログラミング中に意識し，開発チーム内で注意しあわなければいけないことはいくつか存在します． (これは個人開発でも同様に重要だと考えています) これらを意識し続けることは簡単ではありません． コンパイルエラーが発生するのであれば特に大きな問題にはなりませんが， そういう訳ではないので｢無意識のうちに良くない書き方をする｣ということが考えられます． このようなミスを エンジニアリングによって解決する というのは非常にスマートで美しい考え方だと思います． GitのコミットやPRごとにCIが回るようにして，ルールに即した静的解析ツールが動く． そして自動的にアンチパターンを防げるようになれば，生産性は大幅に向上するはずです． 学生で，かつ(長期インターン等を除き)個人の学習の範囲でしか開発していないような私からすると， このような 実際に業務で開発している人 の考え方に触れることができて，凄い勉強になりました． 1.1.3 課題まとめここからは，講義資料に記載されている課題について考え，実装してみたいと思います． 本インターンの活動中には時間が足りず解答しきれなかったので，夜の時間等にちまちま解き進めていました． 1.1.3.1 構文解析でわかること，わからないこと パッケージ変数の数 ファイル中に定義されている構造体のフィールド名 インポートしているファイルのパス は構文解析で検出することができます． go/ast のAPIでは(おそらく)以下のように対応しています． パッケージ変数の数 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.VAR を数える ファイル中に定義されている構造体のフィールド名 ast.File.Decls のエントリを *ast.GenDecl にtype assertionし， .Tok == token.TYPE を検査する ast.GenDecl.Specs のエントリを *ast.TypeSpec にキャストし， .Type を *ast.StructType にキャストする go/types から Info.Defs のマップを探索し，types.Object.Type() を *types.Struct にキャストして NumFields() 分ループ インポートしているファイルのパス ast.File.Imports のエントリのそれぞれが， $GOPATH/src もしくは $GOROOT/src に存在するかどうかを調べれば良い $GOPATH/src にはサードパーティ製パッケージが， $GOROOT/src には標準パッケージが存在 逆に，context.Context 型のフィールドを持つ構造体の検出は構文解析だけではできません． 型チェックの機構やパッケージ解析を用いて， std/context.Context であるかどうか判断しないといけません． (別パッケージの Context 構造体をフィールドに持っている場合も，構文上は全く同じであり，ASTを探索するだけで検出できない) 1.1.3.2 型チェックでわかること，わからないことまず ｢context.Context 型のフィールドを持つ構造体の検出｣は型チェックで検出することができます． これについてはこちらのリポジトリにそのまま同じ用途の静的解析ツールが存在するので，参考にすると良いでしょう． 次に ｢使われていない非公開なパッケージ関数｣ ですが，これも構文解析や型チェックの情報で検出可能です． ASTを探索し， 関数呼び出し 関数ポインタ代入 返り値に指定されているか のいずれも行われていない場合を検出すれば良さそうです． 逆に， ｢100 + 200 などの定数式の評価｣ は型チェックだけでは不可能です． go/constants のように定数評価器のパッケージを利用する等が必要です． 講義資料には go/types や go/constant によって定数式の評価が可能と書いてあります． Golangのパッケージはとても優秀ですね． また， ｢インタフェース型のメソッドを呼び出した場合のレシーバの実際の型｣ も型チェックだけでは難しいと思います． インタフェース型メソッドの呼び出しに渡される引数から ポインタ解析( golang.org/x/tools/go/pointer ) を行う必要がありそうです． 1.1.3.3 不要なパッケージ関数の判別パッケージ外部に公開されておらず，またどの関数からも呼び出されていないような関数の検出です． こちらの課題で解き方を簡単に考察してみたので，これの通り実装","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:1","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#抽象構文木の確認"},{"categories":null,"content":" 1.2 開発私が今回作成したのは，次のようなツール群です． 本当はこれに加えて const 宣言すべきである識別子の検出 もやりたかったんですが，ちょっと実装速度が足りませんでした． やりたい人いらっしゃったら一緒にやりましょう!(投げやり) Golangでは var や id := を用いると標準でミュータブルですが， プリミティブな型かつ不変性を許容できる場合については const 宣言を使うことができます． これを検出するようなツールも作ろうと考えていました． (パッケージ変数，自動変数のどちらも検出したかった) リポジトリはこちら． 1.2.1 パッケージ間の依存関係可視化ツールGolangパッケージを指定すると，そのパッケージが依存するパッケージを探索し， DOT言語( グラフ構造の記述言語 )を生成する，というツールです． これはどちらかというと習作というか，勉強の為に作った感じがあります． というのも，今回私が作ったものよりも更に高機能で便利なものが既に存在するからです． しかし実際に作ってみると以下のような問題があり，勉強になりました． Golangプログラミングではほぼ必ず 標準パッケージ を利用する 先述したように多くの機能が含まれている これらを依存グラフに含むと，真っ黒で何もわからなくなってしまう 多くのパッケージが同じパッケージを参照する，みたいなこともよくある 愚直に再帰で実装 すると，同じパッケージに対し何回も探索を実行してしまう ちゃんと 枝刈り することが求められる 実際にこのツールを用いて， goanalyzer パッケージ自体を解析してみた結果を示します． ただノードの関係のみマッピングしている状態なのであんまりきれいではないんですが， graphvizの機能を使ってノードを区別したり色つけたり，サブグラフ使えばもうちょっときれいになりそうです． 先程列挙した既存ツールでは “どの関数に依存しているか” もわかるので便利なんですが， この簡易な依存グラフを見るだけでもいろいろなことがわかります． 例えば，非推奨のパッケージを使っていることが視覚的にわかるなど． 1.2.2 スコープチェッカー宣言されている識別子についてルールを適用し， 背いていれば検出対象とする，みたいなチェッカです． これについては具体例を見るほうがわかりやすいと思います． package a import \"fmt\" func f() { x := f2() // want \"This identifier is only referenced in a scope so should move the declaration to it\" if true { y := f2() // OK fmt.Println(x, y) } else { fmt.Println(0) } } func f2() int { f() return 30 } このように， 識別子が定義されているスコープと参照されているスコープに親子関係がある 識別子を参照するスコープの集合に 定義スコープ自体が含まれない ような構造を見つけて検出します． これはあくまでも個人の意見ですが， Golangに限らず “一般に\"スコープは狭ければ狭いほうがいい と思っているので，作ってみました． go func() { ... }() として即時関数を定義するケース if x := f(); condition {} のようなケース ネストしたブロックのケース でも一応正しく動作しています． 関数内の識別子だけでなく，トップレベル( unexportedなパッケージ変数)でも動作します． package a import \"fmt\" var ( strGV1 string = \"A\" // want \"NG\" ) const ( strGC1 string = \"A\" // want \"NG\" ) type strGT1 string // want \"NG\" type strGT2 = string // want \"NG\" func A() { fmt.Println(strGV1) fmt.Println(strGC1) var a strGT1 = \"A\" var b strGT2 = \"A\" fmt.Println(a, b) } func B() { fmt.Println(\"B\") } func C() { A() } Goでは， type-alias( type Name T ) named-type( type Name = T ) var const という種類の宣言がトップレベルで可能になっています(関数も可能だけど今回は関係ないので省略)． しかし，これらは関数内でも同様に可能です． 関数内スコープと同じ概念で， 一つの関数からしか利用されない識別子はその中で定義すればいい というシンプルなルールを持って解析します． 注意したのは unexportedな識別子に制限する ということです． GolangでUpperCamelCaseで宣言された識別子はパッケージ内で使用されていないとしても， 外部パッケージから参照される可能性があるので，そのような実装にしています． 上の例の宣言を移動してもPlaygroundで正しく動きます． 時間内に実装しきれなかった機能としては，例えば次のようなものがあります． 機能というよりは痛いバグですね． package main import ( \"fmt\" ) func main() { x := 30 if true { fmt.Println(x) } else { fmt.Println(x) } } この場合の x はどちらのブロックでも用いられているため，宣言スコープが関数のトップレベルで正しいはずです． しかし現在の私のツールでは，これを\"宣言を移動するべき\"として検出してしまいます． 1.2.3 ユニットテストが定義されていない関数の検出この例を見ていただけるとわかりやすいのですが， f … 関数のテストが書かれていない F2 … 関数のテストが書かれている みたいなコードです． この f みたいな関数を検出するようなアナライザを実装しました． どうしてもテストが難しいような関数や，テストが必要な場合もあるかもしれないので， そういう場合には // @ignore みたいなDocCommentがついていたら検出しないようにもしたかったのですが， スコープチェッカの実装に大きな穴が見つかってそっちを直していたらインターンが終わってしまいました． ","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:2","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#開発"},{"categories":null,"content":" 1.2 開発私が今回作成したのは，次のようなツール群です． 本当はこれに加えて const 宣言すべきである識別子の検出 もやりたかったんですが，ちょっと実装速度が足りませんでした． やりたい人いらっしゃったら一緒にやりましょう!(投げやり) Golangでは var や id := を用いると標準でミュータブルですが， プリミティブな型かつ不変性を許容できる場合については const 宣言を使うことができます． これを検出するようなツールも作ろうと考えていました． (パッケージ変数，自動変数のどちらも検出したかった) リポジトリはこちら． 1.2.1 パッケージ間の依存関係可視化ツールGolangパッケージを指定すると，そのパッケージが依存するパッケージを探索し， DOT言語( グラフ構造の記述言語 )を生成する，というツールです． これはどちらかというと習作というか，勉強の為に作った感じがあります． というのも，今回私が作ったものよりも更に高機能で便利なものが既に存在するからです． しかし実際に作ってみると以下のような問題があり，勉強になりました． Golangプログラミングではほぼ必ず 標準パッケージ を利用する 先述したように多くの機能が含まれている これらを依存グラフに含むと，真っ黒で何もわからなくなってしまう 多くのパッケージが同じパッケージを参照する，みたいなこともよくある 愚直に再帰で実装 すると，同じパッケージに対し何回も探索を実行してしまう ちゃんと 枝刈り することが求められる 実際にこのツールを用いて， goanalyzer パッケージ自体を解析してみた結果を示します． ただノードの関係のみマッピングしている状態なのであんまりきれいではないんですが， graphvizの機能を使ってノードを区別したり色つけたり，サブグラフ使えばもうちょっときれいになりそうです． 先程列挙した既存ツールでは “どの関数に依存しているか” もわかるので便利なんですが， この簡易な依存グラフを見るだけでもいろいろなことがわかります． 例えば，非推奨のパッケージを使っていることが視覚的にわかるなど． 1.2.2 スコープチェッカー宣言されている識別子についてルールを適用し， 背いていれば検出対象とする，みたいなチェッカです． これについては具体例を見るほうがわかりやすいと思います． package a import \"fmt\" func f() { x := f2() // want \"This identifier is only referenced in a scope so should move the declaration to it\" if true { y := f2() // OK fmt.Println(x, y) } else { fmt.Println(0) } } func f2() int { f() return 30 } このように， 識別子が定義されているスコープと参照されているスコープに親子関係がある 識別子を参照するスコープの集合に 定義スコープ自体が含まれない ような構造を見つけて検出します． これはあくまでも個人の意見ですが， Golangに限らず “一般に\"スコープは狭ければ狭いほうがいい と思っているので，作ってみました． go func() { ... }() として即時関数を定義するケース if x := f(); condition {} のようなケース ネストしたブロックのケース でも一応正しく動作しています． 関数内の識別子だけでなく，トップレベル( unexportedなパッケージ変数)でも動作します． package a import \"fmt\" var ( strGV1 string = \"A\" // want \"NG\" ) const ( strGC1 string = \"A\" // want \"NG\" ) type strGT1 string // want \"NG\" type strGT2 = string // want \"NG\" func A() { fmt.Println(strGV1) fmt.Println(strGC1) var a strGT1 = \"A\" var b strGT2 = \"A\" fmt.Println(a, b) } func B() { fmt.Println(\"B\") } func C() { A() } Goでは， type-alias( type Name T ) named-type( type Name = T ) var const という種類の宣言がトップレベルで可能になっています(関数も可能だけど今回は関係ないので省略)． しかし，これらは関数内でも同様に可能です． 関数内スコープと同じ概念で， 一つの関数からしか利用されない識別子はその中で定義すればいい というシンプルなルールを持って解析します． 注意したのは unexportedな識別子に制限する ということです． GolangでUpperCamelCaseで宣言された識別子はパッケージ内で使用されていないとしても， 外部パッケージから参照される可能性があるので，そのような実装にしています． 上の例の宣言を移動してもPlaygroundで正しく動きます． 時間内に実装しきれなかった機能としては，例えば次のようなものがあります． 機能というよりは痛いバグですね． package main import ( \"fmt\" ) func main() { x := 30 if true { fmt.Println(x) } else { fmt.Println(x) } } この場合の x はどちらのブロックでも用いられているため，宣言スコープが関数のトップレベルで正しいはずです． しかし現在の私のツールでは，これを\"宣言を移動するべき\"として検出してしまいます． 1.2.3 ユニットテストが定義されていない関数の検出この例を見ていただけるとわかりやすいのですが， f … 関数のテストが書かれていない F2 … 関数のテストが書かれている みたいなコードです． この f みたいな関数を検出するようなアナライザを実装しました． どうしてもテストが難しいような関数や，テストが必要な場合もあるかもしれないので， そういう場合には // @ignore みたいなDocCommentがついていたら検出しないようにもしたかったのですが， スコープチェッカの実装に大きな穴が見つかってそっちを直していたらインターンが終わってしまいました． ","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:2","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#パッケージ間の依存関係可視化ツール"},{"categories":null,"content":" 1.2 開発私が今回作成したのは，次のようなツール群です． 本当はこれに加えて const 宣言すべきである識別子の検出 もやりたかったんですが，ちょっと実装速度が足りませんでした． やりたい人いらっしゃったら一緒にやりましょう!(投げやり) Golangでは var や id := を用いると標準でミュータブルですが， プリミティブな型かつ不変性を許容できる場合については const 宣言を使うことができます． これを検出するようなツールも作ろうと考えていました． (パッケージ変数，自動変数のどちらも検出したかった) リポジトリはこちら． 1.2.1 パッケージ間の依存関係可視化ツールGolangパッケージを指定すると，そのパッケージが依存するパッケージを探索し， DOT言語( グラフ構造の記述言語 )を生成する，というツールです． これはどちらかというと習作というか，勉強の為に作った感じがあります． というのも，今回私が作ったものよりも更に高機能で便利なものが既に存在するからです． しかし実際に作ってみると以下のような問題があり，勉強になりました． Golangプログラミングではほぼ必ず 標準パッケージ を利用する 先述したように多くの機能が含まれている これらを依存グラフに含むと，真っ黒で何もわからなくなってしまう 多くのパッケージが同じパッケージを参照する，みたいなこともよくある 愚直に再帰で実装 すると，同じパッケージに対し何回も探索を実行してしまう ちゃんと 枝刈り することが求められる 実際にこのツールを用いて， goanalyzer パッケージ自体を解析してみた結果を示します． ただノードの関係のみマッピングしている状態なのであんまりきれいではないんですが， graphvizの機能を使ってノードを区別したり色つけたり，サブグラフ使えばもうちょっときれいになりそうです． 先程列挙した既存ツールでは “どの関数に依存しているか” もわかるので便利なんですが， この簡易な依存グラフを見るだけでもいろいろなことがわかります． 例えば，非推奨のパッケージを使っていることが視覚的にわかるなど． 1.2.2 スコープチェッカー宣言されている識別子についてルールを適用し， 背いていれば検出対象とする，みたいなチェッカです． これについては具体例を見るほうがわかりやすいと思います． package a import \"fmt\" func f() { x := f2() // want \"This identifier is only referenced in a scope so should move the declaration to it\" if true { y := f2() // OK fmt.Println(x, y) } else { fmt.Println(0) } } func f2() int { f() return 30 } このように， 識別子が定義されているスコープと参照されているスコープに親子関係がある 識別子を参照するスコープの集合に 定義スコープ自体が含まれない ような構造を見つけて検出します． これはあくまでも個人の意見ですが， Golangに限らず “一般に\"スコープは狭ければ狭いほうがいい と思っているので，作ってみました． go func() { ... }() として即時関数を定義するケース if x := f(); condition {} のようなケース ネストしたブロックのケース でも一応正しく動作しています． 関数内の識別子だけでなく，トップレベル( unexportedなパッケージ変数)でも動作します． package a import \"fmt\" var ( strGV1 string = \"A\" // want \"NG\" ) const ( strGC1 string = \"A\" // want \"NG\" ) type strGT1 string // want \"NG\" type strGT2 = string // want \"NG\" func A() { fmt.Println(strGV1) fmt.Println(strGC1) var a strGT1 = \"A\" var b strGT2 = \"A\" fmt.Println(a, b) } func B() { fmt.Println(\"B\") } func C() { A() } Goでは， type-alias( type Name T ) named-type( type Name = T ) var const という種類の宣言がトップレベルで可能になっています(関数も可能だけど今回は関係ないので省略)． しかし，これらは関数内でも同様に可能です． 関数内スコープと同じ概念で， 一つの関数からしか利用されない識別子はその中で定義すればいい というシンプルなルールを持って解析します． 注意したのは unexportedな識別子に制限する ということです． GolangでUpperCamelCaseで宣言された識別子はパッケージ内で使用されていないとしても， 外部パッケージから参照される可能性があるので，そのような実装にしています． 上の例の宣言を移動してもPlaygroundで正しく動きます． 時間内に実装しきれなかった機能としては，例えば次のようなものがあります． 機能というよりは痛いバグですね． package main import ( \"fmt\" ) func main() { x := 30 if true { fmt.Println(x) } else { fmt.Println(x) } } この場合の x はどちらのブロックでも用いられているため，宣言スコープが関数のトップレベルで正しいはずです． しかし現在の私のツールでは，これを\"宣言を移動するべき\"として検出してしまいます． 1.2.3 ユニットテストが定義されていない関数の検出この例を見ていただけるとわかりやすいのですが， f … 関数のテストが書かれていない F2 … 関数のテストが書かれている みたいなコードです． この f みたいな関数を検出するようなアナライザを実装しました． どうしてもテストが難しいような関数や，テストが必要な場合もあるかもしれないので， そういう場合には // @ignore みたいなDocCommentがついていたら検出しないようにもしたかったのですが， スコープチェッカの実装に大きな穴が見つかってそっちを直していたらインターンが終わってしまいました． ","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:2","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#スコープチェッカー"},{"categories":null,"content":" 1.2 開発私が今回作成したのは，次のようなツール群です． 本当はこれに加えて const 宣言すべきである識別子の検出 もやりたかったんですが，ちょっと実装速度が足りませんでした． やりたい人いらっしゃったら一緒にやりましょう!(投げやり) Golangでは var や id := を用いると標準でミュータブルですが， プリミティブな型かつ不変性を許容できる場合については const 宣言を使うことができます． これを検出するようなツールも作ろうと考えていました． (パッケージ変数，自動変数のどちらも検出したかった) リポジトリはこちら． 1.2.1 パッケージ間の依存関係可視化ツールGolangパッケージを指定すると，そのパッケージが依存するパッケージを探索し， DOT言語( グラフ構造の記述言語 )を生成する，というツールです． これはどちらかというと習作というか，勉強の為に作った感じがあります． というのも，今回私が作ったものよりも更に高機能で便利なものが既に存在するからです． しかし実際に作ってみると以下のような問題があり，勉強になりました． Golangプログラミングではほぼ必ず 標準パッケージ を利用する 先述したように多くの機能が含まれている これらを依存グラフに含むと，真っ黒で何もわからなくなってしまう 多くのパッケージが同じパッケージを参照する，みたいなこともよくある 愚直に再帰で実装 すると，同じパッケージに対し何回も探索を実行してしまう ちゃんと 枝刈り することが求められる 実際にこのツールを用いて， goanalyzer パッケージ自体を解析してみた結果を示します． ただノードの関係のみマッピングしている状態なのであんまりきれいではないんですが， graphvizの機能を使ってノードを区別したり色つけたり，サブグラフ使えばもうちょっときれいになりそうです． 先程列挙した既存ツールでは “どの関数に依存しているか” もわかるので便利なんですが， この簡易な依存グラフを見るだけでもいろいろなことがわかります． 例えば，非推奨のパッケージを使っていることが視覚的にわかるなど． 1.2.2 スコープチェッカー宣言されている識別子についてルールを適用し， 背いていれば検出対象とする，みたいなチェッカです． これについては具体例を見るほうがわかりやすいと思います． package a import \"fmt\" func f() { x := f2() // want \"This identifier is only referenced in a scope so should move the declaration to it\" if true { y := f2() // OK fmt.Println(x, y) } else { fmt.Println(0) } } func f2() int { f() return 30 } このように， 識別子が定義されているスコープと参照されているスコープに親子関係がある 識別子を参照するスコープの集合に 定義スコープ自体が含まれない ような構造を見つけて検出します． これはあくまでも個人の意見ですが， Golangに限らず “一般に\"スコープは狭ければ狭いほうがいい と思っているので，作ってみました． go func() { ... }() として即時関数を定義するケース if x := f(); condition {} のようなケース ネストしたブロックのケース でも一応正しく動作しています． 関数内の識別子だけでなく，トップレベル( unexportedなパッケージ変数)でも動作します． package a import \"fmt\" var ( strGV1 string = \"A\" // want \"NG\" ) const ( strGC1 string = \"A\" // want \"NG\" ) type strGT1 string // want \"NG\" type strGT2 = string // want \"NG\" func A() { fmt.Println(strGV1) fmt.Println(strGC1) var a strGT1 = \"A\" var b strGT2 = \"A\" fmt.Println(a, b) } func B() { fmt.Println(\"B\") } func C() { A() } Goでは， type-alias( type Name T ) named-type( type Name = T ) var const という種類の宣言がトップレベルで可能になっています(関数も可能だけど今回は関係ないので省略)． しかし，これらは関数内でも同様に可能です． 関数内スコープと同じ概念で， 一つの関数からしか利用されない識別子はその中で定義すればいい というシンプルなルールを持って解析します． 注意したのは unexportedな識別子に制限する ということです． GolangでUpperCamelCaseで宣言された識別子はパッケージ内で使用されていないとしても， 外部パッケージから参照される可能性があるので，そのような実装にしています． 上の例の宣言を移動してもPlaygroundで正しく動きます． 時間内に実装しきれなかった機能としては，例えば次のようなものがあります． 機能というよりは痛いバグですね． package main import ( \"fmt\" ) func main() { x := 30 if true { fmt.Println(x) } else { fmt.Println(x) } } この場合の x はどちらのブロックでも用いられているため，宣言スコープが関数のトップレベルで正しいはずです． しかし現在の私のツールでは，これを\"宣言を移動するべき\"として検出してしまいます． 1.2.3 ユニットテストが定義されていない関数の検出この例を見ていただけるとわかりやすいのですが， f … 関数のテストが書かれていない F2 … 関数のテストが書かれている みたいなコードです． この f みたいな関数を検出するようなアナライザを実装しました． どうしてもテストが難しいような関数や，テストが必要な場合もあるかもしれないので， そういう場合には // @ignore みたいなDocCommentがついていたら検出しないようにもしたかったのですが， スコープチェッカの実装に大きな穴が見つかってそっちを直していたらインターンが終わってしまいました． ","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:1:2","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#ユニットテストが定義されていない関数の検出"},{"categories":null,"content":" 2 まとめ今回このような勉強の機会を与えてくださった株式会社メルカリ様， また講義やコードレビュー等をして頂いたメンターの方々， そしてインターンに一緒に参加していた受講生の方々， 本当にありがとうございました! 上記以外にも，特筆すべき2つの点について述べておきます． ","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:2:0","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#まとめ"},{"categories":null,"content":" 2.1 静的解析はソフトウェア開発の生産性を向上させる選択肢の一つとなりうる当たり前だと思うかもしれませんが，個人的にはこれを再認識できたのは非常に大きいと思っています． 今まで静的解析ツールは 個人開発の時，IDEがバグを早期発見するために提供してくれる機能 ぐらいにしか意識していませんでした． しかし，こちらで話したように， 静的解析ツールを作ることでプロジェクト全体の開発スピードが円滑に，そしてスマートになる可能性を知ることができました． 私も普段のプログラミングでよくやってしまうミスなどはツール自作により技術的に解決したいなと，そう思うことができました． ","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:2:1","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#静的解析はソフトウェア開発の生産性を向上させる選択肢の一つとなりうる"},{"categories":null,"content":" 2.2 実装速度とロバスト性の大切さGolangに対する経験が少ないというのはありますが， 実装速度が非常に遅かったのと，またバグもしばしば生んでしまいました． 他のインターン生を見ると，皆まさに 爆速 で実装していて，それはもうゾクゾクしました． やはり周りが凄いと自分も頑張りたくなるもので，とても良い環境で勉強させていただきました． ","date":"2020-09-04","objectID":"/ja/posts/linter-internship/:2:2","series":null,"tags":["static-analyzer","go"],"title":"Online Summer Internship for Gophers 2020に参加しました","uri":"/ja/posts/linter-internship/#実装速度とロバスト性の大切さ"},{"categories":null,"content":"お久しぶりです． 最近は Peachili というコンパイラドライバの実装や， 今までやってこなかった大学数学の勉強， アルゴリズムやデータ構造を含むCSの基礎知識を1から勉強し直しているという感じで， あまり記事を書けていませんでした． Goコンパイラを読む記事シリーズは何処へ とても短い+ネタ記事になってしまいますが， 最近 計算理論 や 正規言語 らへんの勉強もやり始めたので， それに関連した知識としてオートマトンを取り上げ， 競プロの問題をオートマトンを用いて解いてみようと思います． 私はコンパイラ自作のとき毎回手書きでTokenizerを書いているのですが， 自分で作った字句解析器生成系を使って簡略化したいなあと思っているので， 正規言語･表現周辺の知識を身につけるモチベが上がってきているのです． Rust言語で字句解析器生成系を実装し， クレートとして公開しようと思っているので，その時は利用していただけるとありがたいです． 本記事の内容としては， こちらの本を参考にしています． 再度述べておきますが， この記事はあくまでもネタ記事であり， 有限オートマトンを含む計算理論を体系的に解説する記事ではありません． ｢オートマトンを使って遊んだんだな｣ くらいの認識で見ていただければ． ","date":"2020-07-21","objectID":"/ja/posts/abc-with-automaton/:0:0","series":null,"tags":["programming-competition","joke","rust"],"title":"Rustで有限オートマトンを書いてAtcoder Beginner ContestのA問題を解いてみる","uri":"/ja/posts/abc-with-automaton/#"},{"categories":null,"content":" 1 前提知識まずは， 有限オートマトンを実装する上で必要な前提知識をサラッと復習します． 私は最近計算理論の勉強をし始めたばかりで，わかりやすい解説にはならないと思うので詳細は省略します． しっかりと勉強したい人は記事先頭で紹介した書籍を読んでいただければと思います． まずは以下の図を見てください． 開始状態はsrcのない矢印で明記するのが一般的ですが， ここではわかりやすさを重視するため，s という状態を別途用意しています． 後に，有限オートマトンの正式な定義で下図を記述しますが， その際には s について取り扱わないので，注意してください． これは，二進記数法で表現された数が奇数であるか チェックするオートマトンの状態遷移図になります． ここで，有限オートマトンの定義を以下に示します． 有限オートマトン(finite automaton) は計算モデルの一種であり, 一般に以下の要素を持つ． - Q は状態(state)の集合 - Σ はアルファベット(alphabet)と呼ばれる集合 - δ: Q × Σ -\u003e Q は遷移関数(transition function) - q_0 ∈Q は開始状態(start state) - F ⊆Q は受理状態の集合( set of accept states) 上記定義を持って，先程の有限オートマトンを表現してみます． M = (Q, Σ, δ, q_0, F)\rQ = {q_0, q_1}\rΣ = {0, 1}\rq_0 は開始状態\rF = {q_1} δについてですが，これは 状態遷移表 で表現してみます． 状態遷移表は 集合Σの要素数 × 集合Qの要素数 で表現することができます． 初期状態 0 1 q_0 q_0 q_1 q_1 q_0 q_1 ","date":"2020-07-21","objectID":"/ja/posts/abc-with-automaton/:1:0","series":null,"tags":["programming-competition","joke","rust"],"title":"Rustで有限オートマトンを書いてAtcoder Beginner ContestのA問題を解いてみる","uri":"/ja/posts/abc-with-automaton/#前提知識"},{"categories":null,"content":" 2 本題ここからは， ABC158-A という問題を解いてみます． 問題の解説は省きますので，気になる方は上記リンクを参照してください． ","date":"2020-07-21","objectID":"/ja/posts/abc-with-automaton/:2:0","series":null,"tags":["programming-competition","joke","rust"],"title":"Rustで有限オートマトンを書いてAtcoder Beginner ContestのA問題を解いてみる","uri":"/ja/posts/abc-with-automaton/#本題"},{"categories":null,"content":" 3 素朴な回答まずはオートマトン云々の前に，普通に解いてみます． 愚直に解くと，次のようになりますね． 実装にはRustを用いますが， 適宜解説コメントを加えているので他言語を勉強している人にも読めるはずです． // Atcoderで用いることのできる，便利な入出力クレートのインポート use proconio::input; fn solve(s: \u0026str) -\u003e \u0026'static str { if s == \"AAA\" || s == \"BBB\" { return \"No\"; } \"Yes\" } fn main() { // 適当に期待する型を列挙しておけば，入力文字列をパースしてくれる input! { s: String, } println!(\"{}\", solve(\u0026s)); } なんてことないコードだと思います． ","date":"2020-07-21","objectID":"/ja/posts/abc-with-automaton/:3:0","series":null,"tags":["programming-competition","joke","rust"],"title":"Rustで有限オートマトンを書いてAtcoder Beginner ContestのA問題を解いてみる","uri":"/ja/posts/abc-with-automaton/#素朴な回答"},{"categories":null,"content":" 4 オートマトン解法ではこれを，オートマトンを用いて解いてみます． 今回は入力文字列がとても小さいので， 状態遷移図を考えるのが簡単です． よって，まずは状態遷移図を作ってみます． ABC158-Aは 判定問題 なので， Yes を出力すべき文字列を受理するオートマトンを考えればいい，ということになります． あとは題意に従って作っていくだけです． M = (Q, Σ, δ, q_0, F) Q = {q_0, q_1, q_2, q_3} Σ = {'A', 'B'} δは下記に記載 q_0 は開始状態 F = {q_3} ‘A’ ‘B’ q_0 q_1 q_2 q_1 q_1 q_3 q_2 q_3 q_2 q_3 q_3 q_3 あとはこれを実装していけばいい， という感じになります． ","date":"2020-07-21","objectID":"/ja/posts/abc-with-automaton/:4:0","series":null,"tags":["programming-competition","joke","rust"],"title":"Rustで有限オートマトンを書いてAtcoder Beginner ContestのA問題を解いてみる","uri":"/ja/posts/abc-with-automaton/#オートマトン解法"},{"categories":null,"content":" 4.1 Rustにおけるオートマトン実装 状態 受理状態 開始状態 遷移関数 をRustで実現する方法について考えます． まずは状態です． struct State { /// 受理状態かどうか is_accept: bool, /// State間を比較できるように，IDをつけておく id: usize, } 開始状態はオートマトンが知っていればいいので， State にメンバをもたせる必要はありません． 後述しますが，State はマップのキーに指定するため， 状態同士を比較できるように id を持たせています． 状態名を表すような name: String を持たせてもいいかもしれません 次に遷移関数ですが， 遷移関数とは オートマトン内の各状態に対応した関数 になります． 文字を受け取って比較し，各文字に対する遷移先の状態を返す1引数関数だと仮定すると， 各状態とその遷移関数は 1対1 であることがわかります． よって， HashMap\u003cState, Box\u003cdyn Fn(char) -\u003e String\u003e\u003e のようなものを作り，状態遷移表を実現します． Box\u003cFn()\u003e としているのは，関数オブジェクトをヒープに割り当てることで Sized を充足するためです． 遷移関数の返り値が String となっている理由ですが， 遷移は 状態と状態の関係 であり，状態が持つ情報ではないと考える Fn(char) -\u003e State とすると， State が遷移関数を持つことになり，非直感的である というのが理由です． ","date":"2020-07-21","objectID":"/ja/posts/abc-with-automaton/:4:1","series":null,"tags":["programming-competition","joke","rust"],"title":"Rustで有限オートマトンを書いてAtcoder Beginner ContestのA問題を解いてみる","uri":"/ja/posts/abc-with-automaton/#rustにおけるオートマトン実装"},{"categories":null,"content":" 4.2 サンプルコードここまでの情報をまとめつつ実装すると，以下のようになります． 実際に提出して正解することを確認しています． use proconio::input; use std::collections::HashMap; /// 状態 #[derive(Hash, Eq, PartialEq, Ord, PartialOrd, Clone, Copy)] struct State { /// 受理状態かどうか is_accept: bool, /// State間を比較できるように，IDをつけておく id: usize, } /// 有限オートマトンを表す構造体 struct Automaton { /// 状態の有限集合Q states: HashMap\u003cString, State\u003e, /// 遷移関数群 transitions: HashMap\u003cState, Box\u003cdyn Fn(char) -\u003e String\u003e\u003e, /// Stateにつける通し番号 id: usize, } impl Automaton { /// ある状態qと, qからの遷移関数を登録する fn add_states(\u0026mut self, name: \u0026str, is_accept: bool, transition: fn(char) -\u003e String) { let state = State { is_accept, id: self.id, }; self.id += 1; self.states.insert(name.to_string(), state); self.transitions.insert(state, Box::new(transition)); } /// 言語Lで記述された文字列sを受理するか判定 fn run(\u0026self, s: String) -\u003e bool { // 開始状態からスタート let mut current_state = *self.states.get(\"q0\").unwrap(); // 文字列をイテレートして，状態遷移を次々と行う for c in s.chars() { let transition = self.transitions.get(\u0026current_state).unwrap(); let next_state_name = transition(c); current_state = *self.states.get(\u0026next_state_name).unwrap(); } // 入力文字列を処理し終わったあと，最終的な遷移先が集合Fの元であるかチェック current_state.is_accept } } fn transition_q0(c: char) -\u003e String { if c == 'A' { return \"q1\".to_string(); } \"q2\".to_string() } fn transition_q1(c: char) -\u003e String { if c == 'B' { return \"q3\".to_string(); } \"q1\".to_string() } fn transition_q2(c: char) -\u003e String { if c == 'A' { return \"q3\".to_string(); } \"q2\".to_string() } fn transition_q3(_c: char) -\u003e String { \"q3\".to_string() } fn main() { input! { s: String, } let mut m = Automaton { states: HashMap::new(), transitions: HashMap::new(), id: 0, }; m.add_states(\"q0\", false, transition_q0); m.add_states(\"q1\", false, transition_q1); m.add_states(\"q2\", false, transition_q2); m.add_states(\"q3\", true, transition_q3); if m.run(s) { println!(\"Yes\"); } else { println!(\"No\"); } } ","date":"2020-07-21","objectID":"/ja/posts/abc-with-automaton/:4:2","series":null,"tags":["programming-competition","joke","rust"],"title":"Rustで有限オートマトンを書いてAtcoder Beginner ContestのA問題を解いてみる","uri":"/ja/posts/abc-with-automaton/#サンプルコード"},{"categories":null,"content":" 5 おまけ本問題は文字列が3文字であることが決定していたので 素朴な回答 が使えましたが， 文字列長が可変長である場合はそうもいきません． 文字列をsetに格納して， len(set) == 1 かどうかでチェックができそうです． 文字列を一度ループする必要があるので O(len(S)) ですね． s = set() for c in input(): s.add(c) print('No' if len(s) == 1 else 'Yes') ","date":"2020-07-21","objectID":"/ja/posts/abc-with-automaton/:5:0","series":null,"tags":["programming-competition","joke","rust"],"title":"Rustで有限オートマトンを書いてAtcoder Beginner ContestのA問題を解いてみる","uri":"/ja/posts/abc-with-automaton/#おまけ"},{"categories":null,"content":" 6 まとめ今回は有限オートマトンを使って簡単に遊んでみました． Rustを用いるとスッキリ実装できて，楽しかったです． このままレキサージェネレータの実装もスパッとできればいいなあ． ","date":"2020-07-21","objectID":"/ja/posts/abc-with-automaton/:6:0","series":null,"tags":["programming-competition","joke","rust"],"title":"Rustで有限オートマトンを書いてAtcoder Beginner ContestのA問題を解いてみる","uri":"/ja/posts/abc-with-automaton/#まとめ"},{"categories":null,"content":"以前,このような記事を書きました. 実はこのDepthの開発活動はSecHack365という 人材育成プロジェクト に参加した中で作成した成果物であり, またその成果物のうちごく一部を簡単に取り上げたもの,になります. 今回はSecHack365という活動がどういうものかをシェアしていけたらな,と思っています. 本記事は以下のようにして構成されています. ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:0:0","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#"},{"categories":null,"content":" 1 SecHack365の活動を振り返る","date":"2020-02-08","objectID":"/ja/posts/sechack365/:1:0","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#sechack365の活動を振り返る"},{"categories":null,"content":" 1.1 Sechack365とはSecHack365について簡単に述べるとするならば, NICTが主催する通年の人材育成プロジェクト です. SecHack365の理念･概要については上記公式サイトを参照していただくとして, まずは私が参加した理由･参加時点のモチベーションからお話したいと思います. ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:1:1","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#sechack365とは"},{"categories":null,"content":" 1.2 参加の動機私は学内の IPFactory というサークルに所属しているのですが, その サークルの先輩 が実際に昨年度(SecHack365'18)参加していました. 先輩から色々お話を伺ったり,公式サイトを色々と調べているうちに トレーナーの方にフィード・バックを頂きながら開発できる システムプログラミングについて詳しいトレーナーの方がいることを期待した 1年間の開発の中で, 沢山の能動的インプット ができるのではないかと考えた 自分にとってハードルの高い課題 を設定し, 知らないことをガンガン勉強できる のではと思った というモチベーションが湧いてきて,応募することを決意しました. ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:1:2","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#参加の動機"},{"categories":null,"content":" 1.3 応募時点でやりたかったこと私がやりたかったことは 実行バイナリ生成までをサポートするような\"コンパイラドライバ\"のスクラッチ実装 です. 具体的には, 自作言語 -\u003e x64 assemblyのコンパイラ 自作言語であることにこだわりはあまりなくて,別に既存言語の処理系実装でも良かった x64 assembly -\u003e ELF relocatable object file の変換を担うアセンブラ シンボル解決などを行い単一の ELF executable file を生成するリンカ が含まれます. これは応募時点での目標であり, 最終的にはユーザスペースのローダやLLVMパス等多くの機能を追加しました. ここで述べておきたいのが, 応募時点では コンパイラを作ったことなどほぼなかった ということです. 厳密には RuiさんのCコンパイラブック を少し読んだことがある程度でした. なので, 漠然と コンパイラって面白そうだよね というお気持ちで応募した感じになります. この約一年間を通じて コンパイラが大好きな人 になることが出来ました.やったね. ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:1:3","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#応募時点でやりたかったこと"},{"categories":null,"content":" 1.4 応募したコース上記公式サイトを見ていただければわかるのですが, SecHack365は今年度, 5つのコース に分かれており, 自分が応募したいと思うコースに応募する,というようになっていました. 私は以下の理由から 学習駆動コース の 坂井ゼミ を志望しました. 応募時点で作りたいものが明確に定まっていたが, 開発に対する具体的なイメージが湧いていなかった なので,最初は技術書などでインプットなどを行って行きたかった 成果物以外の勉強もガンガンやっていきたかった 具体的にはLinuxOSのカーネルについての勉強やx64アーキテクチャの勉強など コンパイラ開発に役立つかどうかは別 学習駆動コースの 付加学習 という考え方にとても共感した あの 坂井さん に一番質問しやすいポジションであった システムプログラミングをテーマに開発したい私にとってこれは大事 ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:1:4","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#応募したコース"},{"categories":null,"content":" 1.5 最終的に達成したこと基本的にはこちらのスライドを見ていただければと思います. 自作言語からx64アセンブリに変換するようなコンパイラを実装した LLVM IRのパスも存在し,殆ど全ての言語機能をコンパイル出来る 約5k行,(コンパイラ開発の本質に関わるような)ライブラリ未使用 x64アセンブリ(Intel記法)から再配置可能オブジェクトファイルを生成するアセンブラ 約1.2k行 単一ファイルのスタティックリンカを実装 (ELFバイナリを触るAPIを隔離し,リンクの動作のみを抽出すると)120行程度 大体以下のようなことをやる シンボルテーブル上のエントリに決め打ち+累計オフセットを割り当てる 再配置テーブルを操作し, bind 値を見てシンボルテーブルからアドレスを取得,割当 実際に r_offset から始まる機械語を上書き 単一のセグメント/プログラムヘッダを構築 セクションのアライメントとかを調整 ユーザ空間のローダを実装 とてもシンプルかつ超ミニマム実装 checksecを実装 ELFバイナリのフラグを見て,セキュリティ機構が有効化された状態でコンパイルされているか見る Depthコンパイラのバイナリを見るのではなく, gcc や clang によって生成されたバイナリが対象 readelf 出力を出来るだけ見やすくするように意識 かなり大きなバイナリを食べさせても正しく動く(ニッチな属性値の出力はサポートしていないかも) ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:1:5","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#最終的に達成したこと"},{"categories":null,"content":" 2 各集合回のまとめSecHack365では, 集合回 \u003cオンライン期間\u003e 集合回 \u003cオンライン期間\u003e ... というように, 年間のうち何度か集合回が開催され,間はオンラインで開発するスケジューリングになっています. 集合回…トレーナー/トレーニー同士でアウトプットし合ったりコミュニケーションを取り合う場 定期的にアウトプットを 強制 されるので,独学だと勉強スピードが遅くなりやすい人にとってはありがたい 私は一人で勉強し続ける場合でも比較的一定のペースを維持できるので,これは心配していなかった 開発方針を大きく改善できるきっかけが生まれたりするので,ここでのコミュニケーションは非常に大事 オンライン期間…トレーナーの方と連絡を取りつつ,各々が開発する時間 集合回で得たアイデア/アドバイスを元に,やりまくるという感じ この期間にどれだけ本気を出したかで, 集合回の密度に大きな差が生まれる という事で,私はオンライン期間の開発にかなり注力していました. 以下,時系列に沿って各集合回周辺の進捗やモチベーションを紹介していきます. ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:2:0","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#各集合回のまとめ"},{"categories":null,"content":" 2.1 応募時点~神奈川回採択されたのが5/2なのですが, 採択された時点で既に開発をやり始めていました. 最初は先程紹介した RuiさんのCコンパイラブック やgccの出力を参考にしながら, Golang で実装していました. Depthプロジェクトは何度かソースコードを全削除する,みたいなことをやっています. 現在は全削除を2回程度+構成要素ごとに何度か削除を行ったものが存在します. Golang実装はそのうち最も初期のものになります. その時のツイートを見ると懐かしくなりますね. このときはどのようにアセンブリ言語がアセンブルされるのかがよくわかっていなかったので, 頑張って自作コンパイラが吐いたコードを機械語に変換し, それを 標準出力に文字列で 出して確認していました. このときはシステムプログラミングそのものもよく理解していなかったので, 機械語列がどうやって実行できるのか,OSがどうやってメモリに展開しているのか もわかっていませんでした. 自作言語に自動変数を実装したときのスクショです. コンパイラ自作自体が初だったのですが,初めて実装できたので凄い嬉しかったです. 第一回目の集合回である神奈川では,SecHack365自体のガイダンスや顔合わせ等を行いました. かくいう私はこの集合回でも 自分の実装ばかり やっていましたね. もっとコミュニケーションを取っていればよかったと,反省しています. ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:2:1","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#応募時点神奈川回"},{"categories":null,"content":" 2.2 北海道回まで北海道回までは Golang実装を全てRustに置き換える みたいなことをやっていました. 理由としては以下の通りで, Rustをやってみたかった Version1の実装が単純に良くなかった 自動変数のスタックアロケートをパース時にやっていたり 等ですね. 結果,神奈川回でのGolang実装が実現していた言語機能とほぼ同等のものがRustで実装できました. ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:2:2","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#北海道回まで"},{"categories":null,"content":" 2.3 福岡回までここでは, Rust実装(Version2)も\"全削除\"しました . なんでそんな事をやったのかについてなんですが, こちらの記事に示した, セキュリティ・キャンプ の経験が大きく関係しています. 私はseccamp2019でCコンパイラゼミに参加したのですが, ここで講師の方から インクリメンタルに開発する事の重要性 を学びました. 私が身につけたプログラミングに関する技術で最も重要なものだったと思います. 小さい規模からコードを書き始め,必要になったらときに初めて追加する. 例えばCコンパイラなら最初から字句解析器を書きまくるようなことをしないで, 加算がコンパイルできる最も小さいコード みたいなのをできるようにする. 少しずつ,少しずつ規模を大きくしていくような感じです. それ以降のプログラミングにおいても,seccamp2019での経験は生きていると思います. Hikaliumさん,Ruiさん,本当にありがとうございました. ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:2:3","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#福岡回まで"},{"categories":null,"content":" 2.4 宮城回までこの間で, 実際に 実行プログラムを生成する ことが出来ました. これが9/21時点の話になります. SecHack365に採択されてから4ヶ月とちょっとぐらいでしょうか. 最初は 人のコンパイラのコードを真似する ことしか出来なかった私が, 4ヶ月間コンパイラについて調べ,実践し,失敗しては削除しを繰り返す事で なんとかgcc等の既存ツールに頼らない実行バイナリ生成を達成できました. アセンブラ実装中,機械語とオブジェクトファイルの概形を生成できるようにした様子です. まだ.textセクションしか存在しないので,GNU ldに通したりすることは出来ません. 実際に自作コンパイラ,自作アセンブラが協調動作して, GNU ldがリンク可能な(実行バイナリ生成に必要な最小構成の)オブジェクトファイルを生成できた様子です. x64の機械語/ELFについてそれまで身につけてきた知識が報われた気がして, とても嬉しくなりました. ここからはリンカ実装の為に, バイナリに埋め込む情報を追加したりしています. 9/21,実行プログラムが生成出来た瞬間です. ここまででやっと SecHack365応募時の大きな目標の一つ が達成されたことになります. 再度コンパイラのバックエンド部を全削除して, IRを3番地コードで実装している様子です. それまでは独自の適当に考えたIRを用いていたんですが, ドラゴンブックを読みながら3番地コードに変換していたら意外とうまくいきました. CFGを構築して,DOT言語を吐き出しているようすです. DOT言語を吐き出す部分も勿論スクラッチでやっています. 結果的にこんぐらいの変換になりました. ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:2:4","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#宮城回まで"},{"categories":null,"content":" 2.5 愛媛回までここからは スクラッチな実行プログラム生成基盤を使ってなにか出来ないかな…? というのを考えていました. アイデアを色々考えていたので実装はあまり出来ていませんね…. やったこととしては, 実行プログラム生成基盤に LLVM IRを吐くパスを追加 していました. 自動変数のサポートです. 勿論IR生成にLLVMのRustバインディング等は用いていません. LLVMの構成要素を全て定義して,スクラッチでIRBuiderみたいなのを作っていました. 静的なサイズの配列を実装している様子です. これ,結構大変だったんですよねー. ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:2:5","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#愛媛回まで"},{"categories":null,"content":" 2.6 沖縄回まで主に以下のものを実装しました. GNU readelfとほぼ同等の機能を持つ解析ツールの実装 checksec( Canaryが存在するかとか等. 実行バイナリのセキュリティ) 独自セクション の実装 readelfっぽいものを作っている様子です. 自作のデバッグセクションみたいなのを実装して, 自作readelfを用いると 自作言語にある型･シンボル情報 を取得できているサンプルです. 独自セクションその2です. 関数定義時にコメントを付属すると, バイナリに自然言語のコメントを埋め込めて, 自作readelfを用いる事で解析できるようになっています. ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:2:6","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#沖縄回まで"},{"categories":null,"content":" 3 全体を通して私が個人的にSecHack365で学んだことは,以下の3つにまとめられるかな,と思います. システムプログラミングの考え方,重い実装に対するモチベーション 実際の知識(特にOSがバイナリを実行するまでの流れ,x64機械語,ELF) ｢凄い事をやる｣という漠然とした表現を実現する方法のいくつか ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:3:0","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#全体を通して"},{"categories":null,"content":" 4 付加学習･SecHack365以外の活動SecHack365に参加してきたこの一年間ですが, 実はSecHack365に費やして来た時間というのはそこまで多くないんですよね. 私は SecHack365の成果物以外にもいろいろな勉強をやって, それが結局SecHack365の為になることもあるかな,という期待から. ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:4:0","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#付加学習sechack365以外の活動"},{"categories":null,"content":" 4.1 自作OS個人的にちょっとずつ頑張っていこうと思っている自作OSです. これは愛媛回から沖縄回までの期間にやっていました. [https://twitter.com/drumato/status/1211575163499606021?s=20:embed] SDKを使わずにUEFIアプリケーションでブートローダを記述し, OSカーネル(raw binary)をメモリ上に載っけて制御を移しています. カーネルではシリアルポートの初期化とバイト列の送信を行っていて, QEMUの -serial stdio によって表示している,みたいな感じです. 川合さんの緑本を読んだことはあったんですがあれはLegacy BIOSを用いたブートだし, あれは写経していただけなので,自分で書いたことはないです. 具体的には何人かの実装を参考にしたんですが. 自作OSについてはまだまだ勉強不足なので,少しずつやっていきたいと思っています. ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:4:1","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#自作os"},{"categories":null,"content":" 4.2 c–Cコンパイラドライバの実装もはじめました. 1/1から作り始めたので,これも愛媛-沖縄間にやりはじめました. Depthプロジェクトにはバイナリ生成まで1ヶ月かかっていますが, その知識を生かして 10日程度で実行可能なバイナリを生成できた のが嬉しいですね. [https://twitter.com/drumato/status/1215876625947971585?s=20:embed] 野良のCプログラムを投げてもうまく動くぐらいに育てていきたいと思います. ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:4:2","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#c--"},{"categories":null,"content":" 5 まとめ今回SecHack365'19に参加する事で, コンパイラを作ったことがない状態から,実行プログラム基盤を作成できるようになった. 活動をトリガーに多くの能動的学習を行うことが出来た 等の恩恵が得られました. これからの活動をもっといいものにできるよう,頑張っていきたいと思います. ","date":"2020-02-08","objectID":"/ja/posts/sechack365/:5:0","series":null,"tags":["compiler","assembler","linker","rust"],"title":"SecHack365 3rdとして一年間活動しました","uri":"/ja/posts/sechack365/#まとめ"},{"categories":null,"content":"この記事は 言語実装 Advent Calendar 2019 の8日目です. 言語処理系の理論,自作言語の実装については既に他の方が記事を出してくださると思うので, 私は 実行可能なプログラム に変換する部分を主軸に置きながら自作言語のお話をさせていただければと思います. 実装自体はこちらに置いてあります. ","date":"2019-12-08","objectID":"/ja/posts/toolchain-in-rust/:0:0","series":null,"tags":["compiler","assembler","linker","rust"],"title":"実行プログラム作成基盤をスクラッチで書いた","uri":"/ja/posts/toolchain-in-rust/#"},{"categories":null,"content":" 1 自己紹介 screenName: Drumato よく使う言語: Rust/Go エディタ: Neovim 年齢: 19 職業: 学生 興味のあること: コンパイラ/アセンブラ/リンカ/OS/全ての低レイヤに関するなにか 技術の勉強を 2018年5月から始めました. 言語処理系に興味を持ったのは2019年の2月からですね. ","date":"2019-12-08","objectID":"/ja/posts/toolchain-in-rust/:1:0","series":null,"tags":["compiler","assembler","linker","rust"],"title":"実行プログラム作成基盤をスクラッチで書いた","uri":"/ja/posts/toolchain-in-rust/#自己紹介"},{"categories":null,"content":" 2 本題","date":"2019-12-08","objectID":"/ja/posts/toolchain-in-rust/:2:0","series":null,"tags":["compiler","assembler","linker","rust"],"title":"実行プログラム作成基盤をスクラッチで書いた","uri":"/ja/posts/toolchain-in-rust/#本題"},{"categories":null,"content":" 2.1 Motivation一般的に(自作言語をサポートする)言語処理系を実装しようと思った時,以下のような方法が考えられます. 自作言語 -\u003e LLVM IR のコンパイラを実装する この場合,吐き出したLLVM IR( *.ll )を clang 等に入力する必要がある lli 等,簡単に実行結果をチェックできるツールも 自作言語 -\u003e assembly のコンパイラを実装する 同様に吐かれたアセンブリコードを別のツールに入力することで初めて実行結果が得られる GNU binutils 等を用いる事が多い インタプリタ を実装する プログラムを生成する必要がないため,外部ツールを用いる必要が無くなる JIT機能を追加して実行速度を早める方法もある 最近の主流はタダのTree-walkedなインタプリタより VM型 かも? etc 特に最近よく見るのは一番上の方法ですね. LLVM を用いることでかなり高速なプログラムを生成することができるし, 各言語にLLVMバインディングが存在する為,それを用いることで比較的カジュアルに実装できます. しかし,私は フルスクラッチ( from scratch )病 にかかっています. なので, LLVMは凄い,実際に使ってもみたい でも 外部ライブラリはできる限り使いたくない LLVM自体も 超巨大なコンパイラ実装ライブラリ ということができる(?) フロントエンド,バックエンド 全て自分で作って こそ勉強になるんじゃないか! という事で, 完全にフルスクラッチな実行プログラム生成基盤(コンパイラドライバ) の実装にチャレンジすることにしました. 具体的なアーキテクチャ(これは初期,今年の5月頃に考えていたもので現在とは異なります)を以下に示します. ","date":"2019-12-08","objectID":"/ja/posts/toolchain-in-rust/:2:1","series":null,"tags":["compiler","assembler","linker","rust"],"title":"実行プログラム作成基盤をスクラッチで書いた","uri":"/ja/posts/toolchain-in-rust/#motivation"},{"categories":null,"content":" 2.2 motivationまとめつまりまとめると,実行可能プログラムを生成したくなった理由は以下の通りになります. コンパイラの勉強を始める -\u003e 外部ライブラリは使いたくない! 低レイヤーが好きだ -\u003e アセンブリ吐くところで終わりたくない! なのでアセンブラ作ってオブジェクトファイル吐いたりリンカ作ったりしました 自作言語が実行可能な機械語に変換できる -\u003e 単純にお気持ちが満たされる! ここまでで読者の皆さんと意識を共有できたと思うので, ここから先は実装の流れ,そして今できる事を話していきたいと思います. ","date":"2019-12-08","objectID":"/ja/posts/toolchain-in-rust/:2:2","series":null,"tags":["compiler","assembler","linker","rust"],"title":"実行プログラム作成基盤をスクラッチで書いた","uri":"/ja/posts/toolchain-in-rust/#motivationまとめ"},{"categories":null,"content":" 3 実装の歴史 時期 やったこと 2019年5月 Golangで実装開始 2019年5月下旬 Goで書いたコード全削除 \u0026 Rustで再実装始め 2019年8月18日 再度ソースコード全削除 2019年9月 x64アセンブラ実装に着手 2019年9月10日 GNU ldでリンク可能なオブジェクトファイルの生成 2019年9月下旬 スタティックリンカ実装に着手 2019年9月21日 実行可能バイナリの生成に成功 2019年9月26日 コンパイラバックエンド部全削除 2019年10月15日 コンパイラフロントエンド部全削除 こうしてみると コード削除しすぎだろ という感じがしますね. コード削除の主な理由は 単純にコードが汚い モジュールの分け方がうまくなかった,やり直したくなった IRを適当なものから3番地コードに変換 等の理由があります. 8/18にソースコードを全削除してから 9/26に(外部ツールを用いない)実行可能バイナリを生成するまで 約一ヶ月間 かかっています. この間はもう日常生活のうち自由時間を全てこの実装に費やしていましたね. 初めて自作バイナリが実行できた時の様子がこちらになります. 発狂していますが,私のTwitterは大体こんな感じです. 本記事の執筆時点( 2019/11/24 )では,以下のような規模のプログラムになっています. 全てRustで実装されています. 実はこのプロジェクトを書く過程でRustを勉強したので, Rust自体の勉強時間はほぼ0 なんですよね. コンパイルエラーが優しい言語 はこういう勉強方法ができるので好きです. ","date":"2019-12-08","objectID":"/ja/posts/toolchain-in-rust/:3:0","series":null,"tags":["compiler","assembler","linker","rust"],"title":"実行プログラム作成基盤をスクラッチで書いた","uri":"/ja/posts/toolchain-in-rust/#実装の歴史"},{"categories":null,"content":" 4 今できることコンパイラ,アセンブラ,リンカの各コンポーネントに分けて, それぞれができることを解説していきます. ","date":"2019-12-08","objectID":"/ja/posts/toolchain-in-rust/:4:0","series":null,"tags":["compiler","assembler","linker","rust"],"title":"実行プログラム作成基盤をスクラッチで書いた","uri":"/ja/posts/toolchain-in-rust/#今できること"},{"categories":null,"content":" 4.1 コンパイラ 式 二項演算 加算 減算 乗算 除算 剰余算 ==,!= \u003c,\u003c=,\u003e,\u003e= \u003c\u003c,\u003e\u003e 変数式 x 括弧 (1+2)*3 アドレス式 \u0026x 負の数 -x 逆参照 *x 関数呼び出し function(arg1,arg2) インデックス ary[10] メンバ式 Struct.member 配列リテラル [10,20,30+40] 構造体リテラル Name{foo:30,bar:50} 文 if if (condition) statement if-else if (condition) statement else statement return return expression assign mutable_ident = expression compound-statement { statement * n } 構造体定義 struct Name { foo : i64, bar : i64 } while的なやつ condloop(condition) statement let let ident_name : type_name = expression ミュータブルなやつ let mut ident_name : type_name = expression 引数定義 func add(x:i64, y:i64) ラベル :fin goto文 goto :fin type-alias type type_name = target_type 組み込み型 i64 … 符号付き64bit整数 Pointer\u003cT\u003e …スタック上に置かれるポインタ Array\u003cT,N\u003e … 固定長 ( N 個 ) の配列,これもスタック上 ユーザ定義型 … type-alias したものとか, 構造体とか その他 --dump-cfg … 制御フローグラフをDOT言語に変換して吐き出す --dump-tac … 3番地コードを出力 --emit-llvm … LLVM IRを吐くパス(鋭意製作中) 実際にデモをいくつか見せます. まず,これは自作言語で書いた 6重ポインタ (?) です. func main(){ let a : i64 = 30 let b : Pointer\u003ci64\u003e = \u0026a let c : Pointer\u003cPointer\u003ci64\u003e\u003e = \u0026b let d : Pointer\u003cPointer\u003cPointer\u003ci64\u003e\u003e\u003e = \u0026c let e : Pointer\u003cPointer\u003cPointer\u003cPointer\u003ci64\u003e\u003e\u003e\u003e = \u0026d let f : Pointer\u003cPointer\u003cPointer\u003cPointer\u003cPointer\u003ci64\u003e\u003e\u003e\u003e\u003e = \u0026e return *****f } めちゃくちゃ冗長なんですが, type-alias 実装しているのでまぁ良しとしましょう. $ depth sample.dep すると, a.out が吐き出されます. こんな感じです. 構造体のサンプルも見てみましょう. ついでに --dump-cfg も使ってみます. struct Depth { foo: i64 bar: i64 } func main() { let depth : Depth = Depth{ foo: 30, bar: 60 } let mut x : i64 = 0 if (depth.foo == 30){ x = depth.foo } else { x = depth.bar } return x } 吐かれた cfg.dot に対しGraphvizを用いて,画像に変換しています. メンバ名がいまのところスタックオフセットになってしまっているんですが, ちゃんと制御フローグラフは構築出来ています. ","date":"2019-12-08","objectID":"/ja/posts/toolchain-in-rust/:4:1","series":null,"tags":["compiler","assembler","linker","rust"],"title":"実行プログラム作成基盤をスクラッチで書いた","uri":"/ja/posts/toolchain-in-rust/#コンパイラ"},{"categories":null,"content":" 4.2 アセンブラ次にアセンブラのできる事を紹介するんですが, 言語処理系の勉強をしている人の中にはアセンブラがどんな役割を持つソフトウェアなのかしらない人も多いと思うので, ここでは簡単に解説しようと思います. アセンブラとはとてもシンプルに言えば, アセンブリ言語をオブジェクトファイルに変換するソフトウェア のことです. 一般的には アセンブリ言語を機械語に翻訳するソフトウェア と言われていて,それは至極正しいんですが, 実際に実装してみると アセンブラの本質ってオブジェクトファイルにまとめる部分では と感じる事が多かったです. コンパイラと同じように字句解析,構文解析をします. 但し,コンパイラの構文解析における AST のような複雑な構造よりはシンプルですけどね. そして命令列に対応する機械語を生成します. これによって 通常 .text に書き込む機械語列が得られるというわけです. 次に, リンカが必要とする情報 をバイナリに埋め込む作業に入ります. これはあくまで私の場合ですが, GNU ld がリンク可能な最小のオブジェクトファイルには シンボルテーブル .symtab ( リンカがmainシンボルを見つける為 ) つまりシンボル文字列テーブル .strtab も必要 .text (当たり前) .shstrtab (リンカはセクションの判断にきちんとこの名前を用いている) 勿論 セクションヘッダテーブル も必要 各ヘッダに適切な値を設定する必要もある. が必要でした. これらの情報を過不足なく,そして正確に設定することで初めてリンク可能なオブジェクトファイルとなります. 逆に言うと, リンカが実行プログラムを作成する上で 最低限必要な情報 ということです. それぞれの解説をここで加えているととてつもない分量になってしまうので, こちらで是非電子版を購入してください.私が3章を担当しています( 宣伝を入れるな ) 3章ではELFの基礎的な知識について解説しています. 気を取り直して,現在Depthアセンブラができることです. 生成セクション NULLセクション .text .strtab .symtab .rela.text .shstrtab relative offset jump(めちゃくちゃ難しかった) e9 c1 ff ff ff jmp 400031 \u003cmain+0x19\u003e みたいなやつ. アドレス変位の部分に現在の機械語からの相対オフセットが指定されている 対応命令 movzx ret pop push cqo add sub idiv imul cmp setle syscall setl setg setge sete setne call lea neg mov jmp sal sar jz コンパイラが吐くアセンブリコードには全て対応しています. メモリアドレッシング もできるし,システムコールの発行も出来ます. これもデモ画像を見てみましょう. うまくアセンブル出来ている事がわかります. ELFの仕様に則ったオブジェクトファイルを生成しているので, ちゃんとreadelfしてもエラーが起きず解析できます. ","date":"2019-12-08","objectID":"/ja/posts/toolchain-in-rust/:4:2","series":null,"tags":["compiler","assembler","linker","rust"],"title":"実行プログラム作成基盤をスクラッチで書いた","uri":"/ja/posts/toolchain-in-rust/#アセンブラ"},{"categories":null,"content":" 4.3 リンカリンカもアセンブラ同様,簡潔に解説します. アセンブラの出力によって オブジェクトファイル が得られますが, これは 再配置可能 な状態と言って, 実際にメモリ上のどこにロードされるか という情報を持っていません. そこで リンカ というソフトウェアが このシンボルはここにおいて この文字列はここにおいて, 読み取り専用にして ファイル上のここからここは プログラム実行に必要ないから要らないフラグをつけて みたいに, オブジェクトファイルを全解析 します. この作業によって初めて 実行可能なプログラム , つまり メモリにロードし,CPUが実行可能なプログラム に変化します. リンカがやることは沢山あるのですが,これについて説明しておくとこれまた一冊本が出来てしまうので, 実際に出来上がった本を購入していただければと思います. 日本語で解説されている 最も詳しい リンカ実装本です. 非常にニッチかつ内容も簡単ではないですが, システムプログラミングの基礎 と言ってもいいと思います. まだ読んでない人は絶対に読むべきです. 自作リンカは 約230行 で実装されているので,できる事はめちゃくちゃ少ないです. 静的リンク 動的リンクは出来ません. 単一オブジェクトファイル 複数ファイルには対応していません 再配置 再配置テーブルを見て,アドレス解決されていないシンボルを見つけます 再配置テーブルの情報から,参照されている機械語上のオフセットにアドレスを書き込みます とてもシンプル エントリポイントの指定 _start シンボルを見つけ,割り当てたアドレスを elf64_ehdr.e_entry に書き込みます これぐらいかな,できる事はとてもシンプルです. しかしこれだけですが, 現状Depth言語で記述した動くプログラムの全て をリンク可能です. つまり,アセンブラが吐いた全てのオブジェクトファイルは正常にリンクできます. これは, リンカというソフトウェアの特性 にも絡んできます. リンカの実装フェーズは,大まかに3つに分かれていると個人的に思っています. スタティックリンクできるようにする(今のDepthリンカ) 動的リンクにすると結構難しくなる ライブラリリンクできるようになる(ここにめちゃくちゃ大きな壁がある) ファイル間のシンボル解決,アーカイブフォーマットの解析,セクション位置の調整… 対応しなければならないセクション数も爆発する LTO( Link Time Optimization ) の実装 逆に言うと,細かな改良が 最低限の機能に限定すれば あまり必要ないということです. 1番が実装できているので,2番が必要になるまでは現在のリンカで対応できます. 上記コンポーネントを全て利用すると,以下のような事ができます. ","date":"2019-12-08","objectID":"/ja/posts/toolchain-in-rust/:4:3","series":null,"tags":["compiler","assembler","linker","rust"],"title":"実行プログラム作成基盤をスクラッチで書いた","uri":"/ja/posts/toolchain-in-rust/#リンカ"},{"categories":null,"content":" 4.4 追記:2019/12/4:ローダ普通シェルで ./binary 等とした時, ユーザプロセスから execve(2) システムコールが呼ばれ, 内部で様々な処理を実行した後カーネル組み込みのELFローダが起動, プログラムを実行するという流れになっています. 私は ここも自作したい という気持ちから, 更にコンパイラドライバ内にELFローダも実装しました. 実装全体はこちらに. 下ではロードのメイン処理を載せておきます. pub fn load(elf_file: ELF) -\u003e i32 { let (program, page) = Self::setup_page_with_using_mmap(); let binary = elf_file.to_vec(); if let Some(unwrapped_phdrs) = elf_file.phdrs { let offset = unwrapped_phdrs[0].p_offset as usize; let segment_size = unwrapped_phdrs[0].p_filesz as usize; /* get segment from binary */ let load_segment = binary[offset..offset + segment_size].to_vec(); let pointer_to_segment = load_segment.as_ptr(); unsafe { program.copy_from_nonoverlapping(pointer_to_segment, segment_size as usize); let f: fn() -\u003e i32 = ::std::mem::transmute(page); f() } } else { eprintln!(\"not found program header table\"); 0 } } 実行可能なメモリ領域を mmap(2) 関数ポインタにキャスト,実行 ローダ側でキャッチして,それをプロセスの終了ステータスとする みたいなことをやっています. リンカが吐き出す ET_EXEC ファイルのセグメント数が1つだからこそ出来る簡易実装になっています. 実際に動作している様子です. Linuxカーネル組み込みのローダを使わずに， (といってもDepthコンパイラドライバのロードには使っているんですが) add.dep の実行結果を受け取っていることがわかります. ","date":"2019-12-08","objectID":"/ja/posts/toolchain-in-rust/:4:4","series":null,"tags":["compiler","assembler","linker","rust"],"title":"実行プログラム作成基盤をスクラッチで書いた","uri":"/ja/posts/toolchain-in-rust/#追記2019124ローダ"},{"categories":null,"content":" 4.5 困ったことここからは実装時に困ったことをつらつら書いていきます. 4.5.1 ELFに蔓延るNULL三姉妹詳しくはこちらスライドを見ていただきたいのですが, 要は ELF独特の仕様に苦しめられた ということです. 自分で色々値を変えてみたりして検証する必要がありました. 4.5.2 そもそもやっている人がいない坂井さんの本はリンカ実装の上でとても役に立ったんですが, あれはあくまでも GNU binutils (GNU as)の吐いたオブジェクトファイルをリンクできるプログラムなんですよね. つまり 自作アセンブラ には対応していないんですよ( 当たり前 ). ということは, オブジェクトファイルの生成から間違っているかもしれない という可能性を捨てきれないまま, リンカを作ったりする必要があるわけですね… これは辛い,非常に大変だった. どうやら世の中で コンパイラ･アセンブラ･リンカ を作っている人はかなり少数みたいで, ドキュメントはほぼありませんでした. というか全く無い. 一番大変だったのは 自作バイナリのデバッグ ですね. リンカがある程度出来上がったときに実行すると BusError で落ちる事がよくあったんですね. これ,カーネルが仮想メモリに(アラインメントエラーとかで)うまくマッピング出来なかったりすると起こるんですが どこを調べても解決方法は載っていないので大変でした. gdbを使ってもデバッグ情報なんか無いですからね! 後実行してもELFヘッダを機械語列と見てしまっていたり,もう大変でした. ","date":"2019-12-08","objectID":"/ja/posts/toolchain-in-rust/:4:5","series":null,"tags":["compiler","assembler","linker","rust"],"title":"実行プログラム作成基盤をスクラッチで書いた","uri":"/ja/posts/toolchain-in-rust/#困ったこと"},{"categories":null,"content":" 4.5 困ったことここからは実装時に困ったことをつらつら書いていきます. 4.5.1 ELFに蔓延るNULL三姉妹詳しくはこちらスライドを見ていただきたいのですが, 要は ELF独特の仕様に苦しめられた ということです. 自分で色々値を変えてみたりして検証する必要がありました. 4.5.2 そもそもやっている人がいない坂井さんの本はリンカ実装の上でとても役に立ったんですが, あれはあくまでも GNU binutils (GNU as)の吐いたオブジェクトファイルをリンクできるプログラムなんですよね. つまり 自作アセンブラ には対応していないんですよ( 当たり前 ). ということは, オブジェクトファイルの生成から間違っているかもしれない という可能性を捨てきれないまま, リンカを作ったりする必要があるわけですね… これは辛い,非常に大変だった. どうやら世の中で コンパイラ･アセンブラ･リンカ を作っている人はかなり少数みたいで, ドキュメントはほぼありませんでした. というか全く無い. 一番大変だったのは 自作バイナリのデバッグ ですね. リンカがある程度出来上がったときに実行すると BusError で落ちる事がよくあったんですね. これ,カーネルが仮想メモリに(アラインメントエラーとかで)うまくマッピング出来なかったりすると起こるんですが どこを調べても解決方法は載っていないので大変でした. gdbを使ってもデバッグ情報なんか無いですからね! 後実行してもELFヘッダを機械語列と見てしまっていたり,もう大変でした. ","date":"2019-12-08","objectID":"/ja/posts/toolchain-in-rust/:4:5","series":null,"tags":["compiler","assembler","linker","rust"],"title":"実行プログラム作成基盤をスクラッチで書いた","uri":"/ja/posts/toolchain-in-rust/#elfに蔓延るnull三姉妹"},{"categories":null,"content":" 4.5 困ったことここからは実装時に困ったことをつらつら書いていきます. 4.5.1 ELFに蔓延るNULL三姉妹詳しくはこちらスライドを見ていただきたいのですが, 要は ELF独特の仕様に苦しめられた ということです. 自分で色々値を変えてみたりして検証する必要がありました. 4.5.2 そもそもやっている人がいない坂井さんの本はリンカ実装の上でとても役に立ったんですが, あれはあくまでも GNU binutils (GNU as)の吐いたオブジェクトファイルをリンクできるプログラムなんですよね. つまり 自作アセンブラ には対応していないんですよ( 当たり前 ). ということは, オブジェクトファイルの生成から間違っているかもしれない という可能性を捨てきれないまま, リンカを作ったりする必要があるわけですね… これは辛い,非常に大変だった. どうやら世の中で コンパイラ･アセンブラ･リンカ を作っている人はかなり少数みたいで, ドキュメントはほぼありませんでした. というか全く無い. 一番大変だったのは 自作バイナリのデバッグ ですね. リンカがある程度出来上がったときに実行すると BusError で落ちる事がよくあったんですね. これ,カーネルが仮想メモリに(アラインメントエラーとかで)うまくマッピング出来なかったりすると起こるんですが どこを調べても解決方法は載っていないので大変でした. gdbを使ってもデバッグ情報なんか無いですからね! 後実行してもELFヘッダを機械語列と見てしまっていたり,もう大変でした. ","date":"2019-12-08","objectID":"/ja/posts/toolchain-in-rust/:4:5","series":null,"tags":["compiler","assembler","linker","rust"],"title":"実行プログラム作成基盤をスクラッチで書いた","uri":"/ja/posts/toolchain-in-rust/#そもそもやっている人がいない"},{"categories":null,"content":" 5 まとめここでは自作言語の実装の中でもかなりニッチな 自作バイナリの生成 についてお話しました. ブラックボックスでやりたくないんだ! という方や 低レイヤが好きなんだ! という方は 是非実装に挑戦してみてくださいね. ","date":"2019-12-08","objectID":"/ja/posts/toolchain-in-rust/:5:0","series":null,"tags":["compiler","assembler","linker","rust"],"title":"実行プログラム作成基盤をスクラッチで書いた","uri":"/ja/posts/toolchain-in-rust/#まとめ"},{"categories":null,"content":"IPFactory Advent Calendar 2019 一日目. 急遽開いた弊サークルのカレンダー,既に一日目が終わろうとしている. 私は日頃から勉強した内容をMarkdownにまとめ, Gitリポジトリに保存するようにしている. ここではそのリポジトリから, Linuxにおけるシステムコールの流れのメモを取り出して紹介しよう. 誰も投稿しないよりよっぽどマシだし, おそらく誰かの何かになれると思う. 要は急にやることになったので何もなかった. ","date":"2019-12-01","objectID":"/ja/posts/glibc-reading/:0:0","series":null,"tags":["c","linux"],"title":"glibc wrapperから読み始めてsystem call handlerまで","uri":"/ja/posts/glibc-reading/#"},{"categories":null,"content":" 1 システムコールの流れアプリケーションプログラムがシステムコールを発行した時, 内部ではどのようなフローをたどるのかについて解説する. これを一度理解しておくことで, ユーザランドとカーネルランドのインタフェースについて理解を深められる. ","date":"2019-12-01","objectID":"/ja/posts/glibc-reading/:1:0","series":null,"tags":["c","linux"],"title":"glibc wrapperから読み始めてsystem call handlerまで","uri":"/ja/posts/glibc-reading/#システムコールの流れ"},{"categories":null,"content":" 1.1 glibcでのシステムコールラッパーの処理まず,ユーザアプリケーションでシステムコールを呼び出す時, 往々にして glibc等で定義されたシステムコールラッパー を利用する. 後々実際に見ていくが, このラッパーは内部で syscall命令 を実行している. 例えば brk(2) は以下のようなラッパーが定義されている. /* This must be initialized data because commons can't have aliases. */ void *__curbrk = 0; int __brk (void *addr) { void *newbrk; __curbrk = newbrk = (void *) INLINE_SYSCALL (brk, 1, addr); if (newbrk \u003c addr) { __set_errno (ENOMEM); return -1; } return 0; } weak_alias (__brk, brk) このコードで重要なのは, INLINE_SYSCALL (brk, 1, addr); マクロの実行である. # define INLINE_SYSCALL(name, nr, args...) \\ ({ \\ unsigned long int resultvar = INTERNAL_SYSCALL (name, , nr, args); \\ if (__glibc_unlikely (INTERNAL_SYSCALL_ERROR_P (resultvar, ))) \\ { \\ __set_errno (INTERNAL_SYSCALL_ERRNO (resultvar, )); \\ resultvar = (unsigned long int) -1; \\ } \\ (long int) resultvar; }) 少し見づらいが,簡単にまとめる. INTERNAL_SYSCALL マクロでシステムコールを実行する このマクロについては後述 INLINE_SYSCALL の第一引数を直接受け取る( 上記例なら brk ) INTERNAL_SYSCALL の第三引数に引数の個数が渡る( 仮引数名 -\u003e nr ) INTERNAL_SYSCALL_ERROR_P はエラーチェック INTERNAL_SYSCALL 内部について見てみる. #define INTERNAL_SYSCALL(name, err, nr, args...) \\ internal_syscall##nr (SYS_ify (name), err, args) 引数で渡された nr と internal_syscall が結合される. つまり, 1 が渡されれば internal_syscall1() という関数マクロの呼び出しになる. #define SYS_ify(syscall_name) __NR_##syscall_name と定義されているので,brk(2)における INTERNAL_SYSCALL の呼び出しは次のようになる. (__NR_brk は 12 とマクロ定数で定義されている). internal_syscall1(12, , addr) #undef internal_syscall1 #define internal_syscall1(number, err, arg1) \\ ({ \\ unsigned long int resultvar; \\ TYPEFY (arg1, __arg1) = ARGIFY (arg1); \\ register TYPEFY (arg1, _a1) asm (\"rdi\") = __arg1; \\ asm volatile ( \\ \"syscall\\n\\t\" \\ : \"=a\" (resultvar) \\ : \"0\" (number), \"r\" (_a1) \\ : \"memory\", REGISTERS_CLOBBERED_BY_SYSCALL); \\ (long int) resultvar; \\ }) syscall 命令の実行が確認できる. ","date":"2019-12-01","objectID":"/ja/posts/glibc-reading/:1:1","series":null,"tags":["c","linux"],"title":"glibc wrapperから読み始めてsystem call handlerまで","uri":"/ja/posts/glibc-reading/#glibcでのシステムコールラッパーの処理"},{"categories":null,"content":" 1.2 syscall 命令の実行Intel x64 SDM を読むと, syscall 命令時には IA32_LSTAR というレジスタの値を RIP に入れていることが分かる. なんとなくこの IA32_LSTAR にLinuxカーネルのシステムコールハンドラ(のアドレス)が入っていそうだなあ,という予感がする linux/arch/x86/kernel/cpu/common.c を見ると, syscall_init() 関数を発見できる. この関数は linux/arch/x86/kernel/cpu/common.c の cpu_init() で呼ばれている. /* May not be marked __init: used by software suspend */ void syscall_init(void) { wrmsr(MSR_STAR, 0, (__USER32_CS \u003c\u003c 16) | __KERNEL_CS); wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64); ","date":"2019-12-01","objectID":"/ja/posts/glibc-reading/:1:2","series":null,"tags":["c","linux"],"title":"glibc wrapperから読み始めてsystem call handlerまで","uri":"/ja/posts/glibc-reading/#syscall-命令の実行"},{"categories":null,"content":" 1.3 LinuxのシステムコールハンドラMSR_LSTAR に entry_SYSCALL_64 というアドレスを格納している. このシンボルが システムコールハンドラ だと推測できる. linux/arch/x86/entry/entry_64.S を見てみる. SYM_CODE_START(entry_SYSCALL_64) UNWIND_HINT_EMPTY /* * Interrupts are off on entry. * We do not frame this tiny irq-off block with TRACE_IRQS_OFF/ON, * it is too small to ever cause noticeable irq latency. */ swapgs swapgs 命令によって, IA32_KERNEL_GS_BASE; に格納されたカーネルデータ構造へのポインタを gs レジスタに格納できる. /* Construct struct pt_regs on stack */ pushq $__USER_DS /* pt_regs-\u003ess */ pushq PER_CPU_VAR(cpu_tss_rw + TSS_sp2) /* pt_regs-\u003esp */ pushq %r11 /* pt_regs-\u003eflags */ pushq $__USER_CS /* pt_regs-\u003ecs */ pushq %rcx /* pt_regs-\u003eip */ SYM_INNER_LABEL(entry_SYSCALL_64_after_hwframe, SYM_L_GLOBAL) pushq %rax /* pt_regs-\u003eorig_ax */ PUSH_AND_CLEAR_REGS rax=$-ENOSYS TRACE_IRQS_OFF /* IRQs are off. */ movq %rax, %rdi movq %rsp, %rsi call do_syscall_64 /* returns with IRQs disabled */ LinuxにおけるC言語の呼び出し規約として, 第一引数は rdi , 第二引数は rsi レジスタを用いる. つまり rax (先程のインラインアセンブリによるシステムコール番号)が第一引数, rsp ( pt_regs 構造体がスタックにつまれていて,そのアドレス) が第二引数ということになる. そして呼ばれる do_syscall_64. #ifdef CONFIG_X86_64 __visible void do_syscall_64(unsigned long nr, struct pt_regs *regs) { struct thread_info *ti; enter_from_user_mode(); local_irq_enable(); ti = current_thread_info(); if (READ_ONCE(ti-\u003eflags) \u0026 _TIF_WORK_SYSCALL_ENTRY) nr = syscall_trace_enter(regs); if (likely(nr \u003c NR_syscalls)) { nr = array_index_nospec(nr, NR_syscalls); regs-\u003eax = sys_call_table[nr](regs); #ifdef CONFIG_X86_X32_ABI } else if (likely((nr \u0026 __X32_SYSCALL_BIT) \u0026\u0026 (nr \u0026 ~__X32_SYSCALL_BIT) \u003c X32_NR_syscalls)) { nr = array_index_nospec(nr \u0026 ~__X32_SYSCALL_BIT, X32_NR_syscalls); regs-\u003eax = x32_sys_call_table[nr](regs); #endif } syscall_return_slowpath(regs); } #endif sys_call_table から該当するシステムコールの番号で検索し, 対応する __x64_sys_name() の関数ポインタを取得, rax に入れる. ","date":"2019-12-01","objectID":"/ja/posts/glibc-reading/:1:3","series":null,"tags":["c","linux"],"title":"glibc wrapperから読み始めてsystem call handlerまで","uri":"/ja/posts/glibc-reading/#linuxのシステムコールハンドラ"},{"categories":null,"content":" 2 おわりに取り敢えずここまでで, システムコールラッパー syscall 命令 内部で RIP に MSR_LSTAR の値を格納していることが分かる システムコールハンドラ までの流れが確認できた. ユーザランドとカーネルランドの切り替わり部分が理解出来たので,良しとする. 後で更に深くまで書き足すかもしれないが, 急にやることになった記事としては 悪くない. ","date":"2019-12-01","objectID":"/ja/posts/glibc-reading/:2:0","series":null,"tags":["c","linux"],"title":"glibc wrapperから読み始めてsystem call handlerまで","uri":"/ja/posts/glibc-reading/#おわりに"},{"categories":null,"content":"セキュリティ･キャンプ全国大会2019に参加しました! 集中開発コースの中にある ｢Y-Ⅱ Cコンパイラを自作してみよう!｣ のテーマに応募し､ 普段独学では絶対に出来ない事を多く経験し､ 尊敬する講師の方々からたくさんの事を教えて頂きました｡ セキュリティ･キャンプ(以下seccamp)とは?と思った方は こちらのページを参考にしてください! この記事では､ 応募課題はどんな内容のものだったか 提出したものをそのまま載せます 集中開発コースはどのようなものだったか? 特に Yトラックについて詳しく 成果物の進捗､所感 総評 という流れでお話できればと思います｡ 来年以降seccampに参加しようとしている方の参考になれば幸いです｡ 注意:応募課題についてですが､ 応募時点での状況､応募時点での知識を元にした内容が書かれています｡ コンパイラの知識として間違っているものもあり､ また現在はラボユースとして活動していない等､ いくつか現時点とは異なる部分もあります｡ご了承ください｡ ","date":"2019-08-18","objectID":"/ja/posts/seccamp2019/:0:0","series":null,"tags":["c","compiler"],"title":"seccamp2019のCコンパイラゼミに参加しました","uri":"/ja/posts/seccamp2019/#"},{"categories":null,"content":" 1 応募課題についてseccampでは参加したいコースを選んだ後､ 各コースが用意する設問に解答した上で応募する必要があります｡ 私達Yトラックの課題はこちらに記載されており､ 4/15~5/27の間に問題に答えて提出する､という流れでした｡ 問題はトラック･コースによって様々なのですが私が個人的に抱いた感想は､ ｢うわぁ､どのコースもその分野をどれだけ好きか問うているなあ｣ というものでした｡ 例えばCコンパイラゼミでは､ プログラミングそのものに対する熱意 コンパイラ実装には 実装に大きな時間と労力を要する為 コンパイラの役割･用いられている技術に対する知識 実際にCコンパイラを書いたことがあるか､又はCコンパイラ実装に具体的なイメージが湧いているか 等､ まさにコンパイラについての思い･情熱を答える問題が用意されていました｡ ここではその応募課題(4問のうち3問)を記載したいと思います｡ 参考になるかどうかはわかりませんが､ どちらかというと 後々自分自身で見返したいという気持ちがありますね… 今見返してみるととても恥ずかしいですね｡ コンパイラについての勉強は 応募課題を描き始めた4月にし始めたこともあり､ 少ない知識からとにかくたくさん書いた記憶があります｡ ということはコンパイラの勉強を始めて4ヶ月ぐらいが経過したということになりますね｡ なにか成長できたのだろうか… 以下に各問の回答を示しますが､ 応募時の内容を一切改変せず記載しています｡ 冒頭に述べたように 現在とは状況が異なり､間違っている知識が含まれている可能性がある点をご了承ください｡ 問1 問2 問3 ","date":"2019-08-18","objectID":"/ja/posts/seccamp2019/:1:0","series":null,"tags":["c","compiler"],"title":"seccamp2019のCコンパイラゼミに参加しました","uri":"/ja/posts/seccamp2019/#応募課題について"},{"categories":null,"content":" 2 集中開発コースについて 後述しますが私は選択コースに参加したわけではないので､ 選択コースの魅力･内容を十分に理解していません｡ よってあくまでも集中開発コースについての内容のみ取り上げることにします｡ 自分のやりたいもの､受けたい講義を幅広い種類から選択し､ それらを受講することで知識を深める 選択コースとは異なり､ 集中開発コースはまさに 一本勝負 といえるコースです｡ いくつかのテーマがありますが､ Yトラックでは OS開発ゼミ Cコンパイラ自作ゼミ の2つが存在し､ (事前学習+)3日間OS/コンパイラを書き続けるという夢のような時間を過ごす事ができます｡ やりたい事が一つに定まっている且つ､ とにかく\"実装したい\"という熱意がある 人には､集中開発コースはおすすめです｡ 各ゼミによって差異はあると思いますが､ Cコンパイラゼミでは 受講生が自主的にゴリゴリ進めていきつつ､ 講師の方が適宜助けてくれる､というスタンスでした｡ これは事前学習の時期でも当日でも変わりません｡ 受講生全員がCコンパイラを書いている ので､ 受講生同士で ｢この実装どうやりました?｣ みたいな会話もあって凄い充実していました｡ 私は普段学校やサークルで勉強しているときも､ 周りにコンパイラ･言語処理系の勉強をしているひとがいないので､ こんな環境憧れだなあ と思っていましたね｡ ","date":"2019-08-18","objectID":"/ja/posts/seccamp2019/:2:0","series":null,"tags":["c","compiler"],"title":"seccamp2019のCコンパイラゼミに参加しました","uri":"/ja/posts/seccamp2019/#集中開発コースについて"},{"categories":null,"content":" 3 成果物とその進捗について今回seccampで私が実装したコンパイラはこちらのリポジトリに置いてあります｡ 何故アーカイブしたのかについてですが単純で､ seccamp2019の取り組みでやったものとして保持したかった 後々見返して駄目だったところ等見返したい 確実にもう一度作り直すが､そのときには 新しくリポジトリを作りたい という理由です｡ 事前学習期間と当日に分けてそれぞれ追っていきます｡ ","date":"2019-08-18","objectID":"/ja/posts/seccamp2019/:3:0","series":null,"tags":["c","compiler"],"title":"seccamp2019のCコンパイラゼミに参加しました","uri":"/ja/posts/seccamp2019/#成果物とその進捗について"},{"categories":null,"content":" 3.1 事前学習期間Cコンパイラにあてた時間というのはかなり長くて､ 応募した段階で取り組み始めていました｡ しかし “インクリメンタルな開発\"から離れた方法で実装していた為に､ 何度もつまづき､かなり歯痒い思いをしていました｡ 具体的には､ まずC言語の仕様を読んで完全準拠したLexerを作ろうとする 全部の予約語に対応して､小数も整数もやって､みたいな 結果的に コード生成まで結びつかないコードが何百行もある みたいな状態になっていた 次に 構文解析を延々書きまくる みたいなことをしていた それらも全てコード生成出来ない､みたいな 実際にコード生成出来ていたのは加減算だけだった みたいな感じです｡ これは コードの全体感が全く把握できないし､ モチベーションも上がりません｡ また コード生成まで結びつかないコードが大量にある為に､ テストも非常にしづらい みたいな状況になっていました｡ そこで講師のRuiさんから もう一度作り直したほうが良いという旨のアドバイスを頂きました｡ 結果､セキュキャン当日まで開発していたコンパイラを作り始めたのは 7/8 ということになります｡ 7/15の状態です｡ このときは RuiさんのCコンパイラブックを参照しながら 実装していました｡ その翌日ですね｡ 引数ありの関数呼び出しができるようになったみたいなやつです｡ このときになって やっぱりインクリメンタルな開発って大事だなあと痛感しました｡ ここまで実装したコードが全てコード生成に結びついており､ 全てのコードを 何のために書いたのか説明できる 状態だったからです｡ 7/23､ポインタが実装出来た部分かな? ポインタ演算ができる様になっていますね｡ 実はこれが 事前学習期間の最後の進捗 となってしまいます｡ このあと配列の実装に着手するんですが､ これがわからなすぎて 悠久の時を過ごしました｡ このときは あぁ､当日も三日間配列の実装に手こずったまま終わるんだろうなあとか思っていました｡ ","date":"2019-08-18","objectID":"/ja/posts/seccamp2019/:3:1","series":null,"tags":["c","compiler"],"title":"seccamp2019のCコンパイラゼミに参加しました","uri":"/ja/posts/seccamp2019/#事前学習期間"},{"categories":null,"content":" 3.2 当日1日目､この日はコースごとの活動は行わず､ 全体で講義を受けたりする時間が大半を占めていました｡ 日中の活動が終了し､ 自室に戻って私がしたことは 正規表現エンジンの実装 でした｡ 配列実装がトラウマになっていて逃げていたのかもしれません｡ 一応コンパイラには関係あるんですよね｡ タイガーブック では字句解析器を実装する方法として正規表現が取り上げられており､ 今まで手書きしかやったこと無いなあと思って取り組んでいました｡ 2日目､いよいよコースでの活動が始まりました｡ 肝心の配列実装はというと､講師の方々にアドバイスして頂いたおかげで案外パクっと出来てしまいました｡ 事前学習期間延々悩んでいた事が数時間で実装できて､ とてもスッキリした気分でした｡ 次に実装したのは グローバル変数 ですね｡ あとは char型と､'A'みたいなリテラルの実装をやりました｡ 私は #100DaysOf精進や #100dPeach 等､ 毎日の進捗をツイートしているのですが､ キャンプ中も変わらず実施していました｡ 三日目です｡変わらずコンパイラを書きまくります｡ 文字列リテラルが実装できた瞬間です｡ これはかなり感動したなあ｡ 構造体実装の瞬間｡ うおお､プログラミング言語になってきている! という実感がありましたね｡ ハリボテ実装感は否めないのですが｡ Tweetはしていないのですが､ int x = 30; のような初期化式みたいなのも簡易的にやりましたし､ 0b10010 , 0777 , 0xff みたいな2進8進16進リテラルの実装もやっていました｡ 3日目の終わりのほう､ あ､これセルフホストは出来ないかもなみたいな諦めが生じ､ だったら他にできる事をやろうと思い､ float型の実装に着手しました｡ 4日目の様子です｡ C言語は int main(){ int x = 30.0; //小数リテラルが整数に型変換される float y = 30; //整数リテラルが小数に型変換される int z = x + y; //加算時にxがfloatに型変換された後､計算結果のfloatをintに型変換して代入される } みたいな特徴があるのですが､ これの実装をやってみたくなりました(何故) gcc -S -O0 -masm=intel -fno-asynchronous-unwind-tables の吐くアセンブリを見ると､ .file \"c.c\" .intel_syntax noprefix .text .section .rodata .LC1: .string \"%d\\n\" .text .globl main .type main, @function main: push rbp mov rbp, rsp sub rsp, 16 mov DWORD PTR -12[rbp], 30 ;30.0が変換された状態でストア movss xmm0, DWORD PTR .LC0[rip] movss DWORD PTR -8[rbp], xmm0 ;30を変換しストア cvtsi2ss xmm0, DWORD PTR -12[rbp] ; int xを変換しつつxmmレジスタにロード addss xmm0, DWORD PTR -8[rbp] ; float同士の加算 cvttss2si eax, xmm0 ;整数型に変換し mov DWORD PTR -4[rbp], eax ; 格納 mov eax, DWORD PTR -4[rbp] mov esi, eax lea rdi, .LC1[rip] mov eax, 0 call printf@PLT mov eax, 0 leave ret .size main, .-main .section .rodata .align 4 .LC0: .long 1106247680 .ident \"GCC: (Ubuntu 8.3.0-6ubuntu1) 8.3.0\" .section .note.GNU-stack,\"\",@progbits みたいな感じになっています｡ 私は意味解析時に型を変換するところまでは出来たんですが､ cvttss2si 命令等を使ってアセンブリで変換する､みたいな事ができなかったですね｡ これはCコンパイラを再実装したときの課題にしたいなあと思っています｡ ","date":"2019-08-18","objectID":"/ja/posts/seccamp2019/:3:2","series":null,"tags":["c","compiler"],"title":"seccamp2019のCコンパイラゼミに参加しました","uri":"/ja/posts/seccamp2019/#当日"},{"categories":null,"content":" 4 総評ここまでプログラミングに本気になった3日間はなかったし､ コンパイラ実装について多くの事を学べるとてもいい経験が出来ました｡ 同じことに熱意を持っている人と知り合う事ができたし､ 考え方が大きく変わったという意味でも凄いイベントに参加できたなあ､と思っています｡ ","date":"2019-08-18","objectID":"/ja/posts/seccamp2019/:4:0","series":null,"tags":["c","compiler"],"title":"seccamp2019のCコンパイラゼミに参加しました","uri":"/ja/posts/seccamp2019/#総評"},{"categories":null,"content":" About","date":"0001-01-01","objectID":"/ja/about/:0:0","series":[],"tags":[],"title":"","uri":"/ja/about/#about"},{"categories":null,"content":" 1 Profile Key Value Name Yamato Sugawara Handle Drumato Interests Computer Science/Network/Infrastructure/Cloud/SRE/etc ","date":"0001-01-01","objectID":"/ja/about/:1:0","series":[],"tags":[],"title":"","uri":"/ja/about/#profile"},{"categories":null,"content":" 2 OSS Contributions FRRouting Zebra SRv6 Manager zebra: Add support for json output in srv6 locator detail command enable to transition to SEGMENT_ROUTING_NODE when pathd is disabled ","date":"0001-01-01","objectID":"/ja/about/:2:0","series":[],"tags":[],"title":"","uri":"/ja/about/#oss-contributions"},{"categories":null,"content":" 3 My Products drumato.comv2 … My Blog with TypeScript + Next.js Depth … my 1st generation x86_64 toolchain Peachili … my toy compiler that doesn’t depends on libc asmpeach elfpeach elf-utilities ","date":"0001-01-01","objectID":"/ja/about/:3:0","series":[],"tags":[],"title":"","uri":"/ja/about/#my-products"},{"categories":null,"content":" 4 Slides Japanese as an SRE in SAKURA Internet Inc. 社内のKubernetes運用状況の改善活動について indivisual x64/aarch64コンパイラを含むミニツールチェーン+αの開発 - Cybozu Labs Youth 10th an incremental approach to implement an admission controller eBPF disassemblerを作る English Components of Kubernetes Cluster ","date":"0001-01-01","objectID":"/ja/about/:4:0","series":[],"tags":[],"title":"","uri":"/ja/about/#slides"},{"categories":null,"content":" 5 Books Japanese Zenn.dev [全部無料]最小限で理解しつつ作るELF parser入門 in Rust Rustでpopularに使われるparser combinatorであるnomを使ってELF parserを作る本です ELFについて全く知らない人にもおすすめです 技術同人誌 OtakuAssembly Vol.1(co-authored) OtakuAssemblyというサークルに所属してELFのことを書きました 行動力のある人たちと一緒に本を書き上げる経験はとても貴重でした ","date":"0001-01-01","objectID":"/ja/about/:5:0","series":[],"tags":[],"title":"","uri":"/ja/about/#books"},{"categories":null,"content":" 6 Events seccamp2019 Y-Ⅱ Cコンパイラを自作してみよう! 受講生 report SecHack365'19 Trainee SecHack365での1年間をまとめたreport 成果物についてfocusしてまとめたreport 第10期サイボウズ･ラボユース生 Report in Japanese Online Summer Internship for Gophers 2020参加 Report in Japanese KLab Expert Camp#3 ","date":"0001-01-01","objectID":"/ja/about/:6:0","series":[],"tags":[],"title":"","uri":"/ja/about/#events"},{"categories":null,"content":" 7 Other posts as an SRE in SAKURA Internet Inc. Japanese 社内のKubernetesクラスタ運用を効率化する基盤について as a part-timer of LINE Corporation Japanese 仮想ルータクラスタを自動でローリングアップデートする仕組みの検討と実装 BGP Graceful Restartに関わる各OSSルーティングプラットフォームの動向調査 ","date":"0001-01-01","objectID":"/ja/about/:7:0","series":[],"tags":[],"title":"","uri":"/ja/about/#other-posts"},{"categories":null,"content":" 8 CareerSee LinkedIn ","date":"0001-01-01","objectID":"/ja/about/:8:0","series":[],"tags":[],"title":"","uri":"/ja/about/#career"},{"categories":null,"content":" drumato.comについて","date":"0001-01-01","objectID":"/ja/thissite/:0:0","series":null,"tags":null,"title":"","uri":"/ja/thissite/#drumatocomについて"},{"categories":null,"content":" 1 免責事項本ブログ内のいかなる情報，知識，記事の内容について，その正確性については保証いたしかねますので，ご了承ください． ","date":"0001-01-01","objectID":"/ja/thissite/:1:0","series":null,"tags":null,"title":"","uri":"/ja/thissite/#免責事項"},{"categories":null,"content":" 2 ライセンスdrumato.comで記載されているソースコードはすべて CC BY 4.0 で配布しています. ","date":"0001-01-01","objectID":"/ja/thissite/:2:0","series":null,"tags":null,"title":"","uri":"/ja/thissite/#ライセンス"}]